{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# single source of truth for dashboard outputs\n",
    "OUTPUT_DIR = Path(\"docs/data/derived\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_csv(df, name):\n",
    "    p = OUTPUT_DIR / name\n",
    "    df.to_csv(p, index=False)\n",
    "    print(f\"✔ saved: {p}\")\n",
    "\n",
    "# …then everywhere you save:\n",
    "# save_csv(provider_scores_df, \"provider_scores_latest.csv\")\n",
    "# save_csv(roi_df,              \"roi_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape provider\n",
    "# --- Async helper that works in notebooks and GitHub Actions ---\n",
    "import asyncio\n",
    "\n",
    "def await_safe(coro):\n",
    "    \"\"\"\n",
    "    Run an async coroutine from anywhere:\n",
    "    - If an event loop is already running (Jupyter/nbconvert), use nest_asyncio + run_until_complete\n",
    "    - Otherwise, use asyncio.run\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            try:\n",
    "                import nest_asyncio\n",
    "                nest_asyncio.apply()\n",
    "            except Exception:\n",
    "                pass\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        # No current loop\n",
    "        return asyncio.run(coro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>8X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>8</td>\n",
       "      <td>2.99</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>4X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>4</td>\n",
       "      <td>3.09</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>2X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>2</td>\n",
       "      <td>3.19</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>1.49</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>3.29</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region    gpu_model       type duration  gpu_count  \\\n",
       "0  Lambda Labs     US  8X H100 SXM  On-Demand       1h          8   \n",
       "1  Lambda Labs     US  4X H100 SXM  On-Demand       1h          4   \n",
       "2  Lambda Labs     US  2X H100 SXM  On-Demand       1h          2   \n",
       "3  Lambda Labs     US      1X H200  On-Demand       1h          1   \n",
       "4  Lambda Labs     US  1X H100 SXM  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                            source_url      fetched_at_utc  \n",
       "0              2.99  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "1              3.09  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "2              3.19  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "3              1.49  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "4              3.29  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lambda Labs \n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from typing import Optional\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _norm_gpu(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")  # treat GH200 as H200\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(s: str) -> Optional[int]:\n",
    "    if not isinstance(s, str): \n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)x\", s, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_in(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): \n",
    "        return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes https://cloud.lambdalabs.com/pricing and returns SLIM rows\n",
    "    for H100/H200 (On-Demand, 1h). If `region` provided, overrides detected region.\n",
    "    \"\"\"\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    rows_out = []\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        tbl_region = region or _infer_region(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds:\n",
    "                continue\n",
    "            row_text = \" | \".join(tds)\n",
    "\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "\n",
    "            price = _price_in(row_text)\n",
    "            if price is None:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            gpu_model = _norm_gpu(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            count = _gpu_count(gpu_model)\n",
    "\n",
    "            rows_out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": gpu_model,      \n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": count,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    df = df[keep].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# Example:\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "display(df_lambda.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lambda_labs] snapshot -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/20250904_112436_lambda_labs.csv\n",
      "[lambda_labs] history  -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/lambda_labs_history.csv\n",
      "[lambda_labs] latest   -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/lambda_labs_latest.csv\n",
      "[runpod] snapshot -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/20250904_112441_runpod.csv\n",
      "[runpod] history  -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/runpod_history.csv\n",
      "[runpod] latest   -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/runpod_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200 141 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>3.59</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-04 11:24:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100 PCIE 80 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-04 11:24:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region        gpu_model       type duration gpu_count  \\\n",
       "4   RunPod  Global      H200 141 GB  On-Demand       1h      None   \n",
       "6   RunPod  Global  H100 PCIE 80 GB  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                     source_url      fetched_at_utc  \n",
       "4              3.59  https://www.runpod.io/pricing 2025-09-04 11:24:41  \n",
       "6              1.99  https://www.runpod.io/pricing 2025-09-04 11:24:41  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Lambda Labs (static) + RunPod (async) with per-provider history =====\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, os, asyncio, pandas as pd, tempfile\n",
    "from typing import Optional, Dict, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "# -------- storage (per-provider history/snapshots) --------\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        # fallback to tmp if workspace is read-only\n",
    "        tmp = Path(tempfile.gettempdir()) / \"gpu_data\"\n",
    "        d = tmp / d.name\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    try:\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    except Exception:\n",
    "        snap_path = Path(tempfile.gettempdir()) / f\"{ts}_{provider_slug}.csv\"\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    try:\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    except Exception:\n",
    "        hist_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_history.csv\"\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    # latest (newest rows only per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    try:\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    except Exception:\n",
    "        latest_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_latest.csv\"\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ------------------------- Lambda Labs (static) -------------------------\n",
    "def _norm_gpu_lambda(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(text: str) -> Optional[int]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"(\\d+)\\s*x\", text, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_dollar(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region_lambda(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30); r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    out = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        tbl_region = region or _infer_region_lambda(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds: continue\n",
    "            row_text = \" | \".join(tds)\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "            price = _price_dollar(row_text)\n",
    "            if price is None: continue\n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            model = _norm_gpu_lambda(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": _gpu_count(model),\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    return _ensure_slim(df[keep].reset_index(drop=True))\n",
    "\n",
    "# --------------------------- RunPod (async) ---------------------------\n",
    "def _extract_gpu_model_runpod(text: str) -> Optional[str]:\n",
    "    if not isinstance(text, str): return None\n",
    "    text_up = re.sub(r\"\\s+\", \" \", text.upper())\n",
    "    m = re.search(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", text_up)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def _price_hourly_runpod(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    t = text.replace(\",\", \"\")\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", t, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "async def scrape_runpod_async() -> pd.DataFrame:\n",
    "    from playwright.async_api import async_playwright\n",
    "    url = \"https://www.runpod.io/pricing\"; region = \"Global\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1200)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\"], class_=re.compile(r\"(price|pricing|card|grid|table)\", re.I))\n",
    "    if not nodes:\n",
    "        nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\",\"p\",\"span\"])\n",
    "\n",
    "    out = []\n",
    "    for n in nodes:\n",
    "        text = n.get_text(\" \", strip=True)\n",
    "        if \"H100\" not in text and \"H200\" not in text: \n",
    "            continue\n",
    "        price = _price_hourly_runpod(text)\n",
    "        if price is None:\n",
    "            continue\n",
    "        model = _extract_gpu_model_runpod(text)\n",
    "        if model is None:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"provider\": \"RunPod\",\n",
    "            \"region\": region,\n",
    "            \"gpu_model\": model,\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": _gpu_count(text),\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    df = df[df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", na=False)]\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# --------------- Runner that works in scripts & notebooks ---------------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "# Lambda Labs\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "_save_provider(df_lambda, \"lambda_labs\")\n",
    "\n",
    "# RunPod\n",
    "df_runpod = arun(scrape_runpod_async())\n",
    "_save_provider(df_runpod, \"runpod\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nebius] snapshot -> docs/data/snapshots/20250904_114258_nebius.csv\n",
      "[nebius] history  -> docs/data/history/nebius_history.csv\n",
      "[nebius] latest   -> docs/data/latest/nebius_latest.csv\n",
      "Nebius rows this run: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04T11:42:58.443649+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04T11:42:58.443649+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0   Nebius  Global      H200  On-Demand       1h      None               2.3   \n",
       "1   Nebius  Global      H100  On-Demand       1h      None               2.0   \n",
       "\n",
       "                  source_url                    fetched_at_utc  \n",
       "0  https://nebius.com/prices  2025-09-04T11:42:58.443649+00:00  \n",
       "1  https://nebius.com/prices  2025-09-04T11:42:58.443649+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Nebius H100/H200 scraper (uses YOUR parsing + per-provider history) ---\n",
    "\n",
    "import re, time, tempfile, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "\n",
    "# ---------- your original config ----------\n",
    "url = \"https://nebius.com/prices\"\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\"}\n",
    "TZ = pytz.utc\n",
    "MIN_PRICE, MAX_PRICE = 0.3, 20.0   # sanity for $/GPU/hr\n",
    "\n",
    "# ---------- storage dirs (snapshot/history/latest) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # we'll fall back to tmp if write fails later\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(TZ).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- your original parsing (unchanged) ----------\n",
    "def find_price_strict(text: str):\n",
    "    \"\"\"Match $X/hr, $X per hour, $X/hour, case-insensitive.\"\"\"\n",
    "    if not text: return None\n",
    "    m = re.search(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", text, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def parse_nebius_from_html(html: str) -> dict:\n",
    "    \"\"\"Return {'H100': price, 'H200': price} if found.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = {}\n",
    "\n",
    "    # Focus on plausible pricing containers first (tables / pricing sections)\n",
    "    blocks = []\n",
    "    blocks.extend(soup.find_all(\"table\"))\n",
    "    if not blocks:\n",
    "        blocks.extend(soup.find_all([\"section\",\"div\"], class_=re.compile(\"price|pricing|compute\", re.I)))\n",
    "    if not blocks:\n",
    "        blocks = soup.find_all([\"div\",\"tr\",\"li\",\"p\",\"span\"])\n",
    "\n",
    "    for blk in blocks:\n",
    "        t = blk.get_text(\" \", strip=True)\n",
    "        if not t: continue\n",
    "\n",
    "        has_h100 = bool(re.search(r\"\\bH100\\b\", t, flags=re.I))\n",
    "        has_h200 = bool(re.search(r\"\\bH200\\b\", t, flags=re.I))\n",
    "        if not (has_h100 or has_h200): continue\n",
    "\n",
    "        price = find_price_strict(t)\n",
    "        if price is None or not (MIN_PRICE <= price <= MAX_PRICE): continue\n",
    "\n",
    "        if has_h100 and \"H100\" not in results:\n",
    "            results[\"H100\"] = price\n",
    "        if has_h200 and \"H200\" not in results:\n",
    "            results[\"H200\"] = price\n",
    "\n",
    "        if len(results) == 2:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------- run (your flow) ----------\n",
    "html = None\n",
    "try:\n",
    "    r = requests.get(url, headers=UA, timeout=30)\n",
    "    if r.status_code == 200 and r.text:\n",
    "        html = r.text\n",
    "except Exception:\n",
    "    html = None\n",
    "\n",
    "results = {}\n",
    "if html:\n",
    "    results = parse_nebius_from_html(html)\n",
    "\n",
    "# Optional Playwright fallback if nothing found\n",
    "if not results:\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "            page.goto(url, wait_until=\"networkidle\", timeout=60000)\n",
    "            page.wait_for_timeout(2000)  # allow dynamic content\n",
    "            html_pw = page.content()\n",
    "            browser.close()\n",
    "        results = parse_nebius_from_html(html_pw)\n",
    "    except Exception:\n",
    "        pass  # proceed with whatever we have\n",
    "\n",
    "# ---------- map YOUR results -> SLIM schema & save ----------\n",
    "rows = []\n",
    "ts = datetime.now(TZ).isoformat()\n",
    "for gpu, price in results.items():\n",
    "    rows.append({\n",
    "        \"provider\": \"Nebius\",\n",
    "        \"region\": \"Global\",            # keep simple; refine if you later detect regions\n",
    "        \"gpu_model\": gpu,              # map gpu_type -> gpu_model\n",
    "        \"type\": \"On-Demand\",\n",
    "        \"duration\": \"1h\",\n",
    "        \"gpu_count\": None,\n",
    "        \"price_hourly_usd\": price,     # map on_demand_price -> price_hourly_usd\n",
    "        \"source_url\": url,\n",
    "        \"fetched_at_utc\": ts,          # map scraped_at -> fetched_at_utc\n",
    "    })\n",
    "\n",
    "df_nebius = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "latest = _save_provider(df_nebius, \"nebius\")\n",
    "print(f\"Nebius rows this run: {len(df_nebius)}\")\n",
    "display(df_nebius.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[voltagepark] snapshot -> docs/data/snapshots/20250904_123626_voltagepark.csv\n",
      "[voltagepark] history  -> docs/data/history/voltagepark_history.csv\n",
      "[voltagepark] latest   -> docs/data/latest/voltagepark_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "      <td>2025-09-04 12:36:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region gpu_model       type duration gpu_count  \\\n",
       "0  VoltagePark     US      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                                         source_url  \\\n",
       "0              1.99  https://dashboard.voltagepark.com/order/config...   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-04 12:36:26  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= VoltagePark (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, tempfile, pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage + schema helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- YOUR scraping logic, adapted to slim schema ----\n",
    "async def scrape_voltagepark() -> pd.DataFrame:\n",
    "    url = \"https://dashboard.voltagepark.com/order/configure-deployment\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        await page.wait_for_timeout(5000)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    for line in html.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                # your original pattern\n",
    "                m = re.search(r\"\\$?(\\d+(?:\\.\\d+)?)(?=/GPU/hour)\", line)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    gpu = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                    rows.append({\n",
    "                        \"provider\": \"VoltagePark\",\n",
    "                        \"region\": \"US\",\n",
    "                        \"gpu_model\": gpu,\n",
    "                        \"type\": \"On-Demand\",\n",
    "                        \"duration\": \"1h\",\n",
    "                        \"gpu_count\": None,\n",
    "                        \"price_hourly_usd\": price,\n",
    "                        \"source_url\": url,\n",
    "                        \"fetched_at_utc\": _now_iso(),\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                # keep silent in prod; print minimal context if you want\n",
    "                # print(f\"[VoltagePark Parse Error] {e}\")\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # dedupe by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # sanity: plausible $/hr range\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in notebooks & scripts ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_voltage = arun(scrape_voltagepark())\n",
    "_save_provider(df_voltage, \"voltagepark\")\n",
    "display(df_voltage.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vastai] snapshot -> docs/data/snapshots/20250904_124214_vastai.csv\n",
      "[vastai] history  -> docs/data/history/vastai_history.csv\n",
      "[vastai] latest   -> docs/data/latest/vastai_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.25</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "      <td>2025-09-04 12:42:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0  Vast.ai  Global      H100  On-Demand       1h      None              1.25   \n",
       "\n",
       "                           source_url      fetched_at_utc  \n",
       "0  https://vast.ai/products/gpu-cloud 2025-09-04 12:42:14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Vast.ai (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled below\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- YOUR scraping logic, adapted to slim schema --------\n",
    "async def scrape_vast_products() -> pd.DataFrame:\n",
    "    url = \"https://vast.ai/products/gpu-cloud\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        # Try to reveal lazy content\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1500)\n",
    "        await page.evaluate(\"window.scrollTo(0, 0)\")\n",
    "        await page.wait_for_timeout(500)\n",
    "\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Your original approach: scan lines and pick $ numbers near H100/H200\n",
    "    for line in content.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                gpu_model = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                # pull all $-bearing tokens in the line\n",
    "                dollars = [s for s in re.split(r\"\\s+\", line) if \"$\" in s]\n",
    "                price_val = None\n",
    "                for token in dollars:\n",
    "                    clean = \"\".join(c for c in token if c.isdigit() or c == \".\")\n",
    "                    if not clean:\n",
    "                        continue\n",
    "                    price = float(clean)\n",
    "                    if 0.1 < price < 100:  # sanity filter like you had\n",
    "                        price_val = price\n",
    "                        break\n",
    "                if price_val is None:\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"Vast.ai\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": gpu_model,\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": None,\n",
    "                    \"price_hourly_usd\": price_val,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "            except Exception:\n",
    "                # swallow parse errors to keep the run clean\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # Deduplicate by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # Sanity clamp\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "df_vastp = arun(scrape_vast_products())\n",
    "_save_provider(df_vastp, \"vastai\")\n",
    "display(df_vastp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:49:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.35</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:49:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "1  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                 source_url      fetched_at_utc  \n",
       "0              1.99  https://www.shadeform.ai/ 2025-09-04 12:49:12  \n",
       "1              2.35  https://www.shadeform.ai/ 2025-09-04 12:49:12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Shadeform: precise matcher (nearest-price + hourly hint) ====\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# slim schema storage helpers (use the same ones you already have)\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "# --------- robust extractors ----------\n",
    "# require an hourly hint, allowing variants like \"/GPU/hour\"\n",
    "PRICE_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/GPU)?\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "GPU_TOKENS = {\n",
    "    \"H100\": re.compile(r\"\\bH100\\b\", re.I),\n",
    "    \"H200\": re.compile(r\"\\bH200\\b\", re.I),\n",
    "    # include B200 so we don't steal its prices\n",
    "    \"_OTHER\": re.compile(r\"\\b(?:B200|H800|A100|A800)\\b\", re.I),\n",
    "}\n",
    "\n",
    "def _find_token_positions(text: str):\n",
    "    positions = {k: [] for k in GPU_TOKENS.keys()}\n",
    "    for name, pat in GPU_TOKENS.items():\n",
    "        for m in pat.finditer(text):\n",
    "            positions[name].append(m.start())\n",
    "    return positions\n",
    "\n",
    "def _find_price_positions(text: str):\n",
    "    return [(float(m.group(1)), m.start()) for m in PRICE_RE.finditer(text)]\n",
    "\n",
    "def _nearest_price_to_token(text: str, token: str, window: int = 220):\n",
    "    \"\"\"Yield (model, price) pairs by attaching each token occurrence\n",
    "       to the nearest price with an hourly hint, only if it is closer\n",
    "       to this token than to any other GPU token.\"\"\"\n",
    "    tok_positions = _find_token_positions(text)\n",
    "    prices = _find_price_positions(text)\n",
    "    if not tok_positions.get(token) or not prices:\n",
    "        return []\n",
    "\n",
    "    # all GPU-ish positions (to compete for 'closeness')\n",
    "    competitor_positions = []\n",
    "    for k, pos_list in tok_positions.items():\n",
    "        if k == token:  # we compare against others later\n",
    "            continue\n",
    "        competitor_positions.extend(pos_list)\n",
    "\n",
    "    rows = []\n",
    "    for gpos in tok_positions[token]:\n",
    "        # candidates within a window around the GPU string\n",
    "        cands = [(price, ppos, abs(ppos - gpos)) for (price, ppos) in prices if abs(ppos - gpos) <= window]\n",
    "        if not cands:\n",
    "            continue\n",
    "        # pick nearest price to this token\n",
    "        price, ppos, dist = min(cands, key=lambda t: t[2])\n",
    "\n",
    "        # ensure this price isn't actually closer to another GPU token (e.g., B200)\n",
    "        if competitor_positions:\n",
    "            nearest_other = min(abs(ppos - op) for op in competitor_positions)\n",
    "            if nearest_other < dist:\n",
    "                continue  # skip: price belongs to another GPU mention\n",
    "\n",
    "        rows.append((token, price))\n",
    "    return rows\n",
    "\n",
    "# --------- scraper ----------\n",
    "async def scrape_shadeform_rich() -> pd.DataFrame:\n",
    "    url = \"https://www.shadeform.ai/\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(900)\n",
    "        body = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    body = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "    rows = []\n",
    "    for gpu in (\"H100\", \"H200\"):\n",
    "        for model, price in _nearest_price_to_token(body, gpu, window=220):\n",
    "            # sanity clamp to avoid accidental captures (tune if needed)\n",
    "            if not (0.25 <= price <= 8.0):\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"provider\": \"Shadeform\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": None,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# Example:\n",
    "df_shade = arun(scrape_shadeform_rich())\n",
    "display(df_shade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[coreweave] snapshot -> docs/data/snapshots/20250904_125500_coreweave.csv\n",
      "[coreweave] history  -> docs/data/history/coreweave_history.csv\n",
      "[coreweave] latest   -> docs/data/latest/coreweave_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>20.0</td>\n",
       "      <td>https://www.coreweave.com/pricing</td>\n",
       "      <td>2025-09-04 12:55:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0  CoreWeave     US      H100  On-Demand       1h      None              20.0   \n",
       "\n",
       "                          source_url      fetched_at_utc  \n",
       "0  https://www.coreweave.com/pricing 2025-09-04 12:55:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= CoreWeave (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- robust token/price matching --------\n",
    "PRICE_HOURLY_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:GPU\\s*/\\s*)?(?:h|hr|hour)\\b\",\n",
    "    re.I\n",
    ")\n",
    "# Fallback if the site omits 'hr' text; avoid monthly and memory suffixes\n",
    "PRICE_DOLLAR_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\b(?!\\s*(?:k|m|b|/mo|per\\s*month|/month|,?\\s*GB))\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "GPU_PATS = {\n",
    "    \"H100\": re.compile(r\"\\bH100\\b\", re.I),\n",
    "    \"H200\": re.compile(r\"\\bH200\\b\", re.I),\n",
    "    \"_OTHER\": re.compile(r\"\\b(?:B200|A100|A800|H800)\\b\", re.I),\n",
    "}\n",
    "\n",
    "def _find_positions(text: str, pat: re.Pattern):\n",
    "    return [m.start() for m in pat.finditer(text)]\n",
    "\n",
    "def _find_prices(text: str, prefer_hourly: bool = True):\n",
    "    pats = [PRICE_HOURLY_RE] + ([] if not prefer_hourly else [])  # first pass\n",
    "    prices = [(float(m.group(1)), m.start()) for m in PRICE_HOURLY_RE.finditer(text)]\n",
    "    if not prices:\n",
    "        prices = [(float(m.group(1)), m.start()) for m in PRICE_DOLLAR_RE.finditer(text)]\n",
    "    return prices\n",
    "\n",
    "def _nearest_prices(text: str, token: str, window: int = 240):\n",
    "    # Positions of our token vs. competitors\n",
    "    tok_pos = _find_positions(text, GPU_PATS[token])\n",
    "    if not tok_pos:\n",
    "        return []\n",
    "    comp_pos = []\n",
    "    for k, pat in GPU_PATS.items():\n",
    "        if k == token: continue\n",
    "        comp_pos.extend(_find_positions(text, pat))\n",
    "    prices = _find_prices(text)\n",
    "    out = []\n",
    "    for gpos in tok_pos:\n",
    "        cands = [(price, ppos, abs(ppos - gpos)) for (price, ppos) in prices if abs(ppos - gpos) <= window]\n",
    "        if not cands: \n",
    "            continue\n",
    "        price, ppos, dist = min(cands, key=lambda t: t[2])\n",
    "        if comp_pos:\n",
    "            nearest_other = min(abs(ppos - op) for op in comp_pos)\n",
    "            if nearest_other < dist:\n",
    "                continue\n",
    "        out.append((token, price))\n",
    "    return out\n",
    "\n",
    "# -------- CoreWeave scraper --------\n",
    "async def scrape_coreweave_async() -> pd.DataFrame:\n",
    "    url = \"https://www.coreweave.com/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help lazy content load\n",
    "        for _ in range(3):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "        # Wait for any pricing text to appear (best-effort)\n",
    "        try:\n",
    "            await page.wait_for_selector(\"text=/H100|H200/\", timeout=5000)\n",
    "        except Exception:\n",
    "            pass\n",
    "        body = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    body = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "    rows = []\n",
    "    for gpu in (\"H100\", \"H200\"):\n",
    "        for model, price in _nearest_prices(body, gpu, window=240):\n",
    "            # reasonable hourly range; widen if CoreWeave posts higher tiers\n",
    "            if not (0.25 <= price <= 25.0):\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"provider\": \"CoreWeave\",\n",
    "                \"region\": \"US\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": None,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # Deduplicate (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# -------- runner (works in scripts & notebooks) --------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_coreweave = arun(scrape_coreweave_async())\n",
    "latest_coreweave = _save_provider(df_coreweave, \"coreweave\")\n",
    "display(df_coreweave.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paperspace] snapshot -> docs/data/snapshots/20250904_130549_paperspace.csv\n",
      "[paperspace] history  -> docs/data/history/paperspace_history.csv\n",
      "[paperspace] latest   -> docs/data/latest/paperspace_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.24</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "      <td>2025-09-04 13:05:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Paperspace  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                          source_url      fetched_at_utc  \n",
       "0              2.24  https://www.paperspace.com/pricing 2025-09-04 13:05:49  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Paperspace (async) — use your approach, keep correct rows =================\n",
    "# Output schema (slim): provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                       price_hourly_usd, source_url, fetched_at_utc\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\"); HIST_DIR = BASE/\"history\"; SNAP_DIR = BASE/\"snapshots\"; LATEST_DIR = BASE/\"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso(): return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df); ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR/f\"{ts}_{slug}.csv\")\n",
    "    # history\n",
    "    hist = HIST_DIR/f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "                    .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                             \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "                    .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR/f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- strict extractors (but still tolerant to site markup) ----\n",
    "GPU_PAT = re.compile(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", re.I)\n",
    "# require an hourly hint somewhere in the same block to avoid platform prices, etc.\n",
    "PRICE_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:GPU\\s*/\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "async def scrape_paperspace() -> pd.DataFrame:\n",
    "    url = \"https://www.paperspace.com/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000)               # your simple navigation\n",
    "        await page.wait_for_timeout(8000)                  # your “just wait a few seconds”\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    rows = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # scan reasonable blocks; stick to your block-scan approach\n",
    "    for blk in soup.find_all([\"tr\",\"div\",\"section\",\"article\",\"li\"], recursive=True):\n",
    "        txt = blk.get_text(\" \", strip=True)\n",
    "        if not txt: \n",
    "            continue\n",
    "        # must mention H100/H200 AND 'hour' to qualify\n",
    "        if (\"H100\" not in txt and \"H200\" not in txt) or (\"hour\" not in txt.lower()):\n",
    "            continue\n",
    "\n",
    "        # model: first explicit H100/H200-ish token found\n",
    "        mm = GPU_PAT.search(txt)\n",
    "        if not mm:\n",
    "            continue\n",
    "        model = mm.group(1).upper()\n",
    "\n",
    "        # price: $… with an hourly hint in the same block\n",
    "        pm = PRICE_HOURLY.search(txt)\n",
    "        if not pm:\n",
    "            continue\n",
    "        price = float(pm.group(1))\n",
    "        # sanity band to drop weird captures\n",
    "        if not (0.2 <= price <= 50.0):\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"provider\": \"Paperspace\",\n",
    "            \"region\": \"Global\",\n",
    "            \"gpu_model\": model,          # \"H100\", \"H100 PCIE 80GB\", etc.\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": None,\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in both scripts & notebooks ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_paperspace = arun(scrape_paperspace())\n",
    "latest_paperspace = _save_provider(df_paperspace, \"paperspace\")\n",
    "display(df_paperspace.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensordock] snapshot -> docs/data/snapshots/20250904_131631_tensordock.csv\n",
      "[tensordock] history  -> docs/data/history/tensordock_history.csv\n",
      "[tensordock] latest   -> docs/data/latest/tensordock_latest.csv\n",
      "     provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  TensorDock  Global      H100  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                       source_url      fetched_at_utc  \n",
      "0              2.25  https://tensordock.com/gpu-h100 2025-09-04 13:16:31  \n"
     ]
    }
   ],
   "source": [
    "# ================= TensorDock H100 (static) — slim schema + per-provider history =================\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, requests, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- pages & patterns (from your code) --------\n",
    "PAGES = [\n",
    "    \"https://tensordock.com/gpu-h100\",\n",
    "    \"https://tensordock.com/cloud-gpus\",\n",
    "    \"https://tensordock.com/comparison-gcp\",\n",
    "]\n",
    "PATTERNS = [\n",
    "    re.compile(r\"H100.*?\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr\", re.I|re.S),\n",
    "    re.compile(r\"from\\s*\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr.*?H100\", re.I|re.S),\n",
    "    re.compile(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hour.*?H100\", re.I|re.S),\n",
    "]\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fall back handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- scraper (uses your logic, mapped to slim schema) --------\n",
    "def scrape_tensordock_public_h100() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for url in PAGES:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "            price = None\n",
    "            for pat in PATTERNS:\n",
    "                m = pat.search(text)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    break\n",
    "            if price and (0.2 <= price <= 50.0):  # sanity band for $/GPU/hr\n",
    "                rows.append({\n",
    "                    \"provider\": \"TensorDock\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": \"H100\",\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": 1,\n",
    "                    \"price_hourly_usd\": price,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"[TensorDock] {url} -> {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        return _ensure_slim(pd.DataFrame(columns=SLIM_COLS))\n",
    "\n",
    "    # Deduplicate: keep the **lowest** \"from\" price across pages\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = (df.sort_values(\"price_hourly_usd\")\n",
    "            .drop_duplicates(subset=[\"provider\",\"gpu_model\"], keep=\"first\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_tensordock = scrape_tensordock_public_h100()\n",
    "latest_tensordock = _save_provider(df_tensordock, \"tensordock\")\n",
    "print(df_tensordock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[brokkr] snapshot -> docs/data/snapshots/20250904_132203_brokkr.csv\n",
      "[brokkr] history  -> docs/data/history/brokkr_history.csv\n",
      "[brokkr] latest   -> docs/data/latest/brokkr_latest.csv\n",
      "              provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
      "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                              source_url  \\\n",
      "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
      "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
      "\n",
      "       fetched_at_utc  \n",
      "0 2025-09-04 13:22:03  \n",
      "1 2025-09-04 13:22:03  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              provider  region gpu_model       type duration  gpu_count  \\\n",
       "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
       "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                              source_url  \\\n",
       "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
       "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-04 13:22:03  \n",
       "1 2025-09-04 13:22:03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============== Hydra Host (Brokkr) — slim schema + per-provider history ==============\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same pattern as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- your Brokkr scraper, tightened to only accept \"per card-hour\" prices --------\n",
    "GPU_RE = r\"(H100|H200)\"\n",
    "PRICE_PER_CARDHR = r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:per\\s*card[-\\s]?hour|/card[-\\s]?hour)\\b\"\n",
    "PATS = [\n",
    "    re.compile(rf\"{GPU_RE}.{{0,220}}?{PRICE_PER_CARDHR}\", re.I | re.S),\n",
    "    re.compile(rf\"{PRICE_PER_CARDHR}.{{0,220}}?{GPU_RE}\", re.I | re.S),\n",
    "]\n",
    "\n",
    "async def scrape_brokkr() -> pd.DataFrame:\n",
    "    url = \"https://brokkr.hydrahost.com/inventory\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help hydrate lazy content\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(700)\n",
    "        body_text = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", body_text)\n",
    "    rows = []\n",
    "\n",
    "    for pat in PATS:\n",
    "        for m in pat.finditer(text):\n",
    "            # Depending on which pattern matched, group order differs\n",
    "            groups = m.groups()\n",
    "            # Normalize extraction: model + price are always present\n",
    "            if len(groups) == 2:\n",
    "                # pattern 1: (GPU, price)\n",
    "                gpu_model, price_str = groups\n",
    "            elif len(groups) == 3:\n",
    "                # pattern 2 returns (price, GPU) because of nested groups; pick numeric+gpu\n",
    "                # groups could be ('12.34', 'H100') or ('12.34', 'card-hour', 'H100') depending on regex engine\n",
    "                nums = [g for g in groups if g and re.fullmatch(r\"[0-9]+(?:\\.[0-9]+)?\", g)]\n",
    "                gpus = [g for g in groups if g and re.fullmatch(r\"H100|H200\", g, flags=re.I)]\n",
    "                if not nums or not gpus:\n",
    "                    continue\n",
    "                price_str, gpu_model = nums[0], gpus[0]\n",
    "            else:\n",
    "                # Safe fallback: find first number and first GPU token in the match\n",
    "                seg = m.group(0)\n",
    "                pm = re.search(r\"[0-9]+(?:\\.[0-9]+)?\", seg)\n",
    "                gm = re.search(r\"H100|H200\", seg, flags=re.I)\n",
    "                if not (pm and gm):\n",
    "                    continue\n",
    "                price_str, gpu_model = pm.group(0), gm.group(0)\n",
    "\n",
    "            try:\n",
    "                price = float(price_str)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # sanity band for per-card hour pricing\n",
    "            if not (0.2 <= price <= 50.0):\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"provider\": \"Hydra Host (Brokkr)\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": gpu_model.upper(),\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": price,   # per card-hour = per-GPU hourly\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# -------- runner that works in both notebooks & scripts --------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_brokkr = arun(scrape_brokkr())\n",
    "latest_brokkr = _save_provider(df_brokkr, \"brokkr\")\n",
    "print(df_brokkr)\n",
    "display(df_brokkr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[crusoecloud] snapshot -> docs/data/snapshots/20250904_132724_crusoecloud.csv\n",
      "[crusoecloud] history  -> docs/data/history/crusoecloud_history.csv\n",
      "[crusoecloud] latest   -> docs/data/latest/crusoecloud_latest.csv\n",
      "      provider  region gpu_model         type duration  gpu_count  \\\n",
      "0  CrusoeCloud  Global      H100    On-Demand       1h          1   \n",
      "1  CrusoeCloud  Global      H100  Reserved-1y       1h          1   \n",
      "2  CrusoeCloud  Global      H100  Reserved-3y       1h          1   \n",
      "3  CrusoeCloud  Global      H100  Reserved-6m       1h          1   \n",
      "4  CrusoeCloud  Global      H200    On-Demand       1h          1   \n",
      "5  CrusoeCloud  Global      H200  Reserved-1y       1h          1   \n",
      "6  CrusoeCloud  Global      H200  Reserved-3y       1h          1   \n",
      "7  CrusoeCloud  Global      H200  Reserved-6m       1h          1   \n",
      "\n",
      "   price_hourly_usd                           source_url      fetched_at_utc  \n",
      "0              3.90  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "1              2.93  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "2              2.54  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "3              3.12  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "4              4.29  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "5              3.22  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "6              2.79  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "7              3.43  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n"
     ]
    }
   ],
   "source": [
    "# ============ Crusoe Cloud (async) — table scrape → slim schema + history ============\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---------- slim schema + storage helpers (same as other providers) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{slug}.csv\")\n",
    "    # history append+dedupe\n",
    "    hist = HIST_DIR / f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "        .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "        .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                 \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "        .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest per gpu/type/region/duration\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- parsing helpers ----------\n",
    "PRICE_RE = re.compile(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "def _parse_price(cell_text: str):\n",
    "    if not cell_text: return None\n",
    "    m = PRICE_RE.search(cell_text.replace(\",\", \"\"))\n",
    "    if not m:\n",
    "        # fallback: plain $N.NN without explicit /hr\n",
    "        m2 = re.search(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\b\", cell_text.replace(\",\", \"\"))\n",
    "        return float(m2.group(1)) if m2 else None\n",
    "    return float(m.group(1))\n",
    "\n",
    "def _is_h_model(text: str) -> bool:\n",
    "    t = text.upper()\n",
    "    return (\"H100\" in t) or (\"H200\" in t)\n",
    "\n",
    "def _model_from(text: str) -> str:\n",
    "    return \"H100\" if \"H100\" in text.upper() else \"H200\"\n",
    "\n",
    "# ---------- scraper ----------\n",
    "async def scrape_crusoe_table() -> pd.DataFrame:\n",
    "    url = \"https://www.crusoe.ai/cloud/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help render\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows_out = []\n",
    "\n",
    "    # Find all table rows; filter to those that mention H100/H200\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        tds = [c.get_text(\" \", strip=True) for c in tr.find_all(\"td\")]\n",
    "        if not tds or not any(_is_h_model(c) for c in tds):\n",
    "            continue\n",
    "\n",
    "        model = _model_from(\" \".join(tds))\n",
    "\n",
    "        # Try to map columns conservatively:\n",
    "        # Common layout: [Model, On-Demand, Spot?, Reserved 6m, Reserved 1y, Reserved 3y, ...]\n",
    "        # We’ll grab by position if present, else try to read by header alignment.\n",
    "        on_demand = _parse_price(tds[1]) if len(tds) > 1 else None\n",
    "        res_6m    = _parse_price(tds[3]) if len(tds) > 3 else None\n",
    "        res_1y    = _parse_price(tds[4]) if len(tds) > 4 else None\n",
    "        res_3y    = _parse_price(tds[5]) if len(tds) > 5 else None\n",
    "\n",
    "        # Build normalised slim rows\n",
    "        if on_demand is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": on_demand,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_6m is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-6m\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_6m,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_1y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-1y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_1y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_3y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-3y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_3y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe & sanity\n",
    "    df = (df.sort_values([\"gpu_model\",\"type\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"type\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # plausible hourly band\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---------- runner that works in scripts & notebooks ----------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_crusoe = arun(scrape_crusoe_table())\n",
    "latest_crusoe = _save_provider(df_crusoe, \"crusoecloud\")\n",
    "print(df_crusoe.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   provider  region gpu_model       instance_type  gpu_count  \\\n",
      "0  OVHcloud  Global      H100  public-cloud/Price          2   \n",
      "1  OVHcloud  Global      H100  public-cloud/Price          4   \n",
      "\n",
      "   price_hourly_usd_instance  price_hourly_usd_per_gpu price_reserved_usd  \\\n",
      "0                       5.98                    2.9900               None   \n",
      "1                      11.97                    2.9925               None   \n",
      "\n",
      "  reserved_duration                   timestamp  \n",
      "0              None  2025-09-04T15:18:12.093698  \n",
      "1              None  2025-09-04T15:18:12.093773  \n"
     ]
    }
   ],
   "source": [
    "# OVHcloud H100/H200 — get the *correct per-GPU hourly price* from the public prices table\n",
    "# - Ties each $…/hour to the same row as H100/H200\n",
    "# - Extracts the GPU count from the row (1×/2×/4×/8× or “… GPUs”)\n",
    "# - per_gpu = instance_price / parsed_gpu_count  (NO 8× assumption)\n",
    "# - Returns both instance price and per-GPU price\n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def timestamp(): return datetime.utcnow().isoformat()\n",
    "\n",
    "USD_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "# GPU count detectors (row-level + cell/variant-level)\n",
    "COUNT_PATS = [\n",
    "    re.compile(r\"(\\d+)\\s*[×x]\\s*(?:NVIDIA\\s*)?(H100|H200)\\b\", re.I),  # \"8× H100\"\n",
    "    re.compile(r\"(H100|H200)\\s*[×x]\\s*(\\d+)\\b\", re.I),               # \"H100 × 8\"\n",
    "    re.compile(r\"(\\d+)\\s*(?:GPU|GPUs)\\b\", re.I),                     # \"8 GPUs\"\n",
    "    re.compile(r\"\\b(\\d+)\\s*[×x]\\b\", re.I),                           # \"4x\"\n",
    "]\n",
    "\n",
    "def _parse_count(text: str):\n",
    "    for pat in COUNT_PATS:\n",
    "        m = pat.search(text)\n",
    "        if not m: \n",
    "            continue\n",
    "        for g in m.groups():\n",
    "            if g and g.isdigit():\n",
    "                n = int(g)\n",
    "                if 1 <= n <= 16:\n",
    "                    return n\n",
    "    return None\n",
    "\n",
    "def scrape_ovhcloud_correct(url=\"https://www.ovhcloud.com/en/public-cloud/prices/\") -> pd.DataFrame:\n",
    "    html = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=60).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "    for table in soup.select(\"table\"):\n",
    "        # headers so we can label which column the number came from\n",
    "        headers = [th.get_text(\" \", strip=True) for th in table.select(\"thead th\")]\n",
    "        if not headers:\n",
    "            first = table.find(\"tr\")\n",
    "            if first:\n",
    "                headers = [td.get_text(\" \", strip=True) for td in first.find_all([\"th\",\"td\"])]\n",
    "\n",
    "        body_rows = table.select(\"tbody tr\") or table.select(\"tr\")\n",
    "        for tr in body_rows:\n",
    "            tds = tr.find_all(\"td\")\n",
    "            if not tds: \n",
    "                continue\n",
    "            cells = [td.get_text(\" \", strip=True) for td in tds]\n",
    "            row_txt = \" \".join(cells)\n",
    "            up = row_txt.upper()\n",
    "            if (\"H100\" not in up) and (\"H200\" not in up):\n",
    "                continue\n",
    "\n",
    "            model = \"H100\" if \"H100\" in up else \"H200\"\n",
    "            row_count = _parse_count(row_txt)\n",
    "\n",
    "            for idx, (td, cell) in enumerate(zip(tds, cells)):\n",
    "                m = USD_HOURLY.search(cell)\n",
    "                if not m:\n",
    "                    continue\n",
    "                instance_price = float(m.group(1))\n",
    "\n",
    "                # try counts in cell and header/variant too\n",
    "                var = headers[idx] if idx < len(headers) and headers else f\"col_{idx+1}\"\n",
    "                count = (\n",
    "                    _parse_count(cell) or\n",
    "                    _parse_count(var)  or\n",
    "                    row_count\n",
    "                )\n",
    "                if count is None:\n",
    "                    # if we can't prove node size, skip (prevents wrong divide)\n",
    "                    continue\n",
    "\n",
    "                per_gpu = instance_price / count\n",
    "                if not (0.25 <= per_gpu <= 20.0):\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"OVHcloud\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": model,\n",
    "                    \"instance_type\": f\"public-cloud/{var}\",\n",
    "                    \"gpu_count\": int(count),\n",
    "                    \"price_hourly_usd_instance\": round(instance_price, 4),\n",
    "                    \"price_hourly_usd_per_gpu\": round(per_gpu, 4),\n",
    "                    \"price_reserved_usd\": None,\n",
    "                    \"reserved_duration\": None,\n",
    "                    \"timestamp\": timestamp(),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = (df.sort_values([\"gpu_model\",\"price_hourly_usd_instance\"])\n",
    "                .drop_duplicates(subset=[\"gpu_model\",\"instance_type\",\"price_hourly_usd_instance\"], keep=\"last\")\n",
    "                .reset_index(drop=True))\n",
    "    return df\n",
    "\n",
    "# Example\n",
    "df_ovh = scrape_ovhcloud_correct()\n",
    "print(df_ovh.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived saved:\n",
      "-  docs/data/derived/market_index.csv \n",
      "-  docs/data/derived/provider_scores_latest.csv \n",
      "-  docs/data/derived/price_iq_latest.json\n",
      "            provider  region gpu_model       type  price_hourly_usd  price_score\n",
      "             Vast.ai  Global      H100  On-Demand              1.25        150.0\n",
      "              Nebius  Global      H100  On-Demand              2.00        112.2\n",
      "          Paperspace  Global      H100  On-Demand              2.24        100.2\n",
      "          TensorDock  Global      H100  On-Demand              2.25         99.8\n",
      " Hydra Host (Brokkr)  Global      H100  On-Demand              2.30         97.6\n",
      "         CrusoeCloud  Global      H100  On-Demand              3.90         57.6\n",
      "         VoltagePark      US      H100  On-Demand              1.99        150.0\n",
      "           CoreWeave      US      H100  On-Demand             20.00         55.0\n",
      "              Nebius  Global      H200  On-Demand              2.30        107.6\n",
      "           Shadeform  Global      H200  On-Demand              2.45        101.0\n",
      " Hydra Host (Brokkr)  Global      H200  On-Demand              2.50         99.0\n",
      "         CrusoeCloud  Global      H200  On-Demand              4.29         57.7\n"
     ]
    }
   ],
   "source": [
    "# === Aggregate all providers → baseline market → score → save derived CSVs ===\n",
    "# Input: docs/data/latest/*_latest.csv (slim schema)\n",
    "# Output: docs/data/derived/market_index.csv, provider_scores_latest.csv, price_iq_latest.json\n",
    "\n",
    "import pandas as pd, numpy as np, json, tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Paths\n",
    "BASE = Path(\"docs/data\")\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "DERIVED_DIR = BASE / \"derived\"\n",
    "for d in (LATEST_DIR, DERIVED_DIR, BASE / \"history\", BASE / \"snapshots\"):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso(): \n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path) -> Path:\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "# ---------------- 1) Load & unify (build df FIRST) ----------------\n",
    "frames = []\n",
    "for p in sorted(LATEST_DIR.glob(\"*_latest.csv\")):\n",
    "    try:\n",
    "        d = pd.read_csv(p)\n",
    "        for c in SLIM_COLS:\n",
    "            if c not in d.columns: d[c] = None\n",
    "        frames.append(d[SLIM_COLS])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load {p.name}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No latest provider CSVs found in docs/data/latest/\")\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Basic hygiene\n",
    "raw[\"gpu_model\"] = raw[\"gpu_model\"].astype(str).str.upper().str.strip()\n",
    "raw[\"region\"]    = raw[\"region\"].astype(str).replace({\"nan\": None}).fillna(\"Global\").str.strip()\n",
    "raw[\"type\"]      = raw[\"type\"].astype(str).replace({\"nan\": None}).fillna(\"On-Demand\").str.strip()\n",
    "raw[\"duration\"]  = raw[\"duration\"].astype(str).replace({\"nan\": None}).fillna(\"1h\").str.strip()\n",
    "raw[\"gpu_count\"] = pd.to_numeric(raw[\"gpu_count\"], errors=\"coerce\")\n",
    "raw[\"price_hourly_usd\"] = pd.to_numeric(raw[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "raw[\"fetched_at_utc\"]   = pd.to_datetime(raw[\"fetched_at_utc\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Only H100/H200 and sane prices\n",
    "df = raw[\n",
    "    raw[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "].copy()\n",
    "df = df[(df[\"price_hourly_usd\"] > 0.05) & (df[\"price_hourly_usd\"] < 200)]\n",
    "\n",
    "# De-dup (latest wins)\n",
    "dedupe_keys = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\"]\n",
    "df = (df.sort_values(\"fetched_at_utc\")\n",
    "        .drop_duplicates(subset=dedupe_keys, keep=\"last\")\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# ---------------- 2) QA (now that df exists) ----------------\n",
    "qa = df.copy()\n",
    "\n",
    "# OVH must come from the correct scraper (per-GPU ~ $2–$4 for H100)\n",
    "qa = qa[~(\n",
    "    (qa[\"provider\"].eq(\"OVHcloud\")) &\n",
    "    (qa[\"gpu_model\"].eq(\"H100\")) &\n",
    "    ((qa[\"price_hourly_usd\"] < 2.0) | (qa[\"price_hourly_usd\"] > 4.0))\n",
    ")]\n",
    "\n",
    "# Shadeform: drop obvious marketing/outlier rows\n",
    "qa = qa[~(\n",
    "    (qa[\"provider\"].eq(\"Shadeform\")) &\n",
    "    (qa[\"price_hourly_usd\"] > 6.0)\n",
    ")]\n",
    "\n",
    "if qa.empty:\n",
    "    raise RuntimeError(\"No rows after QA; check scrapers or relax QA filters.\")\n",
    "\n",
    "# ---------------- 3) Market baselines from On-Demand only ----------------\n",
    "GROUP = [\"gpu_model\", \"region\"]\n",
    "od = qa[qa[\"type\"].eq(\"On-Demand\")].copy()\n",
    "if od.empty:\n",
    "    raise RuntimeError(\"No On-Demand rows to build market baselines.\")\n",
    "\n",
    "g = od.groupby(GROUP)[\"price_hourly_usd\"]\n",
    "market = pd.DataFrame({\n",
    "    \"market_count\": g.size(),\n",
    "    \"market_median\": g.median(),\n",
    "    \"market_mean\": g.mean(),\n",
    "    \"market_p25\": g.quantile(0.25),\n",
    "    \"market_p75\": g.quantile(0.75),\n",
    "}).reset_index()\n",
    "market[\"market_iqr\"] = market[\"market_p75\"] - market[\"market_p25\"]\n",
    "market[\"asof_utc\"] = _now_iso()\n",
    "\n",
    "# ---------------- 4) Score all rows vs market ----------------\n",
    "scored = qa.merge(market, on=GROUP, how=\"left\")\n",
    "scored[\"premium_vs_median\"] = (scored[\"price_hourly_usd\"] - scored[\"market_median\"]) / scored[\"market_median\"]\n",
    "iqr = scored[\"market_iqr\"].replace(0, np.nan)\n",
    "scored[\"is_outlier\"] = (\n",
    "    (scored[\"price_hourly_usd\"] < scored[\"market_p25\"] - 1.5*iqr) |\n",
    "    (scored[\"price_hourly_usd\"] > scored[\"market_p75\"] + 1.5*iqr)\n",
    ")\n",
    "# Price-IQ score (100 = at median; higher = cheaper)\n",
    "scored[\"price_score\"] = (scored[\"market_median\"] / scored[\"price_hourly_usd\"] * 100).clip(50, 150).round(1)\n",
    "\n",
    "# ---------------- 5) Save derived artifacts ----------------\n",
    "market_path = _safe_to_csv(market, DERIVED_DIR / \"market_index.csv\")\n",
    "scores_path = _safe_to_csv(scored, DERIVED_DIR / \"provider_scores_latest.csv\")\n",
    "\n",
    "# Lightweight JSON for dashboard (On-Demand leaderboard)\n",
    "leaderboard = (scored[scored[\"type\"].eq(\"On-Demand\")]\n",
    "               .sort_values([\"gpu_model\",\"region\",\"price_score\"], ascending=[True, True, False]))\n",
    "(lb := leaderboard[[\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"price_score\",\"premium_vs_median\",\"source_url\"]].copy()) \\\n",
    "    .assign(premium_vs_median=lambda x: x[\"premium_vs_median\"].round(4)) \\\n",
    "    .to_json(DERIVED_DIR / \"price_iq_latest.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(\"Derived saved:\\n- \", market_path, \"\\n- \", scores_path, \"\\n- \", DERIVED_DIR / \"price_iq_latest.json\")\n",
    "print(leaderboard.head(12)[[\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"price_score\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: docs/data/derived/price_iq_explainable.csv and price_iq_config.json\n",
      "            provider  region gpu_model         type  price_hourly_usd  market_median  price_component  perf  avail  sla  price_iq  premium_vs_median                                                    source_url\n",
      "             Vast.ai  Global      H100    On-Demand              1.25          2.245         1.796000   1.0    1.0  1.0     150.0            -0.4432                            https://vast.ai/products/gpu-cloud\n",
      "              Nebius  Global      H100    On-Demand              2.00          2.245         1.122500   1.0    1.0  1.0     112.2            -0.1091                                     https://nebius.com/prices\n",
      "          Paperspace  Global      H100    On-Demand              2.24          2.245         1.002232   1.0    1.0  1.0     100.2            -0.0022                            https://www.paperspace.com/pricing\n",
      "          TensorDock  Global      H100    On-Demand              2.25          2.245         0.997778   1.0    1.0  1.0      99.8             0.0022                               https://tensordock.com/gpu-h100\n",
      " Hydra Host (Brokkr)  Global      H100    On-Demand              2.30          2.245         0.976087   1.0    1.0  1.0      97.6             0.0245                        https://brokkr.hydrahost.com/inventory\n",
      "         CrusoeCloud  Global      H100  Reserved-3y              2.54          2.245         0.883858   1.0    1.0  1.0      88.4             0.1314                           https://www.crusoe.ai/cloud/pricing\n",
      "         CrusoeCloud  Global      H100  Reserved-1y              2.93          2.245         0.766212   1.0    1.0  1.0      76.6             0.3051                           https://www.crusoe.ai/cloud/pricing\n",
      "         CrusoeCloud  Global      H100  Reserved-6m              3.12          2.245         0.719551   1.0    1.0  1.0      72.0             0.3898                           https://www.crusoe.ai/cloud/pricing\n",
      "         CrusoeCloud  Global      H100    On-Demand              3.90          2.245         0.575641   1.0    1.0  1.0      57.6             0.7372                           https://www.crusoe.ai/cloud/pricing\n",
      "         VoltagePark      US      H100    On-Demand              1.99         10.995         5.525126   1.0    1.0  1.0     150.0            -0.8190  https://dashboard.voltagepark.com/order/configure-deployment\n",
      "           CoreWeave      US      H100    On-Demand             20.00         10.995         0.549750   1.0    1.0  1.0      55.0             0.8190                             https://www.coreweave.com/pricing\n",
      "              Nebius  Global      H200    On-Demand              2.30          2.475         1.076087   1.0    1.0  1.0     107.6            -0.0707                                     https://nebius.com/prices\n"
     ]
    }
   ],
   "source": [
    "# ====================== PRICE IQ FORMULA (Explainable, Weighted) ======================\n",
    "# Input: docs/data/derived/provider_scores_latest.csv (created by the aggregator)\n",
    "# Output: docs/data/derived/price_iq_explainable.csv  +  docs/data/derived/price_iq_config.json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd, numpy as np, json, tempfile, re\n",
    "\n",
    "DERIVED_DIR = Path(\"docs/data/derived\")\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- 1) Load scored rows (has market_* columns already) ----\n",
    "scores_path = DERIVED_DIR / \"provider_scores_latest.csv\"\n",
    "if not scores_path.exists():\n",
    "    raise SystemExit(f\"Missing {scores_path}. Run the aggregator cell first.\")\n",
    "\n",
    "df = pd.read_csv(scores_path)\n",
    "\n",
    "# ---- 2) Minimal features we need ----\n",
    "MUST = [\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"market_median\",\"source_url\"]\n",
    "for c in MUST:\n",
    "    if c not in df.columns:\n",
    "        raise SystemExit(f\"Missing column in provider_scores_latest.csv: {c}\")\n",
    "\n",
    "# Keep just the latest per (provider, region, model, type)\n",
    "if \"fetched_at_utc\" in df.columns:\n",
    "    df = df.sort_values(\"fetched_at_utc\")\n",
    "df = df.drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\"], keep=\"last\")\n",
    "\n",
    "# ---- 3) Optional signals (perf/avail/sla) — default to neutral 1.0 ----\n",
    "def _infer_perf(name:str) -> float:\n",
    "    n = (str(name) or \"\").upper()\n",
    "    if \"SXM\" in n: return 1.00\n",
    "    if \"NVL\" in n: return 1.02\n",
    "    if \"PCIE\" in n: return 0.95\n",
    "    return 1.00\n",
    "\n",
    "def _infer_avail(_): return 1.00\n",
    "def _infer_sla(_):   return 1.00\n",
    "\n",
    "df[\"perf\"]  = df[\"gpu_model\"].map(_infer_perf)\n",
    "df[\"avail\"] = df[\"provider\"].map(_infer_avail)\n",
    "df[\"sla\"]   = df[\"provider\"].map(_infer_sla)\n",
    "\n",
    "# ---- 4) Price component (primary signal) ----\n",
    "# price_component = market_median / price_hourly_usd\n",
    "#   1.0 = at market median, >1 cheaper than median, <1 pricier than median\n",
    "df[\"price_component\"] = (df[\"market_median\"] / df[\"price_hourly_usd\"]).clip(0.01, 10.0)\n",
    "\n",
    "# ---- 5) Weights (edit here or surface in UI later) ----\n",
    "WEIGHTS = {\n",
    "    \"price\": 1.00,   # strongest driver (v1: price-only)\n",
    "    \"perf\":  0.00,   # enable (e.g., 0.10) to reward SXM/NVL over PCIe (heuristic)\n",
    "    \"avail\": 0.00,   # future capacity/queue signal\n",
    "    \"sla\":   0.00,   # future reliability/SLA signal\n",
    "}\n",
    "w_sum = sum(WEIGHTS.values()) or 1.0\n",
    "W = {k: v / w_sum for k, v in WEIGHTS.items()}\n",
    "\n",
    "# ---- 6) Price-IQ score ----\n",
    "# IQ = 100 * ( w_price * (M/P) + w_perf*perf + w_avail*avail + w_sla*sla ), clipped to [50,150]\n",
    "raw = (W[\"price\"]*df[\"price_component\"] +\n",
    "       W.get(\"perf\",0)*df[\"perf\"] +\n",
    "       W.get(\"avail\",0)*df[\"avail\"] +\n",
    "       W.get(\"sla\",0)*df[\"sla\"])\n",
    "df[\"price_iq\"] = (100 * raw).clip(50, 150).round(1)\n",
    "\n",
    "# Also expose premium vs median for UI copy\n",
    "df[\"premium_vs_median\"] = ((df[\"price_hourly_usd\"] - df[\"market_median\"]) / df[\"market_median\"]).round(4)\n",
    "\n",
    "# ---- 7) Save explainable table + config ----\n",
    "keep = [\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"market_median\",\n",
    "        \"price_component\",\"perf\",\"avail\",\"sla\",\"price_iq\",\"premium_vs_median\",\"source_url\"]\n",
    "explain = (df[keep]\n",
    "           .sort_values([\"gpu_model\",\"region\",\"price_iq\"], ascending=[True, True, False])\n",
    "           .reset_index(drop=True))\n",
    "\n",
    "out_csv = DERIVED_DIR / \"price_iq_explainable.csv\"\n",
    "explain.to_csv(out_csv, index=False)\n",
    "\n",
    "cfg = {\n",
    "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"weights\": W,\n",
    "    \"formula\": \"IQ = 100 * (w_price*(M/P) + w_perf*perf + w_avail*avail + w_sla*sla); clipped [50,150]\",\n",
    "    \"notes\": {\n",
    "        \"M\": \"market_median for same gpu_model & region (On-Demand baseline)\",\n",
    "        \"P\": \"provider per-GPU hourly price\",\n",
    "        \"perf/avail/sla\": \"neutral 1.0 by default; plug real signals when available\"\n",
    "    }\n",
    "}\n",
    "with open(DERIVED_DIR / \"price_iq_config.json\",\"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", out_csv, \"and price_iq_config.json\")\n",
    "print(explain.head(12).to_string(index=False))\n",
    "# ====================== /PRICE IQ FORMULA ======================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> docs/data/derived/roi_comparison_latest.csv and docs/data/derived/roi_comparison_latest.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>market_median</th>\n",
       "      <th>gpu_hours_eff</th>\n",
       "      <th>infra_cost</th>\n",
       "      <th>extra_fees</th>\n",
       "      <th>egress_cost</th>\n",
       "      <th>storage_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>delta_vs_cheapest</th>\n",
       "      <th>savings_vs_median</th>\n",
       "      <th>expected_revenue</th>\n",
       "      <th>gross_margin</th>\n",
       "      <th>roi_pct</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7164.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.99</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>5328.0</td>\n",
       "      <td>64836.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100</td>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>7128.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://tensordock.com/gpu-h100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H100</td>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>-396.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H100</td>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>28080.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28080.0</td>\n",
       "      <td>19080.0</td>\n",
       "      <td>-11916.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H100</td>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>-64836.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.coreweave.com/pricing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_model             provider  region       type  price_hourly_usd  \\\n",
       "0      H100              Vast.ai  Global  On-Demand              1.25   \n",
       "1      H100          VoltagePark      US  On-Demand              1.99   \n",
       "2      H100               Nebius  Global  On-Demand              2.00   \n",
       "3      H100           Paperspace  Global  On-Demand              2.24   \n",
       "4      H100           TensorDock  Global  On-Demand              2.25   \n",
       "5      H100  Hydra Host (Brokkr)  Global  On-Demand              2.30   \n",
       "6      H100          CrusoeCloud  Global  On-Demand              3.90   \n",
       "7      H100            CoreWeave      US  On-Demand             20.00   \n",
       "\n",
       "   market_median  gpu_hours_eff  infra_cost  extra_fees  egress_cost  \\\n",
       "0          2.245         7200.0      9000.0         0.0          0.0   \n",
       "1         10.995         7200.0     14328.0         0.0          0.0   \n",
       "2          2.245         7200.0     14400.0         0.0          0.0   \n",
       "3          2.245         7200.0     16128.0         0.0          0.0   \n",
       "4          2.245         7200.0     16200.0         0.0          0.0   \n",
       "5          2.245         7200.0     16560.0         0.0          0.0   \n",
       "6          2.245         7200.0     28080.0         0.0          0.0   \n",
       "7         10.995         7200.0    144000.0         0.0          0.0   \n",
       "\n",
       "   storage_cost  total_cost  delta_vs_cheapest  savings_vs_median  \\\n",
       "0           0.0      9000.0                0.0             7164.0   \n",
       "1           0.0     14328.0             5328.0            64836.0   \n",
       "2           0.0     14400.0             5400.0             1764.0   \n",
       "3           0.0     16128.0             7128.0               36.0   \n",
       "4           0.0     16200.0             7200.0              -36.0   \n",
       "5           0.0     16560.0             7560.0             -396.0   \n",
       "6           0.0     28080.0            19080.0           -11916.0   \n",
       "7           0.0    144000.0           135000.0           -64836.0   \n",
       "\n",
       "   expected_revenue  gross_margin  roi_pct  \\\n",
       "0               NaN           NaN      NaN   \n",
       "1               NaN           NaN      NaN   \n",
       "2               NaN           NaN      NaN   \n",
       "3               NaN           NaN      NaN   \n",
       "4               NaN           NaN      NaN   \n",
       "5               NaN           NaN      NaN   \n",
       "6               NaN           NaN      NaN   \n",
       "7               NaN           NaN      NaN   \n",
       "\n",
       "                                          source_url  \n",
       "0                 https://vast.ai/products/gpu-cloud  \n",
       "1  https://dashboard.voltagepark.com/order/config...  \n",
       "2                          https://nebius.com/prices  \n",
       "3                 https://www.paperspace.com/pricing  \n",
       "4                    https://tensordock.com/gpu-h100  \n",
       "5             https://brokkr.hydrahost.com/inventory  \n",
       "6                https://www.crusoe.ai/cloud/pricing  \n",
       "7                  https://www.coreweave.com/pricing  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========================== ROI / EARNINGS CALCULATOR (fixed) ==========================\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "DERIVED = Path(\"docs/data/derived\"); DERIVED.mkdir(parents=True, exist_ok=True)\n",
    "scores_path = DERIVED / \"provider_scores_latest.csv\"\n",
    "if not scores_path.exists():\n",
    "    raise SystemExit(\"Missing provider_scores_latest.csv. Run the aggregator first.\")\n",
    "scores = pd.read_csv(scores_path)\n",
    "\n",
    "def roi_compare(\n",
    "    gpu_model=\"H100\",\n",
    "    region=None,                     # e.g. \"Global\"; None = all regions\n",
    "    type_filter=(\"On-Demand\",),      # include reserved types if desired\n",
    "    gpus=8,\n",
    "    hours=1000,\n",
    "    utilization=0.9,                 # 0..1\n",
    "    extra_fees_perc=0.00,            # platform/markup %, e.g. 0.05 = +5%\n",
    "    egress_gb=0.0,\n",
    "    egress_cost_per_gb=0.0,\n",
    "    storage_cost=0.0,                # flat $\n",
    "    revenue_per_unit=None,           # optional revenue model\n",
    "    units_per_gpu_hour=None          # optional throughput\n",
    ") -> pd.DataFrame:\n",
    "    df = scores.copy()\n",
    "    df = df[df[\"gpu_model\"].str.upper().eq(gpu_model.upper())]\n",
    "    if region is not None:\n",
    "        df = df[df[\"region\"].eq(region)]\n",
    "    if type_filter:\n",
    "        df = df[df[\"type\"].isin(type_filter)]\n",
    "    df = df.dropna(subset=[\"price_hourly_usd\",\"market_median\"])\n",
    "\n",
    "    if df.empty:\n",
    "        cols = [\"gpu_model\",\"provider\",\"region\",\"type\",\"price_hourly_usd\",\"market_median\",\n",
    "                \"gpu_hours_eff\",\"infra_cost\",\"extra_fees\",\"egress_cost\",\"storage_cost\",\n",
    "                \"total_cost\",\"delta_vs_cheapest\",\"savings_vs_median\",\n",
    "                \"expected_revenue\",\"gross_margin\",\"roi_pct\",\"source_url\"]\n",
    "        return pd.DataFrame(columns=cols)\n",
    "\n",
    "    # Effective GPU-hours actually used\n",
    "    gpu_hours_eff = float(gpus) * float(hours) * max(0.0, min(1.0, float(utilization)))\n",
    "    df = df.assign(gpu_hours_eff=gpu_hours_eff)\n",
    "\n",
    "    # Cost breakdown\n",
    "    df[\"infra_cost\"]   = (df[\"price_hourly_usd\"] * df[\"gpu_hours_eff\"]).round(2)\n",
    "    df[\"extra_fees\"]   = (df[\"infra_cost\"] * float(extra_fees_perc)).round(2)\n",
    "    df[\"egress_cost\"]  = float(egress_gb) * float(egress_cost_per_gb)\n",
    "    df[\"storage_cost\"] = float(storage_cost)\n",
    "    df[\"total_cost\"]   = (df[\"infra_cost\"] + df[\"extra_fees\"] + df[\"egress_cost\"] + df[\"storage_cost\"]).round(2)\n",
    "\n",
    "    # Benchmarks\n",
    "    cheapest_total = df[\"total_cost\"].min()\n",
    "    df[\"delta_vs_cheapest\"] = (df[\"total_cost\"] - cheapest_total).round(2)      # >= 0\n",
    "    df[\"median_cost_same_hours\"] = (df[\"market_median\"] * df[\"gpu_hours_eff\"]).round(2)\n",
    "    df[\"savings_vs_median\"] = (df[\"median_cost_same_hours\"] - df[\"total_cost\"]).round(2)\n",
    "\n",
    "    # Optional earnings / ROI\n",
    "    if (revenue_per_unit is not None) and (units_per_gpu_hour is not None):\n",
    "        exp_rev = float(revenue_per_unit) * float(units_per_gpu_hour) * df[\"gpu_hours_eff\"]\n",
    "        df[\"expected_revenue\"] = exp_rev.round(2)\n",
    "        df[\"gross_margin\"] = (df[\"expected_revenue\"] - df[\"total_cost\"]).round(2)\n",
    "        df[\"roi_pct\"] = np.where(df[\"total_cost\"] > 0,\n",
    "                                 (df[\"gross_margin\"] / df[\"total_cost\"]) * 100, np.nan).round(2)\n",
    "    else:\n",
    "        df[\"expected_revenue\"] = np.nan\n",
    "        df[\"gross_margin\"] = np.nan\n",
    "        df[\"roi_pct\"] = np.nan\n",
    "\n",
    "    # keep gpu_model in output\n",
    "    keep = [\"gpu_model\",\"provider\",\"region\",\"type\",\"price_hourly_usd\",\"market_median\",\n",
    "            \"gpu_hours_eff\",\"infra_cost\",\"extra_fees\",\"egress_cost\",\"storage_cost\",\n",
    "            \"total_cost\",\"delta_vs_cheapest\",\"savings_vs_median\",\n",
    "            \"expected_revenue\",\"gross_margin\",\"roi_pct\",\"source_url\"]\n",
    "\n",
    "    out = (df[keep]\n",
    "           .sort_values([\"gpu_model\",\"total_cost\",\"provider\"], ascending=[True, True, True])\n",
    "           .reset_index(drop=True))\n",
    "    return out\n",
    "\n",
    "# Example run (edit as needed)\n",
    "roi_table = roi_compare(\n",
    "    gpu_model=\"H100\", region=None, type_filter=(\"On-Demand\",),\n",
    "    gpus=8, hours=1000, utilization=0.90,\n",
    "    extra_fees_perc=0.00, egress_gb=0, egress_cost_per_gb=0.0, storage_cost=0.0,\n",
    "    # revenue_per_unit=0.002, units_per_gpu_hour=50000\n",
    ")\n",
    "\n",
    "# Save once\n",
    "csv_out = DERIVED / \"roi_comparison_latest.csv\"\n",
    "json_out = DERIVED / \"roi_comparison_latest.json\"\n",
    "roi_table.to_csv(csv_out, index=False)\n",
    "with open(json_out, \"w\") as f:\n",
    "    json.dump(roi_table.to_dict(orient=\"records\"), f, indent=2)\n",
    "print(\"Saved ->\", csv_out, \"and\", json_out)\n",
    "\n",
    "roi_table.head(10)\n",
    "# ======================== /ROI / EARNINGS CALCULATOR (fixed) ==========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>market_median</th>\n",
       "      <th>gpu_hours_eff</th>\n",
       "      <th>infra_cost</th>\n",
       "      <th>extra_fees</th>\n",
       "      <th>egress_cost</th>\n",
       "      <th>storage_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>delta_vs_cheapest</th>\n",
       "      <th>savings_vs_median</th>\n",
       "      <th>expected_revenue</th>\n",
       "      <th>gross_margin</th>\n",
       "      <th>roi_pct</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7164.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>711000.0</td>\n",
       "      <td>7900.00</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.99</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>5328.0</td>\n",
       "      <td>64836.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>705672.0</td>\n",
       "      <td>4925.13</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100</td>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>705600.0</td>\n",
       "      <td>4900.00</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>7128.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>703872.0</td>\n",
       "      <td>4364.29</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>703800.0</td>\n",
       "      <td>4344.44</td>\n",
       "      <td>https://tensordock.com/gpu-h100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H100</td>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>-396.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>703440.0</td>\n",
       "      <td>4247.83</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H100</td>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>28080.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28080.0</td>\n",
       "      <td>19080.0</td>\n",
       "      <td>-11916.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>691920.0</td>\n",
       "      <td>2464.10</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H100</td>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>-64836.0</td>\n",
       "      <td>720000.0</td>\n",
       "      <td>576000.0</td>\n",
       "      <td>400.00</td>\n",
       "      <td>https://www.coreweave.com/pricing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_model             provider  region       type  price_hourly_usd  \\\n",
       "0      H100              Vast.ai  Global  On-Demand              1.25   \n",
       "1      H100          VoltagePark      US  On-Demand              1.99   \n",
       "2      H100               Nebius  Global  On-Demand              2.00   \n",
       "3      H100           Paperspace  Global  On-Demand              2.24   \n",
       "4      H100           TensorDock  Global  On-Demand              2.25   \n",
       "5      H100  Hydra Host (Brokkr)  Global  On-Demand              2.30   \n",
       "6      H100          CrusoeCloud  Global  On-Demand              3.90   \n",
       "7      H100            CoreWeave      US  On-Demand             20.00   \n",
       "\n",
       "   market_median  gpu_hours_eff  infra_cost  extra_fees  egress_cost  \\\n",
       "0          2.245         7200.0      9000.0         0.0          0.0   \n",
       "1         10.995         7200.0     14328.0         0.0          0.0   \n",
       "2          2.245         7200.0     14400.0         0.0          0.0   \n",
       "3          2.245         7200.0     16128.0         0.0          0.0   \n",
       "4          2.245         7200.0     16200.0         0.0          0.0   \n",
       "5          2.245         7200.0     16560.0         0.0          0.0   \n",
       "6          2.245         7200.0     28080.0         0.0          0.0   \n",
       "7         10.995         7200.0    144000.0         0.0          0.0   \n",
       "\n",
       "   storage_cost  total_cost  delta_vs_cheapest  savings_vs_median  \\\n",
       "0           0.0      9000.0                0.0             7164.0   \n",
       "1           0.0     14328.0             5328.0            64836.0   \n",
       "2           0.0     14400.0             5400.0             1764.0   \n",
       "3           0.0     16128.0             7128.0               36.0   \n",
       "4           0.0     16200.0             7200.0              -36.0   \n",
       "5           0.0     16560.0             7560.0             -396.0   \n",
       "6           0.0     28080.0            19080.0           -11916.0   \n",
       "7           0.0    144000.0           135000.0           -64836.0   \n",
       "\n",
       "   expected_revenue  gross_margin  roi_pct  \\\n",
       "0          720000.0      711000.0  7900.00   \n",
       "1          720000.0      705672.0  4925.13   \n",
       "2          720000.0      705600.0  4900.00   \n",
       "3          720000.0      703872.0  4364.29   \n",
       "4          720000.0      703800.0  4344.44   \n",
       "5          720000.0      703440.0  4247.83   \n",
       "6          720000.0      691920.0  2464.10   \n",
       "7          720000.0      576000.0   400.00   \n",
       "\n",
       "                                          source_url  \n",
       "0                 https://vast.ai/products/gpu-cloud  \n",
       "1  https://dashboard.voltagepark.com/order/config...  \n",
       "2                          https://nebius.com/prices  \n",
       "3                 https://www.paperspace.com/pricing  \n",
       "4                    https://tensordock.com/gpu-h100  \n",
       "5             https://brokkr.hydrahost.com/inventory  \n",
       "6                https://www.crusoe.ai/cloud/pricing  \n",
       "7                  https://www.coreweave.com/pricing  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Include all regions + compute ROI\n",
    "roi_table = roi_compare(\n",
    "    gpu_model=\"H100\",\n",
    "    region=None,                       # <- include Global, US, EU, etc.\n",
    "    type_filter=(\"On-Demand\",),        # add reserved: (\"On-Demand\",\"Reserved-6m\",\"Reserved-1y\",\"Reserved-3y\")\n",
    "    gpus=8, hours=1000, utilization=0.90,\n",
    "    extra_fees_perc=0.00, egress_gb=0, egress_cost_per_gb=0.0, storage_cost=0.0,\n",
    "    revenue_per_unit=0.002,            # e.g., $/unit (token/frame/etc.)\n",
    "    units_per_gpu_hour=50000           # e.g., units produced per GPU-hour\n",
    ")\n",
    "roi_table.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FX-normalized -> provider_scores_latest.csv (price_usd)\n"
     ]
    }
   ],
   "source": [
    "# FX normalize -> add \"price_usd\" everywhere\n",
    "import pandas as pd, json, math\n",
    "from pathlib import Path\n",
    "DERIVED = Path(\"docs/data/derived\")\n",
    "\n",
    "# 1) tiny FX map (extend via ECB later)\n",
    "FX = {\"USD\":1.0,\"EUR\":1.08,\"GBP\":1.29}  # $/unit\n",
    "\n",
    "scores = pd.read_csv(DERIVED/\"provider_scores_latest.csv\")\n",
    "if \"currency\" not in scores.columns: scores[\"currency\"]=\"USD\"\n",
    "scores[\"price_usd\"] = scores.apply(lambda r: (r[\"price_hourly_usd\"] / FX.get(r[\"currency\"],1.0))\n",
    "                                   if not math.isnan(r[\"price_hourly_usd\"]) else None, axis=1)\n",
    "scores.to_csv(DERIVED/\"provider_scores_latest.csv\", index=False)\n",
    "print(\"FX-normalized -> provider_scores_latest.csv (price_usd)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> price_iq_explainable.csv  (weights: {'price': 0.9090909090909091, 'perf': 0.09090909090909091, 'avail': 0.0, 'sla': 0.0} )\n"
     ]
    }
   ],
   "source": [
    "# weights.json drives the formula; edit without touching code\n",
    "from pathlib import Path; import json, pandas as pd, numpy as np, datetime as dt\n",
    "DERIVED = Path(\"docs/data/derived\")\n",
    "cfg_path = DERIVED/\"price_iq_weights.json\"\n",
    "if not cfg_path.exists():\n",
    "    cfg_path.write_text(json.dumps({\"price\":1.0,\"perf\":0.1,\"avail\":0.0,\"sla\":0.0}, indent=2))\n",
    "W = json.loads(cfg_path.read_text())\n",
    "scores = pd.read_csv(DERIVED/\"provider_scores_latest.csv\")\n",
    "\n",
    "# perf heuristic (SXM≈1.00, NVL≈1.02, PCIe≈0.95)\n",
    "def perf(x:str)->float:\n",
    "    s=(x or \"\").upper(); \n",
    "    return 1.02 if \"NVL\" in s else 1.00 if \"SXM\" in s else 0.95 if \"PCIE\" in s else 1.0\n",
    "\n",
    "scores[\"perf\"]  = scores[\"gpu_model\"].map(perf)\n",
    "scores[\"avail\"] = 1.0\n",
    "scores[\"sla\"]   = 1.0\n",
    "scores[\"price_component\"] = (scores[\"market_median\"]/scores[\"price_hourly_usd\"]).clip(0.01,10)\n",
    "\n",
    "totw = sum(W.values()) or 1.0; Wn={k:v/totw for k,v in W.items()}\n",
    "raw = (Wn[\"price\"]*scores[\"price_component\"] + Wn[\"perf\"]*scores[\"perf\"] +\n",
    "       Wn.get(\"avail\",0)*scores[\"avail\"] + Wn.get(\"sla\",0)*scores[\"sla\"])\n",
    "scores[\"price_iq\"] = (100*raw).clip(50,150).round(1)\n",
    "scores.to_csv(DERIVED/\"price_iq_explainable.csv\", index=False)\n",
    "print(\"Saved -> price_iq_explainable.csv  (weights:\", Wn, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alerts: []\n"
     ]
    }
   ],
   "source": [
    "# anomaly scan: day-over-day delta + empty checks\n",
    "from pathlib import Path; import pandas as pd, json\n",
    "DERIVED, HISTORY = Path(\"docs/data/derived\"), Path(\"docs/data/history\")\n",
    "scores = pd.read_csv(DERIVED/\"provider_scores_latest.csv\")\n",
    "alerts=[]\n",
    "\n",
    "# empty checks\n",
    "for f in [\"market_index.csv\",\"provider_scores_latest.csv\",\"price_iq_explainable.csv\"]:\n",
    "    p=DERIVED/f\n",
    "    if not p.exists() or p.stat().st_size<200: alerts.append(f\"{f} missing/empty\")\n",
    "\n",
    "# day-over-day price jumps > 20%\n",
    "for hist in HISTORY.glob(\"*_history.csv\"):\n",
    "    try:\n",
    "        h=pd.read_csv(hist)\n",
    "        last=h.sort_values(\"fetched_at_utc\").groupby([\"provider\",\"gpu_model\"]).tail(1)\n",
    "        cur=scores.groupby([\"provider\",\"gpu_model\"])[\"price_hourly_usd\"].mean()\n",
    "        merged=last.set_index([\"provider\",\"gpu_model\"])[\"price_hourly_usd\"].to_frame(\"prev\").join(cur.to_frame(\"now\"))\n",
    "        jump=((merged[\"now\"]-merged[\"prev\"]).abs()/merged[\"prev\"]).fillna(0)\n",
    "        bad=jump[jump>0.2]\n",
    "        for (prov,model),v in bad.items():\n",
    "            alerts.append(f\"{prov} {model} price jump {v:.0%}\")\n",
    "    except Exception: pass\n",
    "\n",
    "Path(\"docs/data/derived/alerts.json\").write_text(json.dumps(alerts, indent=2))\n",
    "print(\"Alerts:\", alerts[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> roi_scenarios.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>hours</th>\n",
       "      <th>util</th>\n",
       "      <th>price</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>break_even_price</th>\n",
       "      <th>savings_vs_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>200</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>796.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>200</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1592.0</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>200</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>200</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.24</td>\n",
       "      <td>1792.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>200</td>\n",
       "      <td>0.50</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.24</td>\n",
       "      <td>17024.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.25</td>\n",
       "      <td>17100.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>-38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>2.30</td>\n",
       "      <td>17480.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>-418.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>3.90</td>\n",
       "      <td>29640.0</td>\n",
       "      <td>2.245</td>\n",
       "      <td>-12578.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.95</td>\n",
       "      <td>20.00</td>\n",
       "      <td>152000.0</td>\n",
       "      <td>10.995</td>\n",
       "      <td>-68438.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               provider  region  hours  util  price  total_cost  \\\n",
       "4               Vast.ai  Global    200  0.50   1.25      1000.0   \n",
       "3           VoltagePark      US    200  0.50   1.99      1592.0   \n",
       "1                Nebius  Global    200  0.50   2.00      1600.0   \n",
       "6            Paperspace  Global    200  0.50   2.24      1792.0   \n",
       "8            TensorDock  Global    200  0.50   2.25      1800.0   \n",
       "..                  ...     ...    ...   ...    ...         ...   \n",
       "6            Paperspace  Global   1000  0.95   2.24     17024.0   \n",
       "8            TensorDock  Global   1000  0.95   2.25     17100.0   \n",
       "9   Hydra Host (Brokkr)  Global   1000  0.95   2.30     17480.0   \n",
       "16          CrusoeCloud  Global   1000  0.95   3.90     29640.0   \n",
       "7             CoreWeave      US   1000  0.95  20.00    152000.0   \n",
       "\n",
       "    break_even_price  savings_vs_median  \n",
       "4              2.245              796.0  \n",
       "3             10.995             7204.0  \n",
       "1              2.245              196.0  \n",
       "6              2.245                4.0  \n",
       "8              2.245               -4.0  \n",
       "..               ...                ...  \n",
       "6              2.245               38.0  \n",
       "8              2.245              -38.0  \n",
       "9              2.245             -418.0  \n",
       "16             2.245           -12578.0  \n",
       "7             10.995           -68438.0  \n",
       "\n",
       "[72 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# break-even price + sensitivity table\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "DERIVED=Path(\"docs/data/derived\")\n",
    "scores=pd.read_csv(DERIVED/\"provider_scores_latest.csv\")\n",
    "\n",
    "def roi_grid(gpu_model=\"H100\", gpus=8, hours_list=(200,500,1000), util_list=(0.5,0.8,0.95)):\n",
    "    df=scores.query(\"gpu_model == @gpu_model and type == 'On-Demand'\").copy()\n",
    "    out=[]\n",
    "    for h in hours_list:\n",
    "        for u in util_list:\n",
    "            eff=gpus*h*u\n",
    "            cost=(df[\"price_hourly_usd\"]*eff)\n",
    "            be_price=(df[\"market_median\"])  # median is neutral; BE is just M for savings=0\n",
    "            out.append(pd.DataFrame({\n",
    "                \"provider\":df[\"provider\"],\"region\":df[\"region\"],\n",
    "                \"hours\":h,\"util\":u,\"price\":df[\"price_hourly_usd\"].round(3),\n",
    "                \"total_cost\":cost.round(2),\n",
    "                \"break_even_price\":be_price.round(3),\n",
    "                \"savings_vs_median\":(df[\"market_median\"]*eff - cost).round(2)\n",
    "            }))\n",
    "    grid=pd.concat(out).sort_values([\"hours\",\"util\",\"total_cost\"])\n",
    "    grid.to_csv(DERIVED/\"roi_scenarios.csv\", index=False)\n",
    "    print(\"Saved -> roi_scenarios.csv\")\n",
    "    return grid\n",
    "\n",
    "roi_grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows — scores: 19 market: 4 iq: 19 roi: 8\n",
      "found config: True {\n",
      "  \"generated_at_utc\": \"2025-09-08T10:14:01+00:00\",\n",
      "  \"weights\": {\n",
      "    \"price\": 1.0,\n",
      "    \"perf\": 0.0,\n",
      "    \"avail\": 0.0,...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>market_count</th>\n",
       "      <th>...</th>\n",
       "      <th>premium_vs_median</th>\n",
       "      <th>is_outlier</th>\n",
       "      <th>price_score</th>\n",
       "      <th>currency</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>perf</th>\n",
       "      <th>avail</th>\n",
       "      <th>sla</th>\n",
       "      <th>price_component</th>\n",
       "      <th>price_iq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>US</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>https://nebius.com/h200</td>\n",
       "      <td>2025-09-04 11:40:47+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>100.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04 11:42:58.443649+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.109131</td>\n",
       "      <td>False</td>\n",
       "      <td>112.2</td>\n",
       "      <td>USD</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.122500</td>\n",
       "      <td>111.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04 11:42:58.443649+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.070707</td>\n",
       "      <td>False</td>\n",
       "      <td>107.6</td>\n",
       "      <td>USD</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.076087</td>\n",
       "      <td>106.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "      <td>2025-09-04 12:36:26+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.819009</td>\n",
       "      <td>False</td>\n",
       "      <td>150.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>1.99</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.525126</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "      <td>2025-09-04 12:42:14+00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443207</td>\n",
       "      <td>True</td>\n",
       "      <td>150.0</td>\n",
       "      <td>USD</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.796000</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider  region gpu_model       type duration  gpu_count  \\\n",
       "0       Nebius      US      H200  On-Demand       1h        NaN   \n",
       "1       Nebius  Global      H100  On-Demand       1h        NaN   \n",
       "2       Nebius  Global      H200  On-Demand       1h        NaN   \n",
       "3  VoltagePark      US      H100  On-Demand       1h        NaN   \n",
       "4      Vast.ai  Global      H100  On-Demand       1h        NaN   \n",
       "\n",
       "   price_hourly_usd                                         source_url  \\\n",
       "0              2.30                            https://nebius.com/h200   \n",
       "1              2.00                          https://nebius.com/prices   \n",
       "2              2.30                          https://nebius.com/prices   \n",
       "3              1.99  https://dashboard.voltagepark.com/order/config...   \n",
       "4              1.25                 https://vast.ai/products/gpu-cloud   \n",
       "\n",
       "                     fetched_at_utc  market_count  ...  premium_vs_median  \\\n",
       "0         2025-09-04 11:40:47+00:00             1  ...           0.000000   \n",
       "1  2025-09-04 11:42:58.443649+00:00             6  ...          -0.109131   \n",
       "2  2025-09-04 11:42:58.443649+00:00             4  ...          -0.070707   \n",
       "3         2025-09-04 12:36:26+00:00             2  ...          -0.819009   \n",
       "4         2025-09-04 12:42:14+00:00             6  ...          -0.443207   \n",
       "\n",
       "   is_outlier  price_score  currency  price_usd perf  avail  sla  \\\n",
       "0       False        100.0       USD       2.30  1.0    1.0  1.0   \n",
       "1       False        112.2       USD       2.00  1.0    1.0  1.0   \n",
       "2       False        107.6       USD       2.30  1.0    1.0  1.0   \n",
       "3       False        150.0       USD       1.99  1.0    1.0  1.0   \n",
       "4        True        150.0       USD       1.25  1.0    1.0  1.0   \n",
       "\n",
       "   price_component price_iq  \n",
       "0         1.000000    100.0  \n",
       "1         1.122500    111.1  \n",
       "2         1.076087    106.9  \n",
       "3         5.525126    150.0  \n",
       "4         1.796000    150.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>market_median</th>\n",
       "      <th>gpu_hours_eff</th>\n",
       "      <th>infra_cost</th>\n",
       "      <th>extra_fees</th>\n",
       "      <th>egress_cost</th>\n",
       "      <th>storage_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>delta_vs_cheapest</th>\n",
       "      <th>savings_vs_median</th>\n",
       "      <th>expected_revenue</th>\n",
       "      <th>gross_margin</th>\n",
       "      <th>roi_pct</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7164.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.99</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>5328.0</td>\n",
       "      <td>64836.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100</td>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>7128.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://tensordock.com/gpu-h100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_model     provider  region       type  price_hourly_usd  market_median  \\\n",
       "0      H100      Vast.ai  Global  On-Demand              1.25          2.245   \n",
       "1      H100  VoltagePark      US  On-Demand              1.99         10.995   \n",
       "2      H100       Nebius  Global  On-Demand              2.00          2.245   \n",
       "3      H100   Paperspace  Global  On-Demand              2.24          2.245   \n",
       "4      H100   TensorDock  Global  On-Demand              2.25          2.245   \n",
       "\n",
       "   gpu_hours_eff  infra_cost  extra_fees  egress_cost  storage_cost  \\\n",
       "0         7200.0      9000.0         0.0          0.0           0.0   \n",
       "1         7200.0     14328.0         0.0          0.0           0.0   \n",
       "2         7200.0     14400.0         0.0          0.0           0.0   \n",
       "3         7200.0     16128.0         0.0          0.0           0.0   \n",
       "4         7200.0     16200.0         0.0          0.0           0.0   \n",
       "\n",
       "   total_cost  delta_vs_cheapest  savings_vs_median  expected_revenue  \\\n",
       "0      9000.0                0.0             7164.0               NaN   \n",
       "1     14328.0             5328.0            64836.0               NaN   \n",
       "2     14400.0             5400.0             1764.0               NaN   \n",
       "3     16128.0             7128.0               36.0               NaN   \n",
       "4     16200.0             7200.0              -36.0               NaN   \n",
       "\n",
       "   gross_margin  roi_pct                                         source_url  \n",
       "0           NaN      NaN                 https://vast.ai/products/gpu-cloud  \n",
       "1           NaN      NaN  https://dashboard.voltagepark.com/order/config...  \n",
       "2           NaN      NaN                          https://nebius.com/prices  \n",
       "3           NaN      NaN                 https://www.paperspace.com/pricing  \n",
       "4           NaN      NaN                    https://tensordock.com/gpu-h100  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Sanity check: derived artifacts exist & look sane ---\n",
    "from pathlib import Path\n",
    "import pandas as pd, json\n",
    "\n",
    "DER = Path(\"docs/data/derived\")\n",
    "\n",
    "need_csv = [\n",
    "    \"provider_scores_latest.csv\",\n",
    "    \"market_index.csv\",\n",
    "    \"price_iq_explainable.csv\",\n",
    "    \"roi_comparison_latest.csv\",\n",
    "]\n",
    "for f in need_csv:\n",
    "    p = DER / f\n",
    "    assert p.exists() and p.stat().st_size > 200, f\"Missing/empty: {p}\"\n",
    "\n",
    "scores = pd.read_csv(DER/\"provider_scores_latest.csv\")\n",
    "market = pd.read_csv(DER/\"market_index.csv\")\n",
    "iq     = pd.read_csv(DER/\"price_iq_explainable.csv\")\n",
    "roi    = pd.read_csv(DER/\"roi_comparison_latest.csv\")\n",
    "\n",
    "print(\"rows — scores:\", len(scores), \"market:\", len(market), \"iq:\", len(iq), \"roi:\", len(roi))\n",
    "assert scores[\"price_hourly_usd\"].gt(0).any(), \"prices look empty\"\n",
    "if \"price_score\" in scores.columns:\n",
    "    assert scores[\"price_score\"].between(50,150).any(), \"price_score missing/out of range\"\n",
    "assert iq[\"price_iq\"].between(50,150).any(), \"price_iq missing/out of range\"\n",
    "\n",
    "cfg = DER/\"price_iq_config.json\"\n",
    "print(\"found config:\", cfg.exists(), cfg.read_text()[:120] + \"...\" if cfg.exists() else \"no config\")\n",
    "\n",
    "display(iq.head(5))\n",
    "display(roi.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top by Price-IQ (per model):\n",
      "\n",
      "== H100 ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>market_median</th>\n",
       "      <th>price_iq</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.99</td>\n",
       "      <td>10.995</td>\n",
       "      <td>150.0</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>150.0</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.245</td>\n",
       "      <td>111.1</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.245</td>\n",
       "      <td>100.2</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>99.8</td>\n",
       "      <td>https://tensordock.com/gpu-h100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.245</td>\n",
       "      <td>97.8</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-3y</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.245</td>\n",
       "      <td>89.4</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-1y</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.245</td>\n",
       "      <td>78.7</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               provider  region         type  price_hourly_usd  market_median  \\\n",
       "3           VoltagePark      US    On-Demand              1.99         10.995   \n",
       "4               Vast.ai  Global    On-Demand              1.25          2.245   \n",
       "1                Nebius  Global    On-Demand              2.00          2.245   \n",
       "6            Paperspace  Global    On-Demand              2.24          2.245   \n",
       "8            TensorDock  Global    On-Demand              2.25          2.245   \n",
       "9   Hydra Host (Brokkr)  Global    On-Demand              2.30          2.245   \n",
       "14          CrusoeCloud  Global  Reserved-3y              2.54          2.245   \n",
       "15          CrusoeCloud  Global  Reserved-1y              2.93          2.245   \n",
       "\n",
       "    price_iq                                         source_url  \n",
       "3      150.0  https://dashboard.voltagepark.com/order/config...  \n",
       "4      150.0                 https://vast.ai/products/gpu-cloud  \n",
       "1      111.1                          https://nebius.com/prices  \n",
       "6      100.2                 https://www.paperspace.com/pricing  \n",
       "8       99.8                    https://tensordock.com/gpu-h100  \n",
       "9       97.8             https://brokkr.hydrahost.com/inventory  \n",
       "14      89.4                https://www.crusoe.ai/cloud/pricing  \n",
       "15      78.7                https://www.crusoe.ai/cloud/pricing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== H200 ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>market_median</th>\n",
       "      <th>price_iq</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.475</td>\n",
       "      <td>106.9</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.475</td>\n",
       "      <td>100.9</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.300</td>\n",
       "      <td>100.0</td>\n",
       "      <td>https://nebius.com/h200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2.475</td>\n",
       "      <td>99.1</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-3y</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.475</td>\n",
       "      <td>89.7</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-1y</td>\n",
       "      <td>3.22</td>\n",
       "      <td>2.475</td>\n",
       "      <td>79.0</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-6m</td>\n",
       "      <td>3.43</td>\n",
       "      <td>2.475</td>\n",
       "      <td>74.7</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>4.29</td>\n",
       "      <td>2.475</td>\n",
       "      <td>61.5</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               provider  region         type  price_hourly_usd  market_median  \\\n",
       "2                Nebius  Global    On-Demand              2.30          2.475   \n",
       "5             Shadeform  Global    On-Demand              2.45          2.475   \n",
       "0                Nebius      US    On-Demand              2.30          2.300   \n",
       "10  Hydra Host (Brokkr)  Global    On-Demand              2.50          2.475   \n",
       "18          CrusoeCloud  Global  Reserved-3y              2.79          2.475   \n",
       "11          CrusoeCloud  Global  Reserved-1y              3.22          2.475   \n",
       "17          CrusoeCloud  Global  Reserved-6m              3.43          2.475   \n",
       "12          CrusoeCloud  Global    On-Demand              4.29          2.475   \n",
       "\n",
       "    price_iq                              source_url  \n",
       "2      106.9               https://nebius.com/prices  \n",
       "5      100.9               https://www.shadeform.ai/  \n",
       "0      100.0                 https://nebius.com/h200  \n",
       "10      99.1  https://brokkr.hydrahost.com/inventory  \n",
       "18      89.7     https://www.crusoe.ai/cloud/pricing  \n",
       "11      79.0     https://www.crusoe.ai/cloud/pricing  \n",
       "17      74.7     https://www.crusoe.ai/cloud/pricing  \n",
       "12      61.5     https://www.crusoe.ai/cloud/pricing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cheapest total cost (ROI table):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>savings_vs_median</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.25</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>7164.0</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.00</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.24</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.25</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>https://tensordock.com/gpu-h100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>-396.0</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>3.90</td>\n",
       "      <td>28080.0</td>\n",
       "      <td>-11916.0</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              provider  region       type  price_hourly_usd  total_cost  \\\n",
       "0              Vast.ai  Global  On-Demand              1.25      9000.0   \n",
       "1               Nebius  Global  On-Demand              2.00     14400.0   \n",
       "2           Paperspace  Global  On-Demand              2.24     16128.0   \n",
       "3           TensorDock  Global  On-Demand              2.25     16200.0   \n",
       "4  Hydra Host (Brokkr)  Global  On-Demand              2.30     16560.0   \n",
       "5          CrusoeCloud  Global  On-Demand              3.90     28080.0   \n",
       "\n",
       "   savings_vs_median                              source_url  \n",
       "0             7164.0      https://vast.ai/products/gpu-cloud  \n",
       "1             1764.0               https://nebius.com/prices  \n",
       "2               36.0      https://www.paperspace.com/pricing  \n",
       "3              -36.0         https://tensordock.com/gpu-h100  \n",
       "4             -396.0  https://brokkr.hydrahost.com/inventory  \n",
       "5           -11916.0     https://www.crusoe.ai/cloud/pricing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Quick previews: Top Price-IQ and Cheapest ROI ---\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DER = Path(\"docs/data/derived\")\n",
    "iq = pd.read_csv(DER/\"price_iq_explainable.csv\")\n",
    "roi = pd.read_csv(DER/\"roi_comparison_latest.csv\")\n",
    "\n",
    "print(\"\\nTop by Price-IQ (per model):\")\n",
    "for model in sorted(iq[\"gpu_model\"].dropna().unique()):\n",
    "    print(f\"\\n== {model} ==\")\n",
    "    display(iq[iq[\"gpu_model\"]==model]\n",
    "            .sort_values(\"price_iq\", ascending=False)\n",
    "            .loc[:, [\"provider\",\"region\",\"type\",\"price_hourly_usd\",\"market_median\",\"price_iq\",\"source_url\"]]\n",
    "            .head(8))\n",
    "\n",
    "print(\"\\nCheapest total cost (ROI table):\")\n",
    "display(roi.sort_values([\"total_cost\",\"provider\"]).head(10)\n",
    "        [[\"provider\",\"region\",\"type\",\"price_hourly_usd\",\"total_cost\",\"savings_vs_median\",\"source_url\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved daily panel -> docs/data/derived/backtest_panel_daily.csv rows: 18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>date</th>\n",
       "      <th>market_count</th>\n",
       "      <th>market_median</th>\n",
       "      <th>market_mean</th>\n",
       "      <th>market_p25</th>\n",
       "      <th>market_p75</th>\n",
       "      <th>market_iqr</th>\n",
       "      <th>price_iq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04 11:42:58.443649+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2.245</td>\n",
       "      <td>3.085938</td>\n",
       "      <td>1.8125</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>112.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04 11:42:58.443649+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.885000</td>\n",
       "      <td>2.4125</td>\n",
       "      <td>2.9475</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>107.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "      <td>2025-09-04 12:42:14+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2.245</td>\n",
       "      <td>3.085938</td>\n",
       "      <td>1.8125</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:45:03+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.885000</td>\n",
       "      <td>2.4125</td>\n",
       "      <td>2.9475</td>\n",
       "      <td>0.5350</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:45:03+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>2.245</td>\n",
       "      <td>3.085938</td>\n",
       "      <td>1.8125</td>\n",
       "      <td>2.7000</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider  region gpu_model       type duration  gpu_count  \\\n",
       "0     Nebius  Global      H100  On-Demand       1h        NaN   \n",
       "1     Nebius  Global      H200  On-Demand       1h        NaN   \n",
       "2    Vast.ai  Global      H100  On-Demand       1h        NaN   \n",
       "3  Shadeform  Global      H200  On-Demand       1h        NaN   \n",
       "4  Shadeform  Global      H100  On-Demand       1h        NaN   \n",
       "\n",
       "   price_hourly_usd                          source_url  \\\n",
       "0              2.00           https://nebius.com/prices   \n",
       "1              2.30           https://nebius.com/prices   \n",
       "2              1.25  https://vast.ai/products/gpu-cloud   \n",
       "3              2.45           https://www.shadeform.ai/   \n",
       "4             10.00           https://www.shadeform.ai/   \n",
       "\n",
       "                    fetched_at_utc                      date  market_count  \\\n",
       "0 2025-09-04 11:42:58.443649+00:00 2025-09-04 00:00:00+00:00             8   \n",
       "1 2025-09-04 11:42:58.443649+00:00 2025-09-04 00:00:00+00:00             4   \n",
       "2        2025-09-04 12:42:14+00:00 2025-09-04 00:00:00+00:00             8   \n",
       "3        2025-09-04 12:45:03+00:00 2025-09-04 00:00:00+00:00             4   \n",
       "4        2025-09-04 12:45:03+00:00 2025-09-04 00:00:00+00:00             8   \n",
       "\n",
       "   market_median  market_mean  market_p25  market_p75  market_iqr  price_iq  \n",
       "0          2.245     3.085938      1.8125      2.7000      0.8875     112.2  \n",
       "1          2.475     2.885000      2.4125      2.9475      0.5350     107.6  \n",
       "2          2.245     3.085938      1.8125      2.7000      0.8875     150.0  \n",
       "3          2.475     2.885000      2.4125      2.9475      0.5350     101.0  \n",
       "4          2.245     3.085938      1.8125      2.7000      0.8875      50.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= BACKTEST: build daily panel & recompute daily Price-IQ =================\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "from datetime import timezone\n",
    "\n",
    "HIST = Path(\"docs/data/history\")\n",
    "DER  = Path(\"docs/data/derived\"); DER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load all *_history.csv that follow the slim schema (+ timestamp)\n",
    "frames=[]\n",
    "for p in sorted(HIST.glob(\"*_history.csv\")):\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        # expected slim columns; add if missing\n",
    "        for c in [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "                  \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"]:\n",
    "            if c not in df.columns: df[c] = None\n",
    "        frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] {p.name}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise SystemExit(\"No history files in docs/data/history/*.csv — ensure scrapers append to history.\")\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Normalize\n",
    "raw[\"gpu_model\"] = raw[\"gpu_model\"].astype(str).str.upper().str.strip()\n",
    "raw[\"region\"]    = raw[\"region\"].astype(str).replace({\"nan\":None}).fillna(\"Global\").str.strip()\n",
    "raw[\"type\"]      = raw[\"type\"].astype(str).replace({\"nan\":None}).fillna(\"On-Demand\").str.strip()\n",
    "raw[\"price_hourly_usd\"] = pd.to_numeric(raw[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "raw[\"fetched_at_utc\"]   = pd.to_datetime(raw[\"fetched_at_utc\"], errors=\"coerce\", utc=True)\n",
    "raw = raw.dropna(subset=[\"provider\",\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "\n",
    "# Filter to H100/H200 & sane band (let hyperscalers exceed band)\n",
    "raw = raw[(raw[\"gpu_model\"].isin([\"H100\",\"H200\"])) &\n",
    "          ((raw[\"price_hourly_usd\"].between(0.5, 10.0)) | (raw[\"provider\"].isin([\"AWS\",\"Azure\",\"GCP\"])))]\n",
    "\n",
    "# Daily snapshot (latest per provider/model/region/type each UTC date)\n",
    "raw[\"date\"] = raw[\"fetched_at_utc\"].dt.floor(\"D\")\n",
    "keys = [\"date\",\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\"]\n",
    "daily = (raw.sort_values(\"fetched_at_utc\")\n",
    "            .drop_duplicates(subset=keys, keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# Recompute DAILY market baselines (On-Demand only)\n",
    "od = daily[daily[\"type\"].eq(\"On-Demand\")].copy()\n",
    "grp = [\"date\",\"gpu_model\",\"region\"]\n",
    "g = od.groupby(grp)[\"price_hourly_usd\"]\n",
    "market = pd.DataFrame({\n",
    "    \"market_count\": g.size(),\n",
    "    \"market_median\": g.median(),\n",
    "    \"market_mean\": g.mean(),\n",
    "    \"market_p25\": g.quantile(0.25),\n",
    "    \"market_p75\": g.quantile(0.75),\n",
    "}).reset_index()\n",
    "market[\"market_iqr\"] = (market[\"market_p75\"] - market[\"market_p25\"]).round(6)\n",
    "\n",
    "# Join baselines back to all rows on same (date, model, region)\n",
    "panel = daily.merge(market, on=[\"date\",\"gpu_model\",\"region\"], how=\"left\")\n",
    "\n",
    "# Compute daily Price-IQ (price-only)\n",
    "panel[\"price_iq\"] = (100 * (panel[\"market_median\"] / panel[\"price_hourly_usd\"])) \\\n",
    "                        .clip(50,150).round(1)\n",
    "\n",
    "# Keep reasonable coverage only (need ≥2 providers to form a market that day)\n",
    "panel = panel[panel[\"market_count\"] >= 2].reset_index(drop=True)\n",
    "\n",
    "# Save panel for further analysis\n",
    "panel_out = DER / \"backtest_panel_daily.csv\"\n",
    "panel.to_csv(panel_out, index=False)\n",
    "print(\"Saved daily panel ->\", panel_out, \"rows:\", len(panel))\n",
    "display(panel.head())\n",
    "# =================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: \n",
      " - docs/data/derived/backtest_metrics_daily.csv \n",
      " - docs/data/derived/backtest_summary.csv \n",
      " - docs/data/derived/backtest_calibration.csv \n",
      " - docs/data/derived/backtest_report.json\n",
      "\n",
      "Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>top1_acc</th>\n",
       "      <th>avg_regret_per_gpu_hr</th>\n",
       "      <th>median_spearman</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5025</td>\n",
       "      <td>-0.997725</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_model  top1_acc  avg_regret_per_gpu_hr  median_spearman  days\n",
       "0      H100       0.0                 0.5025        -0.997725     1\n",
       "1      H200       1.0                 0.0000        -1.000000     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calibration preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>iq_bucket</th>\n",
       "      <th>mean_savings_ratio</th>\n",
       "      <th>count</th>\n",
       "      <th>mean_savings_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>50</td>\n",
       "      <td>0.224500</td>\n",
       "      <td>1</td>\n",
       "      <td>-77.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>55</td>\n",
       "      <td>0.575641</td>\n",
       "      <td>1</td>\n",
       "      <td>-42.435897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100</td>\n",
       "      <td>70</td>\n",
       "      <td>0.719551</td>\n",
       "      <td>1</td>\n",
       "      <td>-28.044872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>75</td>\n",
       "      <td>0.766212</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.378840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>85</td>\n",
       "      <td>0.883858</td>\n",
       "      <td>1</td>\n",
       "      <td>-11.614173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H100</td>\n",
       "      <td>95</td>\n",
       "      <td>0.986932</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.306763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H100</td>\n",
       "      <td>100</td>\n",
       "      <td>1.002232</td>\n",
       "      <td>1</td>\n",
       "      <td>0.223214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H100</td>\n",
       "      <td>110</td>\n",
       "      <td>1.122500</td>\n",
       "      <td>1</td>\n",
       "      <td>12.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H100</td>\n",
       "      <td>150</td>\n",
       "      <td>2.399672</td>\n",
       "      <td>2</td>\n",
       "      <td>139.967224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H200</td>\n",
       "      <td>55</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>1</td>\n",
       "      <td>-42.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>H200</td>\n",
       "      <td>70</td>\n",
       "      <td>0.721574</td>\n",
       "      <td>1</td>\n",
       "      <td>-27.842566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H200</td>\n",
       "      <td>75</td>\n",
       "      <td>0.768634</td>\n",
       "      <td>1</td>\n",
       "      <td>-23.136646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>H200</td>\n",
       "      <td>85</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>1</td>\n",
       "      <td>-11.290323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H200</td>\n",
       "      <td>95</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H200</td>\n",
       "      <td>100</td>\n",
       "      <td>1.010204</td>\n",
       "      <td>1</td>\n",
       "      <td>1.020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H200</td>\n",
       "      <td>105</td>\n",
       "      <td>1.076087</td>\n",
       "      <td>1</td>\n",
       "      <td>7.608696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_model  iq_bucket  mean_savings_ratio  count  mean_savings_pct\n",
       "0       H100         50            0.224500      1        -77.550000\n",
       "1       H100         55            0.575641      1        -42.435897\n",
       "2       H100         70            0.719551      1        -28.044872\n",
       "3       H100         75            0.766212      1        -23.378840\n",
       "4       H100         85            0.883858      1        -11.614173\n",
       "5       H100         95            0.986932      2         -1.306763\n",
       "6       H100        100            1.002232      1          0.223214\n",
       "7       H100        110            1.122500      1         12.250000\n",
       "8       H100        150            2.399672      2        139.967224\n",
       "9       H200         55            0.576923      1        -42.307692\n",
       "10      H200         70            0.721574      1        -27.842566\n",
       "11      H200         75            0.768634      1        -23.136646\n",
       "12      H200         85            0.887097      1        -11.290323\n",
       "13      H200         95            0.990000      1         -1.000000\n",
       "14      H200        100            1.010204      1          1.020408\n",
       "15      H200        105            1.076087      1          7.608696"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================== BACKTEST METRICS & CALIBRATION ===========================\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "DER = Path(\"docs/data/derived\")\n",
    "panel = pd.read_csv(DER/\"backtest_panel_daily.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# For each (date, model, region), compare ranking by Price-IQ vs true price\n",
    "def per_bucket(df):\n",
    "    # ground truth: cheapest by price that day\n",
    "    truth = df.sort_values(\"price_hourly_usd\").reset_index(drop=True)\n",
    "    by_iq = df.sort_values(\"price_iq\", ascending=False).reset_index(drop=True)\n",
    "    top_price_provider = truth.loc[0,\"provider\"]\n",
    "    top_iq_provider    = by_iq.loc[0,\"provider\"]\n",
    "    hit = (top_price_provider == top_iq_provider)\n",
    "\n",
    "    # regret for a workload (gpus*hours*util) cancels out; use per-GPU-hour regret\n",
    "    regret = by_iq.loc[0,\"price_hourly_usd\"] - truth.loc[0,\"price_hourly_usd\"]   # ≥ 0 when IQ picks worse\n",
    "    return pd.Series({\n",
    "        \"providers\": len(df),\n",
    "        \"top1_hit\": float(hit),\n",
    "        \"regret_per_gpu_hour\": float(max(0.0, regret)),\n",
    "        \"spearman_corr\": df[[\"price_iq\",\"price_hourly_usd\"]].rank().corr(method=\"spearman\").iloc[0,1]\n",
    "    })\n",
    "\n",
    "grp = [\"date\",\"gpu_model\",\"region\"]\n",
    "metrics_daily = panel.groupby(grp, group_keys=False).apply(per_bucket).reset_index()\n",
    "\n",
    "# Aggregate headline metrics per model\n",
    "summary = (metrics_daily.groupby([\"gpu_model\"])\n",
    "           .agg(top1_acc=(\"top1_hit\",\"mean\"),\n",
    "                avg_regret_per_gpu_hr=(\"regret_per_gpu_hour\",\"mean\"),\n",
    "                median_spearman=(\"spearman_corr\",\"median\"),\n",
    "                days=(\"top1_hit\",\"size\"))\n",
    "           .reset_index())\n",
    "\n",
    "# Calibration table: how Price-IQ maps to savings vs median (should be monotonic)\n",
    "panel[\"savings_vs_median_pct\"] = (panel[\"market_median\"]/panel[\"price_hourly_usd\"])\n",
    "panel[\"iq_bucket\"] = (panel[\"price_iq\"]//5*5).astype(int)  # bucket by 5-pt steps\n",
    "calibration = (panel.groupby([\"gpu_model\",\"iq_bucket\"])\n",
    "               .agg(mean_savings_ratio=(\"savings_vs_median_pct\",\"mean\"),\n",
    "                    count=(\"savings_vs_median_pct\",\"size\"))\n",
    "               .reset_index())\n",
    "calibration[\"mean_savings_pct\"] = (calibration[\"mean_savings_ratio\"]-1.0)*100\n",
    "calibration = calibration.sort_values([\"gpu_model\",\"iq_bucket\"])\n",
    "\n",
    "# Save artifacts\n",
    "metrics_daily_out = DER/\"backtest_metrics_daily.csv\"\n",
    "summary_out       = DER/\"backtest_summary.csv\"\n",
    "calib_out         = DER/\"backtest_calibration.csv\"\n",
    "metrics_daily.to_csv(metrics_daily_out, index=False)\n",
    "summary.to_csv(summary_out, index=False)\n",
    "calibration.to_csv(calib_out, index=False)\n",
    "\n",
    "# Small JSON for CI/dashboard\n",
    "report = {\n",
    "    \"models\": {\n",
    "        m: {\n",
    "            \"top1_accuracy\": round(float(summary.set_index(\"gpu_model\").loc[m,\"top1_acc\"]), 3)\n",
    "                               if m in summary[\"gpu_model\"].values else None,\n",
    "            \"avg_regret_per_gpu_hour\": round(float(summary.set_index(\"gpu_model\").loc[m,\"avg_regret_per_gpu_hr\"]), 4)\n",
    "                               if m in summary[\"gpu_model\"].values else None,\n",
    "            \"median_spearman\": round(float(summary.set_index(\"gpu_model\").loc[m,\"median_spearman\"]), 3)\n",
    "                               if m in summary[\"gpu_model\"].values else None,\n",
    "            \"days_evaluated\": int(summary.set_index(\"gpu_model\").loc[m,\"days\"])\n",
    "                               if m in summary[\"gpu_model\"].values else 0\n",
    "        } for m in sorted(panel[\"gpu_model\"].unique())\n",
    "    }\n",
    "}\n",
    "(DER/\"backtest_report.json\").write_text(json.dumps(report, indent=2))\n",
    "\n",
    "print(\"Saved:\",\n",
    "      \"\\n -\", metrics_daily_out,\n",
    "      \"\\n -\", summary_out,\n",
    "      \"\\n -\", calib_out,\n",
    "      \"\\n -\", DER/\"backtest_report.json\")\n",
    "print(\"\\nSummary:\")\n",
    "display(summary)\n",
    "print(\"\\nCalibration preview:\")\n",
    "display(calibration.head(20))\n",
    "# ======================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved robust daily panel -> docs/data/derived/backtest_panel_daily.csv rows: 11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "      <th>date</th>\n",
       "      <th>market_count</th>\n",
       "      <th>market_median</th>\n",
       "      <th>market_mean</th>\n",
       "      <th>market_p25</th>\n",
       "      <th>market_p75</th>\n",
       "      <th>market_iqr</th>\n",
       "      <th>price_iq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04 11:42:58.443649+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.205186</td>\n",
       "      <td>1.74815</td>\n",
       "      <td>2.2750</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04 11:42:58.443649+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.885000</td>\n",
       "      <td>2.41250</td>\n",
       "      <td>2.9475</td>\n",
       "      <td>0.53500</td>\n",
       "      <td>107.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "      <td>2025-09-04 12:42:14+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.205186</td>\n",
       "      <td>1.74815</td>\n",
       "      <td>2.2750</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:45:03+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.885000</td>\n",
       "      <td>2.41250</td>\n",
       "      <td>2.9475</td>\n",
       "      <td>0.53500</td>\n",
       "      <td>101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "      <td>2025-09-04 13:05:49+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.205186</td>\n",
       "      <td>1.74815</td>\n",
       "      <td>2.2750</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>https://tensordock.com/gpu-h100</td>\n",
       "      <td>2025-09-04 13:16:31+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.205186</td>\n",
       "      <td>1.74815</td>\n",
       "      <td>2.2750</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>99.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.885000</td>\n",
       "      <td>2.41250</td>\n",
       "      <td>2.9475</td>\n",
       "      <td>0.53500</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.30</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.205186</td>\n",
       "      <td>1.74815</td>\n",
       "      <td>2.2750</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>97.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.90</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "      <td>2025-09-04 13:27:24+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.205186</td>\n",
       "      <td>1.74815</td>\n",
       "      <td>2.2750</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>57.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.29</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "      <td>2025-09-04 13:27:24+00:00</td>\n",
       "      <td>2025-09-04 00:00:00+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>2.475</td>\n",
       "      <td>2.885000</td>\n",
       "      <td>2.41250</td>\n",
       "      <td>2.9475</td>\n",
       "      <td>0.53500</td>\n",
       "      <td>57.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              provider  region gpu_model       type duration  gpu_count  \\\n",
       "0               Nebius  Global      H100  On-Demand       1h        NaN   \n",
       "1               Nebius  Global      H200  On-Demand       1h        NaN   \n",
       "2              Vast.ai  Global      H100  On-Demand       1h        NaN   \n",
       "3            Shadeform  Global      H200  On-Demand       1h        NaN   \n",
       "4           Paperspace  Global      H100  On-Demand       1h        NaN   \n",
       "5           TensorDock  Global      H100  On-Demand       1h        1.0   \n",
       "6  Hydra Host (Brokkr)  Global      H200  On-Demand       1h        1.0   \n",
       "7  Hydra Host (Brokkr)  Global      H100  On-Demand       1h        1.0   \n",
       "8          CrusoeCloud  Global      H100  On-Demand       1h        1.0   \n",
       "9          CrusoeCloud  Global      H200  On-Demand       1h        1.0   \n",
       "\n",
       "   price_hourly_usd                              source_url  \\\n",
       "0              2.00               https://nebius.com/prices   \n",
       "1              2.30               https://nebius.com/prices   \n",
       "2              1.25      https://vast.ai/products/gpu-cloud   \n",
       "3              2.45               https://www.shadeform.ai/   \n",
       "4              2.24      https://www.paperspace.com/pricing   \n",
       "5              2.25         https://tensordock.com/gpu-h100   \n",
       "6              2.50  https://brokkr.hydrahost.com/inventory   \n",
       "7              2.30  https://brokkr.hydrahost.com/inventory   \n",
       "8              3.90     https://www.crusoe.ai/cloud/pricing   \n",
       "9              4.29     https://www.crusoe.ai/cloud/pricing   \n",
       "\n",
       "                    fetched_at_utc                      date  market_count  \\\n",
       "0 2025-09-04 11:42:58.443649+00:00 2025-09-04 00:00:00+00:00             7   \n",
       "1 2025-09-04 11:42:58.443649+00:00 2025-09-04 00:00:00+00:00             4   \n",
       "2        2025-09-04 12:42:14+00:00 2025-09-04 00:00:00+00:00             7   \n",
       "3        2025-09-04 12:45:03+00:00 2025-09-04 00:00:00+00:00             4   \n",
       "4        2025-09-04 13:05:49+00:00 2025-09-04 00:00:00+00:00             7   \n",
       "5        2025-09-04 13:16:31+00:00 2025-09-04 00:00:00+00:00             7   \n",
       "6        2025-09-04 13:22:03+00:00 2025-09-04 00:00:00+00:00             4   \n",
       "7        2025-09-04 13:22:03+00:00 2025-09-04 00:00:00+00:00             7   \n",
       "8        2025-09-04 13:27:24+00:00 2025-09-04 00:00:00+00:00             7   \n",
       "9        2025-09-04 13:27:24+00:00 2025-09-04 00:00:00+00:00             4   \n",
       "\n",
       "   market_median  market_mean  market_p25  market_p75  market_iqr  price_iq  \n",
       "0          2.240     2.205186     1.74815      2.2750     0.52685     112.0  \n",
       "1          2.475     2.885000     2.41250      2.9475     0.53500     107.6  \n",
       "2          2.240     2.205186     1.74815      2.2750     0.52685     150.0  \n",
       "3          2.475     2.885000     2.41250      2.9475     0.53500     101.0  \n",
       "4          2.240     2.205186     1.74815      2.2750     0.52685     100.0  \n",
       "5          2.240     2.205186     1.74815      2.2750     0.52685      99.6  \n",
       "6          2.475     2.885000     2.41250      2.9475     0.53500      99.0  \n",
       "7          2.240     2.205186     1.74815      2.2750     0.52685      97.4  \n",
       "8          2.240     2.205186     1.74815      2.2750     0.52685      57.4  \n",
       "9          2.475     2.885000     2.41250      2.9475     0.53500      57.7  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= BACKTEST (robust QA & coverage) =================\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "HIST = Path(\"docs/data/history\")\n",
    "DER  = Path(\"docs/data/derived\"); DER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "frames=[]\n",
    "for p in sorted(HIST.glob(\"*_history.csv\")):\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        for c in [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "                  \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"]:\n",
    "            if c not in df.columns: df[c] = None\n",
    "        frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(\"[WARN]\", p.name, e)\n",
    "\n",
    "if not frames:\n",
    "    raise SystemExit(\"No history files in docs/data/history/*.csv — run scrapers with history first.\")\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Normalize\n",
    "raw[\"gpu_model\"] = raw[\"gpu_model\"].astype(str).str.upper().str.strip()\n",
    "raw[\"region\"]    = raw[\"region\"].astype(str).replace({\"nan\":None}).fillna(\"Global\").str.strip()\n",
    "raw[\"type\"]      = raw[\"type\"].astype(str).replace({\"nan\":None}).fillna(\"On-Demand\").str.strip()\n",
    "raw[\"price_hourly_usd\"] = pd.to_numeric(raw[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "raw[\"fetched_at_utc\"]   = pd.to_datetime(raw[\"fetched_at_utc\"], errors=\"coerce\", utc=True)\n",
    "raw = raw.dropna(subset=[\"provider\",\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "\n",
    "# --- QA (match production aggregator) ---\n",
    "raw = raw[raw[\"gpu_model\"].isin([\"H100\",\"H200\"])]\n",
    "\n",
    "# sane price band for market players; allow hyperscalers outside band\n",
    "band = raw[\"price_hourly_usd\"].between(0.5, 10.0)\n",
    "hypers = raw[\"provider\"].isin([\"AWS\",\"Azure\",\"GCP\"])\n",
    "raw = raw[band | hypers]\n",
    "\n",
    "# Shadeform: drop marketing blurbs (e.g., ~10 $/GPU-hr H100)\n",
    "raw = raw[~((raw[\"provider\"].eq(\"Shadeform\")) &\n",
    "            (raw[\"gpu_model\"].eq(\"H100\")) &\n",
    "            (raw[\"price_hourly_usd\"] > 6.0))]\n",
    "\n",
    "# Only On-Demand for baseline\n",
    "raw = raw[raw[\"type\"].eq(\"On-Demand\")].copy()\n",
    "\n",
    "# Daily snapshot (latest per provider/model/region/date)\n",
    "raw[\"date\"] = raw[\"fetched_at_utc\"].dt.floor(\"D\")\n",
    "keys = [\"date\",\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\"]\n",
    "daily = (raw.sort_values(\"fetched_at_utc\")\n",
    "            .drop_duplicates(subset=keys, keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# Compute robust daily baselines\n",
    "grp = [\"date\",\"gpu_model\",\"region\"]\n",
    "g = daily.groupby(grp)[\"price_hourly_usd\"]\n",
    "\n",
    "def trimmed_median(x, trim=0.1):\n",
    "    # drop top/bottom 10% when sample is big enough\n",
    "    if len(x) >= 6:\n",
    "        lo, hi = x.quantile([trim, 1-trim])\n",
    "        x = x[(x>=lo)&(x<=hi)]\n",
    "    return x.median()\n",
    "\n",
    "market = pd.DataFrame({\n",
    "    \"market_count\": g.size(),\n",
    "    \"market_median\": g.apply(trimmed_median),   # more robust\n",
    "    \"market_mean\": g.mean(),\n",
    "    \"market_p25\": g.quantile(0.25),\n",
    "    \"market_p75\": g.quantile(0.75),\n",
    "}).reset_index()\n",
    "market[\"market_iqr\"] = (market[\"market_p75\"] - market[\"market_p25\"]).round(6)\n",
    "\n",
    "panel = daily.merge(market, on=grp, how=\"left\")\n",
    "\n",
    "# Require real coverage (≥ 3 providers in the bucket)\n",
    "panel = panel[panel[\"market_count\"] >= 3].reset_index(drop=True)\n",
    "\n",
    "# Price-IQ (price-only)\n",
    "panel[\"price_iq\"] = (100 * (panel[\"market_median\"] / panel[\"price_hourly_usd\"])) \\\n",
    "                        .clip(50,150).round(1)\n",
    "\n",
    "panel_out = DER / \"backtest_panel_daily.csv\"\n",
    "panel.to_csv(panel_out, index=False)\n",
    "print(\"Saved robust daily panel ->\", panel_out, \"rows:\", len(panel))\n",
    "display(panel.head(10))\n",
    "# ================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: backtest_metrics_daily.csv, backtest_summary.csv, backtest_report.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>top1_acc</th>\n",
       "      <th>avg_regret_per_gpu_hr</th>\n",
       "      <th>median_spearman</th>\n",
       "      <th>days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_model  top1_acc  avg_regret_per_gpu_hr  median_spearman  days\n",
       "0      H100       1.0                    0.0             -1.0     1\n",
       "1      H200       1.0                    0.0             -1.0     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json\n",
    "\n",
    "DER = Path(\"docs/data/derived\")\n",
    "panel = pd.read_csv(DER/\"backtest_panel_daily.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "if panel.empty:\n",
    "    raise SystemExit(\"Backtest panel empty after QA/coverage. Reduce coverage gate to >=2 or collect more days.\")\n",
    "\n",
    "def per_bucket(df):\n",
    "    truth = df.sort_values(\"price_hourly_usd\").reset_index(drop=True)\n",
    "    pred  = df.sort_values(\"price_iq\", ascending=False).reset_index(drop=True)\n",
    "    return pd.Series({\n",
    "        \"providers\": len(df),\n",
    "        \"top1_hit\": float(truth.loc[0,\"provider\"] == pred.loc[0,\"provider\"]),\n",
    "        \"regret_per_gpu_hour\": float(max(0.0, pred.loc[0,\"price_hourly_usd\"] - truth.loc[0,\"price_hourly_usd\"])),\n",
    "        \"spearman_corr\": df[[\"price_iq\",\"price_hourly_usd\"]].rank().corr(method=\"spearman\").iloc[0,1]\n",
    "    })\n",
    "\n",
    "grp = [\"date\",\"gpu_model\",\"region\"]\n",
    "metrics_daily = panel.groupby(grp, group_keys=False).apply(per_bucket).reset_index()\n",
    "\n",
    "summary = (metrics_daily.groupby(\"gpu_model\")\n",
    "           .agg(top1_acc=(\"top1_hit\",\"mean\"),\n",
    "                avg_regret_per_gpu_hr=(\"regret_per_gpu_hour\",\"mean\"),\n",
    "                median_spearman=(\"spearman_corr\",\"median\"),\n",
    "                days=(\"top1_hit\",\"size\"))\n",
    "           .reset_index())\n",
    "\n",
    "metrics_daily.to_csv(DER/\"backtest_metrics_daily.csv\", index=False)\n",
    "summary.to_csv(DER/\"backtest_summary.csv\", index=False)\n",
    "(DER/\"backtest_report.json\").write_text(json.dumps(summary.to_dict(orient=\"records\"), indent=2))\n",
    "\n",
    "print(\"Saved: backtest_metrics_daily.csv, backtest_summary.csv, backtest_report.json\")\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preflight summary ===\n",
      "{\n",
      "  \"generated_at_utc\": \"2025-09-08T13:14:09+00:00\",\n",
      "  \"missing_latest\": [\n",
      "    \"lambda_labs_latest.csv\",\n",
      "    \"runpod_latest.csv\"\n",
      "  ],\n",
      "  \"missing_derived\": [\n",
      "    \"backtest_summary.csv\"\n",
      "  ],\n",
      "  \"schema_problems\": [],\n",
      "  \"coverage_on_demand_unique_providers\": {\n",
      "    \"H100\": 8,\n",
      "    \"H200\": 4\n",
      "  },\n",
      "  \"band_ok_rate\": 0.923,\n",
      "  \"iq_inverse_corr_pass\": true,\n",
      "  \"counts\": {\n",
      "    \"scores_rows\": 19,\n",
      "    \"roi_rows\": 8\n",
      "  }\n",
      "}\n",
      "\n",
      "Leaderboard preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>market_median</th>\n",
       "      <th>price_iq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.99</td>\n",
       "      <td>10.995</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.245</td>\n",
       "      <td>111.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H100</td>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.245</td>\n",
       "      <td>100.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>H100</td>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>99.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>H100</td>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.245</td>\n",
       "      <td>97.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>H100</td>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-3y</td>\n",
       "      <td>2.54</td>\n",
       "      <td>2.245</td>\n",
       "      <td>89.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>H100</td>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-1y</td>\n",
       "      <td>2.93</td>\n",
       "      <td>2.245</td>\n",
       "      <td>78.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H100</td>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>Reserved-6m</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.245</td>\n",
       "      <td>74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>H100</td>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.245</td>\n",
       "      <td>61.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gpu_model             provider  region         type  price_hourly_usd  \\\n",
       "3       H100          VoltagePark      US    On-Demand              1.99   \n",
       "4       H100              Vast.ai  Global    On-Demand              1.25   \n",
       "1       H100               Nebius  Global    On-Demand              2.00   \n",
       "6       H100           Paperspace  Global    On-Demand              2.24   \n",
       "8       H100           TensorDock  Global    On-Demand              2.25   \n",
       "9       H100  Hydra Host (Brokkr)  Global    On-Demand              2.30   \n",
       "14      H100          CrusoeCloud  Global  Reserved-3y              2.54   \n",
       "15      H100          CrusoeCloud  Global  Reserved-1y              2.93   \n",
       "13      H100          CrusoeCloud  Global  Reserved-6m              3.12   \n",
       "16      H100          CrusoeCloud  Global    On-Demand              3.90   \n",
       "\n",
       "    market_median  price_iq  \n",
       "3          10.995     150.0  \n",
       "4           2.245     150.0  \n",
       "1           2.245     111.1  \n",
       "6           2.245     100.2  \n",
       "8           2.245      99.8  \n",
       "9           2.245      97.8  \n",
       "14          2.245      89.4  \n",
       "15          2.245      78.7  \n",
       "13          2.245      74.5  \n",
       "16          2.245      61.4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROI preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>type</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>market_median</th>\n",
       "      <th>gpu_hours_eff</th>\n",
       "      <th>infra_cost</th>\n",
       "      <th>extra_fees</th>\n",
       "      <th>egress_cost</th>\n",
       "      <th>storage_cost</th>\n",
       "      <th>total_cost</th>\n",
       "      <th>delta_vs_cheapest</th>\n",
       "      <th>savings_vs_median</th>\n",
       "      <th>expected_revenue</th>\n",
       "      <th>gross_margin</th>\n",
       "      <th>roi_pct</th>\n",
       "      <th>source_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7164.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1.99</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14328.0</td>\n",
       "      <td>5328.0</td>\n",
       "      <td>64836.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100</td>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>5400.0</td>\n",
       "      <td>1764.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16128.0</td>\n",
       "      <td>7128.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16200.0</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>-36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://tensordock.com/gpu-h100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H100</td>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16560.0</td>\n",
       "      <td>7560.0</td>\n",
       "      <td>-396.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H100</td>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>3.90</td>\n",
       "      <td>2.245</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>28080.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28080.0</td>\n",
       "      <td>19080.0</td>\n",
       "      <td>-11916.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.crusoe.ai/cloud/pricing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H100</td>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>20.00</td>\n",
       "      <td>10.995</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>144000.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>-64836.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.coreweave.com/pricing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_model             provider  region       type  price_hourly_usd  \\\n",
       "0      H100              Vast.ai  Global  On-Demand              1.25   \n",
       "1      H100          VoltagePark      US  On-Demand              1.99   \n",
       "2      H100               Nebius  Global  On-Demand              2.00   \n",
       "3      H100           Paperspace  Global  On-Demand              2.24   \n",
       "4      H100           TensorDock  Global  On-Demand              2.25   \n",
       "5      H100  Hydra Host (Brokkr)  Global  On-Demand              2.30   \n",
       "6      H100          CrusoeCloud  Global  On-Demand              3.90   \n",
       "7      H100            CoreWeave      US  On-Demand             20.00   \n",
       "\n",
       "   market_median  gpu_hours_eff  infra_cost  extra_fees  egress_cost  \\\n",
       "0          2.245         7200.0      9000.0         0.0          0.0   \n",
       "1         10.995         7200.0     14328.0         0.0          0.0   \n",
       "2          2.245         7200.0     14400.0         0.0          0.0   \n",
       "3          2.245         7200.0     16128.0         0.0          0.0   \n",
       "4          2.245         7200.0     16200.0         0.0          0.0   \n",
       "5          2.245         7200.0     16560.0         0.0          0.0   \n",
       "6          2.245         7200.0     28080.0         0.0          0.0   \n",
       "7         10.995         7200.0    144000.0         0.0          0.0   \n",
       "\n",
       "   storage_cost  total_cost  delta_vs_cheapest  savings_vs_median  \\\n",
       "0           0.0      9000.0                0.0             7164.0   \n",
       "1           0.0     14328.0             5328.0            64836.0   \n",
       "2           0.0     14400.0             5400.0             1764.0   \n",
       "3           0.0     16128.0             7128.0               36.0   \n",
       "4           0.0     16200.0             7200.0              -36.0   \n",
       "5           0.0     16560.0             7560.0             -396.0   \n",
       "6           0.0     28080.0            19080.0           -11916.0   \n",
       "7           0.0    144000.0           135000.0           -64836.0   \n",
       "\n",
       "   expected_revenue  gross_margin  roi_pct  \\\n",
       "0               NaN           NaN      NaN   \n",
       "1               NaN           NaN      NaN   \n",
       "2               NaN           NaN      NaN   \n",
       "3               NaN           NaN      NaN   \n",
       "4               NaN           NaN      NaN   \n",
       "5               NaN           NaN      NaN   \n",
       "6               NaN           NaN      NaN   \n",
       "7               NaN           NaN      NaN   \n",
       "\n",
       "                                          source_url  \n",
       "0                 https://vast.ai/products/gpu-cloud  \n",
       "1  https://dashboard.voltagepark.com/order/config...  \n",
       "2                          https://nebius.com/prices  \n",
       "3                 https://www.paperspace.com/pricing  \n",
       "4                    https://tensordock.com/gpu-h100  \n",
       "5             https://brokkr.hydrahost.com/inventory  \n",
       "6                https://www.crusoe.ai/cloud/pricing  \n",
       "7                  https://www.coreweave.com/pricing  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===================== PRICE-IQ / ROI PREFLIGHT & BUNDLE ======================\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, json, sys\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "DER = Path(\"docs/data/derived\"); LATEST = Path(\"docs/data/latest\")\n",
    "DER.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "required_latest = [\n",
    "    # not hard-failing on missing individual providers—just reporting\n",
    "    \"brokkr_latest.csv\",\"coreweave_latest.csv\",\"crusoecloud_latest.csv\",\n",
    "    \"lambda_labs_latest.csv\",\"nebius_latest.csv\",\"paperspace_latest.csv\",\n",
    "    \"runpod_latest.csv\",\"tensordock_latest.csv\",\"vastai_latest.csv\",\n",
    "    \"voltagepark_latest.csv\"\n",
    "]\n",
    "required_derived = [\n",
    "    \"provider_scores_latest.csv\",\n",
    "    \"market_index.csv\",\n",
    "    \"price_iq_explainable.csv\",   # CSV version of leaderboard\n",
    "    \"price_iq_latest.json\",       # JSON leaderboard for site\n",
    "    \"roi_comparison_latest.csv\",\n",
    "    \"roi_comparison_latest.json\",\n",
    "    \"backtest_panel_daily.csv\",\n",
    "    \"backtest_summary.csv\"\n",
    "]\n",
    "\n",
    "def ok_file(p: Path, min_bytes=200):\n",
    "    return p.exists() and p.stat().st_size >= min_bytes\n",
    "\n",
    "# 1) Files present?\n",
    "missing_latest  = [f for f in required_latest  if not (LATEST/f).exists()]\n",
    "missing_derived = [f for f in required_derived if not ok_file(DER/f)]\n",
    "\n",
    "# 2) Schema checks\n",
    "problems = []\n",
    "def need_cols(df, cols, name):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss: problems.append(f\"{name}: missing cols {miss}\")\n",
    "\n",
    "scores_p = DER/\"provider_scores_latest.csv\"\n",
    "market_p = DER/\"market_index.csv\"\n",
    "iq_p     = DER/\"price_iq_explainable.csv\"\n",
    "roi_p    = DER/\"roi_comparison_latest.csv\"\n",
    "\n",
    "try:\n",
    "    scores = pd.read_csv(scores_p)\n",
    "    need_cols(scores,\n",
    "              [\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\n",
    "               \"market_median\",\"price_score\",\"source_url\"],\n",
    "              \"provider_scores_latest.csv\")\n",
    "except Exception as e:\n",
    "    problems.append(f\"Read fail: {scores_p.name}: {e}\")\n",
    "    scores = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    roi = pd.read_csv(roi_p)\n",
    "    need_cols(roi,\n",
    "              [\"gpu_model\",\"provider\",\"region\",\"type\",\"price_hourly_usd\",\n",
    "               \"total_cost\",\"savings_vs_median\",\"source_url\"],\n",
    "              \"roi_comparison_latest.csv\")\n",
    "except Exception as e:\n",
    "    problems.append(f\"Read fail: {roi_p.name}: {e}\")\n",
    "    roi = pd.DataFrame()\n",
    "\n",
    "# 3) Coverage & bands\n",
    "coverage = {}\n",
    "if not scores.empty:\n",
    "    od = scores.query(\"type == 'On-Demand'\").copy()\n",
    "    od[\"gpu_model\"] = od[\"gpu_model\"].astype(str).str.upper()\n",
    "    cov = (od.query(\"gpu_model in ['H100','H200']\")\n",
    "             .groupby(\"gpu_model\")[\"provider\"].nunique().to_dict())\n",
    "    coverage = {\"H100\": cov.get(\"H100\",0), \"H200\": cov.get(\"H200\",0)}\n",
    "    # sane per-GPU bands (hyperscalers exempt)\n",
    "    band = od[\"price_hourly_usd\"].between(0.5, 10.0)\n",
    "    hyper = od[\"provider\"].isin([\"AWS\",\"Azure\",\"GCP\"])\n",
    "    band_ok_rate = float(((band | hyper).sum()) / max(1, len(od)))\n",
    "else:\n",
    "    band_ok_rate = 0.0\n",
    "\n",
    "# 4) IQ calibration quick check (IQ should inversely correlate with price)\n",
    "iq_ok = None\n",
    "try:\n",
    "    iq = pd.read_csv(iq_p)\n",
    "    if not iq.empty:\n",
    "        r = iq[[\"price_iq\",\"price_hourly_usd\"]].rank().corr(method=\"spearman\").iloc[0,1]\n",
    "        iq_ok = (r <= -0.6)  # strongly negative = good\n",
    "except Exception as e:\n",
    "    problems.append(f\"Read fail: {iq_p.name}: {e}\")\n",
    "\n",
    "# 5) Build a compact “dashboard bundle” (optional, but handy for a static site)\n",
    "def to_records(df, cols):\n",
    "    return json.loads(df[cols].to_json(orient=\"records\"))\n",
    "\n",
    "bundle = {}\n",
    "if ok_file(DER/\"price_iq_explainable.csv\"):\n",
    "    iqv = pd.read_csv(DER/\"price_iq_explainable.csv\")\n",
    "    iqv[\"gpu_model\"] = iqv[\"gpu_model\"].astype(str).str.upper()\n",
    "    bundle[\"leaderboard\"] = {\n",
    "        m: to_records(\n",
    "            iqv[iqv[\"gpu_model\"]==m]\n",
    "               .sort_values([\"region\",\"price_iq\"], ascending=[True,False])\n",
    "               .reset_index(drop=True),\n",
    "            [\"gpu_model\",\"provider\",\"region\",\"type\",\"price_hourly_usd\",\"market_median\",\"price_iq\",\"source_url\"]\n",
    "        )\n",
    "        for m in sorted(iqv[\"gpu_model\"].dropna().unique())\n",
    "    }\n",
    "if ok_file(DER/\"roi_comparison_latest.csv\"):\n",
    "    rod = roi.copy()\n",
    "    if not rod.empty:\n",
    "        bundle[\"roi\"] = to_records(\n",
    "            rod.sort_values([\"gpu_model\",\"total_cost\",\"provider\"]),\n",
    "            [\"gpu_model\",\"provider\",\"region\",\"type\",\"price_hourly_usd\",\"total_cost\",\"savings_vs_median\",\"source_url\"]\n",
    "        )\n",
    "\n",
    "manifest = {\n",
    "    \"generated_at_utc\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "    \"missing_latest\": missing_latest,\n",
    "    \"missing_derived\": missing_derived,\n",
    "    \"schema_problems\": problems,\n",
    "    \"coverage_on_demand_unique_providers\": coverage,\n",
    "    \"band_ok_rate\": round(band_ok_rate, 3),\n",
    "    \"iq_inverse_corr_pass\": bool(iq_ok) if iq_ok is not None else None,\n",
    "    \"counts\": {\n",
    "        \"scores_rows\": int(len(scores)) if not scores.empty else 0,\n",
    "        \"roi_rows\": int(len(roi)) if not roi.empty else 0\n",
    "    }\n",
    "}\n",
    "(DER/\"manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "(DER/\"dashboard_bundle.json\").write_text(json.dumps(bundle, indent=2))\n",
    "\n",
    "print(\"=== Preflight summary ===\")\n",
    "print(json.dumps(manifest, indent=2))\n",
    "print(\"\\nLeaderboard preview:\")\n",
    "try:\n",
    "    display(pd.read_csv(DER/\"price_iq_explainable.csv\")\n",
    "              .sort_values([\"gpu_model\",\"price_iq\"], ascending=[True,False])\n",
    "              [[\"gpu_model\",\"provider\",\"region\",\"type\",\"price_hourly_usd\",\"market_median\",\"price_iq\"]]\n",
    "              .head(10))\n",
    "except Exception:\n",
    "    pass\n",
    "print(\"\\nROI preview:\")\n",
    "if not roi.empty:\n",
    "    display(roi.sort_values([\"gpu_model\",\"total_cost\"]).head(10))\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OnDemand    6\n",
       " Reserved    2\n",
       " Name: price_type, dtype: int64,\n",
       " ['Global'],\n",
       " ['Unknown'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['price_type'].value_counts(), sorted(train['region'].unique()), sorted(train['cpu_platform'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[load_quotes] Loaded sfcompute_grid/grid_history.csv -> 30 rows\n",
      "[OLS] Design failed; retrying simplified formula (no CPU terms/provider FE)\n",
      "OLS R^2=0.250  MAE=$0.01/GPU-hr  MAPE=0.94%\n",
      "Artifacts written to: /Users/evieculloty/Documents/Forward Compute/docs/data/derived/priceiq_model\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# PRICEIQ REVERSE-ENGINEERING — SINGLE, CLEAN CELL\n",
    "# ===============================================\n",
    "# It:\n",
    "#  1) Loads quotes CSVs (auto-discovers under docs/ and data/)\n",
    "#  2) Harmonises columns to PriceIQ-like inputs\n",
    "#  3) Trains OLS + Quantile models (p10,p25,p50,p75,p90)\n",
    "#  4) Builds a prediction grid ONLY from categories observed in training\n",
    "#  5) Predicts using training design_info (no column mismatches)\n",
    "#  6) Writes artefacts to docs/data/derived/priceiq_model/\n",
    "#\n",
    "# Dependencies: pandas numpy statsmodels patsy scikit-learn matplotlib\n",
    "# pip install pandas numpy statsmodels patsy scikit-learn matplotlib\n",
    "\n",
    "import os, re, json, math, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "from patsy import dmatrices, dmatrix, build_design_matrices\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "INPUT_FILES = [\n",
    "    # If you know the exact CSVs, put them here; otherwise autodiscovery kicks in.\n",
    "    \"docs/data/derived/provider_quotes_latest.csv\",\n",
    "    \"docs/data/derived/provider_quotes_history.csv\",\n",
    "    \"docs/data/sfcompute_history.csv\",\n",
    "    \"docs/sfcompute_grid/grid_history.csv\",\n",
    "    \"sfcompute_grid/grid_history.csv\",\n",
    "    \"data/sfcompute_history.csv\",\n",
    "    \"docs/data/derived/sfcompute_history.csv\"\n",
    "]\n",
    "OUT_DIR = Path(\"docs/data/derived/priceiq_model\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "QUANTILES = [0.10, 0.25, 0.50, 0.75, 0.90]\n",
    "INCLUDE_PROVIDER_FE = True\n",
    "INCLUDE_DURATION_TERM = True\n",
    "\n",
    "# -------------------------\n",
    "# HELPERS\n",
    "# -------------------------\n",
    "DURATION_MAP = {\n",
    "    \"1 hour\": 1/24, \"6 hours\": 6/24, \"12 hours\": 0.5, \"1 day\": 1, \"3 days\": 3,\n",
    "    \"1 week\": 7, \"2 weeks\": 14, \"3 weeks\": 21, \"1 month\": 30, \"2 months\": 60,\n",
    "    \"3 months\": 90, \"6 months\": 180, \"12 months\": 365\n",
    "}\n",
    "CPU_PLAT_MAP = {\"intel\": \"Intel\", \"amd\": \"AMD\", \"arm\": \"ARM\"}\n",
    "PRICE_TYPE_ALIASES = {\"on-demand\": \"OnDemand\", \"ondemand\": \"OnDemand\", \"on demand\": \"OnDemand\",\n",
    "                      \"reserved\": \"Reserved\", \"commit\": \"Reserved\", \"committed\": \"Reserved\",\n",
    "                      \"spot\": \"Spot\", \"market\": \"Spot\"}\n",
    "REGION_ALIASES = {\"eu\": \"EU\", \"europe\": \"EU\", \"us\": \"US\", \"usa\": \"US\", \"na\": \"US\", \"uk\": \"UK\", \"gb\": \"UK\"}\n",
    "GPU_MODEL_ALIASES = {\"nvidia h100\": \"H100\", \"h100\": \"H100\", \"nvidia h200\": \"H200\", \"h200\": \"H200\",\n",
    "                     \"nvidia a100\": \"A100\", \"a100\": \"A100\"}\n",
    "GB_RE = re.compile(r\"(\\d+\\.?\\d*)\\s*gb\", re.I)\n",
    "\n",
    "def first_path_that_exists(paths: List[str]) -> List[Path]:\n",
    "    return [Path(p) for p in paths if Path(p).exists()]\n",
    "\n",
    "def norm_price_type(x: str) -> str:\n",
    "    if not isinstance(x, str): return None\n",
    "    return PRICE_TYPE_ALIASES.get(x.lower().strip(), x.title())\n",
    "\n",
    "def norm_region(x: str) -> str:\n",
    "    if not isinstance(x, str): return None\n",
    "    return REGION_ALIASES.get(x.lower().strip(), x.upper())\n",
    "\n",
    "def norm_gpu_model(x: str) -> str:\n",
    "    if not isinstance(x, str): return None\n",
    "    return GPU_MODEL_ALIASES.get(x.lower().strip(), x.upper())\n",
    "\n",
    "def extract_gb(s: str) -> float:\n",
    "    if not isinstance(s, str): return np.nan\n",
    "    m = GB_RE.search(s); return float(m.group(1)) if m else np.nan\n",
    "\n",
    "def to_float_safe(x):\n",
    "    try: return float(x)\n",
    "    except Exception: return np.nan\n",
    "\n",
    "def duration_to_days(x) -> float:\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)): return np.nan\n",
    "    if isinstance(x, (int, float)): return float(x)\n",
    "    s = str(x).strip().lower()\n",
    "    if s in DURATION_MAP: return float(DURATION_MAP[s])\n",
    "    m = re.match(r\"(\\d+\\.?\\d*)([dwm])\", s)\n",
    "    if m:\n",
    "        val, unit = float(m.group(1)), m.group(2)\n",
    "        return val * (1 if unit=='d' else 7 if unit=='w' else 30)\n",
    "    try: return float(s)\n",
    "    except Exception: return np.nan\n",
    "\n",
    "# -------------------------\n",
    "# LOAD & HARMONISE\n",
    "# -------------------------\n",
    "CANDIDATE_COL_MAP = {\n",
    "    \"provider\": [\"provider\",\"vendor\",\"cloud\"],\n",
    "    \"region\": [\"region\",\"geo\",\"location\"],\n",
    "    \"gpu_model\": [\"gpu_model\",\"gpu_type\",\"gpu\"],\n",
    "    \"price_type\": [\"price_type\",\"type\",\"market_type\"],\n",
    "    \"price_per_gpu_hr\": [\"usd_per_gpu_hr\",\"price_hourly_usd\",\"price_per_gpu_hr\",\"price_usd_per_hr\"],\n",
    "    \"gpu_ram_gb\": [\"gpu_ram_gb\",\"vram_gb\",\"gpu_mem_gb\",\"gpu_ram\"],\n",
    "    \"cpu_platform\": [\"cpu_platform\",\"cpu_vendor\",\"platform\"],\n",
    "    \"cpu_cores_eff\": [\"cpu_cores_eff\",\"cpu_cores\",\"vcpu\"],\n",
    "    \"cpu_ram_gb\": [\"cpu_ram_gb\",\"system_ram_gb\",\"ram_gb\"],\n",
    "    \"timestamp\": [\"timestamp\",\"ts_utc\",\"time\",\"date\"],\n",
    "    \"reserved_duration\": [\"reserved_duration\",\"duration\",\"term\"]\n",
    "}\n",
    "\n",
    "def harmonise_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    out = {}\n",
    "    for target, candidates in CANDIDATE_COL_MAP.items():\n",
    "        src = None\n",
    "        for cand in candidates:\n",
    "            if cand.lower() in cols:\n",
    "                src = cols[cand.lower()]; break\n",
    "        out[target] = src\n",
    "    uni = pd.DataFrame()\n",
    "    for k, src in out.items():\n",
    "        uni[k] = df[src] if src is not None else np.nan\n",
    "\n",
    "    # Normalise\n",
    "    uni[\"provider\"] = uni[\"provider\"].astype(str).str.strip().replace({\"nan\": np.nan}).fillna(\"Unknown\")\n",
    "    uni[\"region\"] = uni[\"region\"].map(norm_region).fillna(\"Global\")\n",
    "    uni[\"gpu_model\"] = uni[\"gpu_model\"].map(norm_gpu_model)\n",
    "    uni[\"price_type\"] = uni[\"price_type\"].map(norm_price_type)\n",
    "    uni[\"price_per_gpu_hr\"] = uni[\"price_per_gpu_hr\"].apply(to_float_safe)\n",
    "    uni[\"timestamp\"] = pd.to_datetime(uni[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "    # RAM fallbacks\n",
    "    if uni[\"gpu_ram_gb\"].isna().all() and \"gpu_model\" in uni:\n",
    "        uni[\"gpu_ram_gb\"] = uni[\"gpu_model\"].apply(extract_gb)\n",
    "\n",
    "    uni[\"cpu_platform\"] = uni[\"cpu_platform\"].astype(str).str.lower().map(CPU_PLAT_MAP).fillna(\"Unknown\")\n",
    "    for c in [\"gpu_ram_gb\",\"cpu_cores_eff\",\"cpu_ram_gb\"]:\n",
    "        uni[c] = pd.to_numeric(uni[c], errors=\"coerce\")\n",
    "\n",
    "    # Duration\n",
    "    uni[\"duration_days\"] = np.nan\n",
    "    if out.get(\"reserved_duration\") is not None:\n",
    "        uni[\"duration_days\"] = uni[\"reserved_duration\"].apply(duration_to_days)\n",
    "\n",
    "    # Infer price_type from duration if missing\n",
    "    if uni[\"price_type\"].isna().any():\n",
    "        uni.loc[uni[\"price_type\"].isna(), \"price_type\"] = np.where(uni[\"duration_days\"] >= 7, \"Reserved\", \"OnDemand\")\n",
    "\n",
    "    # Keep meaningful rows\n",
    "    uni = uni.dropna(subset=[\"gpu_model\",\"price_type\",\"region\",\"price_per_gpu_hr\"], how=\"any\")\n",
    "    return uni\n",
    "\n",
    "def load_quotes(paths: List[str]) -> pd.DataFrame:\n",
    "    files = first_path_that_exists(paths)\n",
    "    if not files:\n",
    "        # autodiscover\n",
    "        search_roots = [Path(\"docs\"), Path(\"data\"), Path(\".\"), Path(\"docs/data\"), Path(\"docs/data/derived\")]\n",
    "        cand_files = []\n",
    "        for root in search_roots:\n",
    "            if root.exists():\n",
    "                for p in root.rglob(\"*.csv\"):\n",
    "                    name = p.name.lower()\n",
    "                    if any(k in name for k in [\"sfcompute\",\"quotes\",\"history\",\"grid\"]):\n",
    "                        cand_files.append(p)\n",
    "        def has_price_cols(p: Path) -> bool:\n",
    "            try:\n",
    "                head = pd.read_csv(p, nrows=5)\n",
    "                cols = [c.lower() for c in head.columns]\n",
    "                return any(k in cols for k in [\"usd_per_gpu_hr\",\"price_hourly_usd\",\"price_per_gpu_hr\"]) and \\\n",
    "                       any(x in cols for x in [\"gpu_model\",\"gpu_type\",\"gpu\"])\n",
    "            except Exception:\n",
    "                return False\n",
    "        files = [p for p in cand_files if has_price_cols(p)]\n",
    "        if not files:\n",
    "            raise FileNotFoundError(\"No input CSVs found. Put a quotes/history CSV under docs/ or data/ (e.g., docs/data/sfcompute_history.csv)\")\n",
    "    frames = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            raw = pd.read_csv(f)\n",
    "            uni = harmonise_columns(raw)\n",
    "            if not uni.empty:\n",
    "                frames.append(uni)\n",
    "                print(f\"[load_quotes] Loaded {f} -> {len(uni)} rows\")\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Could not load {f}: {e}\")\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No valid frames after harmonisation.\")\n",
    "    df = pd.concat(frames, ignore_index=True).drop_duplicates()\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# MODEL PREP\n",
    "# -------------------------\n",
    "def prepare_for_model(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in [\"gpu_model\",\"price_type\",\"region\",\"cpu_platform\",\"provider\"]:\n",
    "        if c not in df.columns: df[c] = \"Unknown\"\n",
    "        df[c] = df[c].astype(str).fillna(\"Unknown\")\n",
    "    num_cols = [\"cpu_cores_eff\",\"cpu_ram_gb\",\"gpu_ram_gb\",\"duration_days\"]\n",
    "    for c in num_cols:\n",
    "        if c not in df.columns: df[c] = np.nan\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if df[\"duration_days\"].isna().all(): df[\"duration_days\"] = 0.0\n",
    "    med_by_gpu = df.groupby(\"gpu_model\")[num_cols].transform(lambda s: s.fillna(s.median()))\n",
    "    for c in num_cols:\n",
    "        df[c] = med_by_gpu[c].fillna(df[c].median())\n",
    "    df = df.dropna(subset=[\"price_per_gpu_hr\"]) \n",
    "    df = df[(df[\"price_per_gpu_hr\"] > 0) & (df[\"price_per_gpu_hr\"] < 100.0)]\n",
    "    return df\n",
    "\n",
    "def build_formula(include_provider_fe=True, include_duration_term=True) -> str:\n",
    "    base = \"price_per_gpu_hr ~ C(gpu_model) + C(price_type) + C(region) + C(cpu_platform) + cpu_cores_eff + cpu_ram_gb + gpu_ram_gb\"\n",
    "    if include_provider_fe:\n",
    "        base += \" + C(provider)\"\n",
    "    if include_duration_term:\n",
    "        base += \" + np.log1p(duration_days)\"\n",
    "    return base\n",
    "\n",
    "# -------------------------\n",
    "# FIT\n",
    "# -------------------------\n",
    "def fit_ols(df: pd.DataFrame, formula: str):\n",
    "    y, X = dmatrices(formula, data=df, return_type='dataframe')\n",
    "    model = sm.OLS(y, X).fit(cov_type='HC1')\n",
    "    return model, X, y, X.design_info\n",
    "\n",
    "def fit_quantiles(df: pd.DataFrame, formula_rhs: str, taus: List[float]) -> Tuple[Dict[float, sm.regression.linear_model.RegressionResultsWrapper], object]:\n",
    "    y = df['price_per_gpu_hr']\n",
    "    X = dmatrix(formula_rhs, data=df, return_type='dataframe')\n",
    "    models = {}\n",
    "    for tau in taus:\n",
    "        try:\n",
    "            qr = QuantReg(y, X).fit(q=tau)\n",
    "            models[tau] = qr\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Quantile {tau} failed: {e}\")\n",
    "    return models, X.design_info\n",
    "\n",
    "# -------------------------\n",
    "# GRID + PREDICT (category-safe)\n",
    "# -------------------------\n",
    "def unique_nonnull(series: pd.Series, topk: int = 20) -> List[str]:\n",
    "    vals = [v for v in series.dropna().astype(str).unique().tolist() if v]\n",
    "    return vals[:topk]\n",
    "\n",
    "def build_prediction_grid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Use only categories SEEN in training to avoid Patsy unseen-level errors\n",
    "    gpus = unique_nonnull(df['gpu_model'])\n",
    "    types = unique_nonnull(df['price_type'])\n",
    "    regions = unique_nonnull(df['region'])\n",
    "    platforms = unique_nonnull(df['cpu_platform'])\n",
    "\n",
    "    med = df.groupby('gpu_model')[['gpu_ram_gb','cpu_cores_eff','cpu_ram_gb']].median()\n",
    "    rows = []\n",
    "    for gpu in gpus:\n",
    "        mrow = med.loc[gpu] if gpu in med.index else df[['gpu_ram_gb','cpu_cores_eff','cpu_ram_gb']].median()\n",
    "        for pt in types:\n",
    "            for reg in regions:\n",
    "                for plat in platforms:\n",
    "                    rows.append({\n",
    "                        'gpu_model': gpu,\n",
    "                        'price_type': pt,\n",
    "                        'region': reg,\n",
    "                        'cpu_platform': plat,\n",
    "                        'gpu_ram_gb': float(mrow.get('gpu_ram_gb', np.nan)),\n",
    "                        'cpu_cores_eff': float(mrow.get('cpu_cores_eff', np.nan)),\n",
    "                        'cpu_ram_gb': float(mrow.get('cpu_ram_gb', np.nan)),\n",
    "                        'duration_days': 0.0  # set non-zero if you want reserved curve\n",
    "                    })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def predict_with_models(df_design: pd.DataFrame, ols, quantile_models: Dict[float, sm.regression.linear_model.RegressionResultsWrapper], design_info_ols=None, design_info_q=None) -> pd.DataFrame:\n",
    "    if df_design is None or df_design.empty:\n",
    "        return pd.DataFrame()\n",
    "    # Build matrices using training design schema\n",
    "    Xg = build_design_matrices([design_info_ols], df_design, return_type='dataframe')[0] if design_info_ols is not None else None\n",
    "    out = df_design.copy()\n",
    "    if Xg is not None and Xg.shape[1] == len(ols.params):\n",
    "        out['pred_ols'] = ols.predict(Xg)\n",
    "    else:\n",
    "        print(\"[predict] OLS design mismatch; filling NaN\")\n",
    "        out['pred_ols'] = np.nan\n",
    "    if quantile_models:\n",
    "        Xq = build_design_matrices([design_info_q], df_design, return_type='dataframe')[0] if design_info_q is not None else Xg\n",
    "        for tau, m in quantile_models.items():\n",
    "            col = f'pred_q{int(tau*100)}'\n",
    "            try:\n",
    "                if Xq is not None and Xq.shape[1] == len(m.params):\n",
    "                    out[col] = m.predict(Xq)\n",
    "                else:\n",
    "                    out[col] = np.nan\n",
    "            except Exception as e:\n",
    "                print(f\"[predict] Quantile {tau} failed: {e}\")\n",
    "                out[col] = np.nan\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# EXPORTS + PLOTS\n",
    "# -------------------------\n",
    "def evaluate_and_export(df: pd.DataFrame, ols, X, y, quantile_models):\n",
    "    y_hat = ols.predict(X).values.ravel()\n",
    "    r2 = r2_score(y, y_hat)\n",
    "    mae = mean_absolute_error(y, y_hat)\n",
    "    mape = float(np.mean(np.abs((y.values.ravel() - y_hat) / np.clip(y.values.ravel(), 1e-8, None))))\n",
    "\n",
    "    coef = (pd.Series(ols.params, name='coef').to_frame().assign(se=ols.bse).reset_index().rename(columns={'index':'term'}))\n",
    "    coef.to_csv(OUT_DIR / 'coefficients_ols.csv', index=False)\n",
    "\n",
    "    qcoef = []\n",
    "    for tau, m in quantile_models.items():\n",
    "        s = pd.Series(m.params, name='coef').to_frame().reset_index().rename(columns={'index':'term'})\n",
    "        s['tau'] = tau; qcoef.append(s)\n",
    "    if qcoef:\n",
    "        pd.concat(qcoef, ignore_index=True).to_csv(OUT_DIR / 'coefficients_quantiles.csv', index=False)\n",
    "\n",
    "    with open(OUT_DIR / 'metrics.json', 'w') as f:\n",
    "        json.dump({\"r2\": float(r2), \"mae\": float(mae), \"mape\": float(mape)}, f, indent=2)\n",
    "    print(f\"OLS R^2={r2:.3f}  MAE=${mae:.2f}/GPU-hr  MAPE={mape:.2%}\")\n",
    "\n",
    "def plot_price_bands(obs: pd.DataFrame, preds: pd.DataFrame, gpu: str, savepath: Path):\n",
    "    sub_obs = obs[obs['gpu_model'] == gpu]\n",
    "    sub_pred = preds[preds['gpu_model'] == gpu]\n",
    "    if sub_obs.empty or sub_pred.empty:\n",
    "        print(f\"[plot] No data for {gpu}\")\n",
    "        return\n",
    "    plt.figure(figsize=(10,6))\n",
    "    g = (sub_obs.groupby(['region','price_type'])['price_per_gpu_hr'].median().reset_index())\n",
    "    for _, r in g.iterrows():\n",
    "        plt.scatter(f\"{r['region']}\\n{r['price_type']}\", r['price_per_gpu_hr'], alpha=0.7)\n",
    "    for _, r in sub_pred.iterrows():\n",
    "        x = f\"{r['region']}\\n{r['price_type']}\"\n",
    "        p10, p50, p90 = r.get('pred_q10', np.nan), r.get('pred_q50', np.nan), r.get('pred_q90', np.nan)\n",
    "        if not (np.isnan(p10) or np.isnan(p90)):\n",
    "            plt.vlines(x, p10, p90)\n",
    "        if not np.isnan(p50):\n",
    "            plt.scatter(x, p50, marker='x')\n",
    "    plt.title(f\"{gpu}: Observed median vs PriceIQ-style bands\")\n",
    "    plt.ylabel(\"$/GPU-hour\"); plt.xticks(rotation=45, ha='right'); plt.tight_layout(); plt.savefig(savepath); plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# RUN\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df0 = load_quotes(INPUT_FILES)\n",
    "    df0 = prepare_for_model(df0)\n",
    "\n",
    "    # time-based split if timestamp exists\n",
    "    if df0['timestamp'].notna().any():\n",
    "        df0 = df0.sort_values('timestamp'); cutoff = int(len(df0) * 0.8)\n",
    "        train, test = df0.iloc[:cutoff].copy(), df0.iloc[cutoff:].copy()\n",
    "    else:\n",
    "        train, test = df0.copy(), None\n",
    "\n",
    "    # tighten categories to what exists\n",
    "    for c in ['price_type','region','cpu_platform','gpu_model']:\n",
    "        train[c] = train[c].astype(str)\n",
    "\n",
    "    formula = build_formula(INCLUDE_PROVIDER_FE, INCLUDE_DURATION_TERM)\n",
    "\n",
    "    try:\n",
    "        ols_model, X_train, y_train, design_info_ols = fit_ols(train, formula)\n",
    "    except Exception:\n",
    "        print(\"[OLS] Design failed; retrying simplified formula (no CPU terms/provider FE)\")\n",
    "        simple_rhs = \"C(gpu_model) + C(price_type) + C(region) + np.log1p(duration_days)\"\n",
    "        y, X = dmatrices(\"price_per_gpu_hr ~ \" + simple_rhs, data=train, return_type='dataframe')\n",
    "        ols_model = sm.OLS(y, X).fit(cov_type='HC1')\n",
    "        X_train, y_train = X, y\n",
    "        design_info_ols = X.design_info\n",
    "        formula = \"price_per_gpu_hr ~ \" + simple_rhs\n",
    "\n",
    "    rhs = formula.split('~',1)[1]\n",
    "    q_models, design_info_q = fit_quantiles(train, rhs, QUANTILES)\n",
    "\n",
    "    evaluate_and_export(train, ols_model, X_train, y_train, q_models)\n",
    "\n",
    "    grid = build_prediction_grid(train)\n",
    "    preds = predict_with_models(grid, ols_model, q_models, design_info_ols=design_info_ols, design_info_q=design_info_q)\n",
    "    preds.to_csv(OUT_DIR / 'predictions_grid.csv', index=False)\n",
    "\n",
    "    if test is not None and not test.empty:\n",
    "        X_test = build_design_matrices([design_info_ols], test, return_type='dataframe')[0]\n",
    "        test['pred_ols'] = ols_model.predict(X_test)\n",
    "        for tau, m in q_models.items():\n",
    "            Xq = build_design_matrices([design_info_q], test, return_type='dataframe')[0]\n",
    "            test[f'pred_q{int(tau*100)}'] = m.predict(Xq)\n",
    "        test.to_csv(OUT_DIR / 'test_scored.csv', index=False)\n",
    "        with open(OUT_DIR / 'metrics_test.json','w') as f:\n",
    "            json.dump({'r2': float(r2_score(test['price_per_gpu_hr'], test['pred_ols'])),\n",
    "                       'mae': float(mean_absolute_error(test['price_per_gpu_hr'], test['pred_ols']))}, f, indent=2)\n",
    "\n",
    "    for gpu in ['H100','H200']:\n",
    "        plot_price_bands(train, preds, gpu, OUT_DIR / f\"bands_{gpu}.png\")\n",
    "\n",
    "    print(f\"Artifacts written to: {OUT_DIR.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
