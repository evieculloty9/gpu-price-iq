{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape provider\n",
    "# --- Async helper that works in notebooks and GitHub Actions ---\n",
    "import asyncio\n",
    "\n",
    "def await_safe(coro):\n",
    "    \"\"\"\n",
    "    Run an async coroutine from anywhere:\n",
    "    - If an event loop is already running (Jupyter/nbconvert), use nest_asyncio + run_until_complete\n",
    "    - Otherwise, use asyncio.run\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            try:\n",
    "                import nest_asyncio\n",
    "                nest_asyncio.apply()\n",
    "            except Exception:\n",
    "                pass\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        # No current loop\n",
    "        return asyncio.run(coro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>8X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>8</td>\n",
       "      <td>2.99</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>4X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>4</td>\n",
       "      <td>3.09</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>2X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>2</td>\n",
       "      <td>3.19</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>1.49</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>3.29</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region    gpu_model       type duration  gpu_count  \\\n",
       "0  Lambda Labs     US  8X H100 SXM  On-Demand       1h          8   \n",
       "1  Lambda Labs     US  4X H100 SXM  On-Demand       1h          4   \n",
       "2  Lambda Labs     US  2X H100 SXM  On-Demand       1h          2   \n",
       "3  Lambda Labs     US      1X H200  On-Demand       1h          1   \n",
       "4  Lambda Labs     US  1X H100 SXM  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                            source_url      fetched_at_utc  \n",
       "0              2.99  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "1              3.09  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "2              3.19  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "3              1.49  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "4              3.29  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lambda Labs \n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from typing import Optional\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _norm_gpu(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")  # treat GH200 as H200\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(s: str) -> Optional[int]:\n",
    "    if not isinstance(s, str): \n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)x\", s, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_in(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): \n",
    "        return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes https://cloud.lambdalabs.com/pricing and returns SLIM rows\n",
    "    for H100/H200 (On-Demand, 1h). If `region` provided, overrides detected region.\n",
    "    \"\"\"\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    rows_out = []\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        tbl_region = region or _infer_region(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds:\n",
    "                continue\n",
    "            row_text = \" | \".join(tds)\n",
    "\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "\n",
    "            price = _price_in(row_text)\n",
    "            if price is None:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            gpu_model = _norm_gpu(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            count = _gpu_count(gpu_model)\n",
    "\n",
    "            rows_out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": gpu_model,      \n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": count,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    df = df[keep].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# Example:\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "display(df_lambda.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lambda_labs] snapshot -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/20250904_112436_lambda_labs.csv\n",
      "[lambda_labs] history  -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/lambda_labs_history.csv\n",
      "[lambda_labs] latest   -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/lambda_labs_latest.csv\n",
      "[runpod] snapshot -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/20250904_112441_runpod.csv\n",
      "[runpod] history  -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/runpod_history.csv\n",
      "[runpod] latest   -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/runpod_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200 141 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>3.59</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-04 11:24:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100 PCIE 80 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-04 11:24:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region        gpu_model       type duration gpu_count  \\\n",
       "4   RunPod  Global      H200 141 GB  On-Demand       1h      None   \n",
       "6   RunPod  Global  H100 PCIE 80 GB  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                     source_url      fetched_at_utc  \n",
       "4              3.59  https://www.runpod.io/pricing 2025-09-04 11:24:41  \n",
       "6              1.99  https://www.runpod.io/pricing 2025-09-04 11:24:41  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Lambda Labs (static) + RunPod (async) with per-provider history =====\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, os, asyncio, pandas as pd, tempfile\n",
    "from typing import Optional, Dict, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "# -------- storage (per-provider history/snapshots) --------\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        # fallback to tmp if workspace is read-only\n",
    "        tmp = Path(tempfile.gettempdir()) / \"gpu_data\"\n",
    "        d = tmp / d.name\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    try:\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    except Exception:\n",
    "        snap_path = Path(tempfile.gettempdir()) / f\"{ts}_{provider_slug}.csv\"\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    try:\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    except Exception:\n",
    "        hist_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_history.csv\"\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    # latest (newest rows only per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    try:\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    except Exception:\n",
    "        latest_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_latest.csv\"\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ------------------------- Lambda Labs (static) -------------------------\n",
    "def _norm_gpu_lambda(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(text: str) -> Optional[int]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"(\\d+)\\s*x\", text, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_dollar(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region_lambda(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30); r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    out = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        tbl_region = region or _infer_region_lambda(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds: continue\n",
    "            row_text = \" | \".join(tds)\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "            price = _price_dollar(row_text)\n",
    "            if price is None: continue\n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            model = _norm_gpu_lambda(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": _gpu_count(model),\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    return _ensure_slim(df[keep].reset_index(drop=True))\n",
    "\n",
    "# --------------------------- RunPod (async) ---------------------------\n",
    "def _extract_gpu_model_runpod(text: str) -> Optional[str]:\n",
    "    if not isinstance(text, str): return None\n",
    "    text_up = re.sub(r\"\\s+\", \" \", text.upper())\n",
    "    m = re.search(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", text_up)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def _price_hourly_runpod(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    t = text.replace(\",\", \"\")\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", t, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "async def scrape_runpod_async() -> pd.DataFrame:\n",
    "    from playwright.async_api import async_playwright\n",
    "    url = \"https://www.runpod.io/pricing\"; region = \"Global\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1200)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\"], class_=re.compile(r\"(price|pricing|card|grid|table)\", re.I))\n",
    "    if not nodes:\n",
    "        nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\",\"p\",\"span\"])\n",
    "\n",
    "    out = []\n",
    "    for n in nodes:\n",
    "        text = n.get_text(\" \", strip=True)\n",
    "        if \"H100\" not in text and \"H200\" not in text: \n",
    "            continue\n",
    "        price = _price_hourly_runpod(text)\n",
    "        if price is None:\n",
    "            continue\n",
    "        model = _extract_gpu_model_runpod(text)\n",
    "        if model is None:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"provider\": \"RunPod\",\n",
    "            \"region\": region,\n",
    "            \"gpu_model\": model,\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": _gpu_count(text),\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    df = df[df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", na=False)]\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# --------------- Runner that works in scripts & notebooks ---------------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "# Lambda Labs\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "_save_provider(df_lambda, \"lambda_labs\")\n",
    "\n",
    "# RunPod\n",
    "df_runpod = arun(scrape_runpod_async())\n",
    "_save_provider(df_runpod, \"runpod\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nebius] snapshot -> docs/data/snapshots/20250904_114258_nebius.csv\n",
      "[nebius] history  -> docs/data/history/nebius_history.csv\n",
      "[nebius] latest   -> docs/data/latest/nebius_latest.csv\n",
      "Nebius rows this run: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04T11:42:58.443649+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04T11:42:58.443649+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0   Nebius  Global      H200  On-Demand       1h      None               2.3   \n",
       "1   Nebius  Global      H100  On-Demand       1h      None               2.0   \n",
       "\n",
       "                  source_url                    fetched_at_utc  \n",
       "0  https://nebius.com/prices  2025-09-04T11:42:58.443649+00:00  \n",
       "1  https://nebius.com/prices  2025-09-04T11:42:58.443649+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Nebius H100/H200 scraper (uses YOUR parsing + per-provider history) ---\n",
    "\n",
    "import re, time, tempfile, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "\n",
    "# ---------- your original config ----------\n",
    "url = \"https://nebius.com/prices\"\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\"}\n",
    "TZ = pytz.utc\n",
    "MIN_PRICE, MAX_PRICE = 0.3, 20.0   # sanity for $/GPU/hr\n",
    "\n",
    "# ---------- storage dirs (snapshot/history/latest) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # we'll fall back to tmp if write fails later\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(TZ).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- your original parsing (unchanged) ----------\n",
    "def find_price_strict(text: str):\n",
    "    \"\"\"Match $X/hr, $X per hour, $X/hour, case-insensitive.\"\"\"\n",
    "    if not text: return None\n",
    "    m = re.search(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", text, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def parse_nebius_from_html(html: str) -> dict:\n",
    "    \"\"\"Return {'H100': price, 'H200': price} if found.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = {}\n",
    "\n",
    "    # Focus on plausible pricing containers first (tables / pricing sections)\n",
    "    blocks = []\n",
    "    blocks.extend(soup.find_all(\"table\"))\n",
    "    if not blocks:\n",
    "        blocks.extend(soup.find_all([\"section\",\"div\"], class_=re.compile(\"price|pricing|compute\", re.I)))\n",
    "    if not blocks:\n",
    "        blocks = soup.find_all([\"div\",\"tr\",\"li\",\"p\",\"span\"])\n",
    "\n",
    "    for blk in blocks:\n",
    "        t = blk.get_text(\" \", strip=True)\n",
    "        if not t: continue\n",
    "\n",
    "        has_h100 = bool(re.search(r\"\\bH100\\b\", t, flags=re.I))\n",
    "        has_h200 = bool(re.search(r\"\\bH200\\b\", t, flags=re.I))\n",
    "        if not (has_h100 or has_h200): continue\n",
    "\n",
    "        price = find_price_strict(t)\n",
    "        if price is None or not (MIN_PRICE <= price <= MAX_PRICE): continue\n",
    "\n",
    "        if has_h100 and \"H100\" not in results:\n",
    "            results[\"H100\"] = price\n",
    "        if has_h200 and \"H200\" not in results:\n",
    "            results[\"H200\"] = price\n",
    "\n",
    "        if len(results) == 2:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------- run (your flow) ----------\n",
    "html = None\n",
    "try:\n",
    "    r = requests.get(url, headers=UA, timeout=30)\n",
    "    if r.status_code == 200 and r.text:\n",
    "        html = r.text\n",
    "except Exception:\n",
    "    html = None\n",
    "\n",
    "results = {}\n",
    "if html:\n",
    "    results = parse_nebius_from_html(html)\n",
    "\n",
    "# Optional Playwright fallback if nothing found\n",
    "if not results:\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "            page.goto(url, wait_until=\"networkidle\", timeout=60000)\n",
    "            page.wait_for_timeout(2000)  # allow dynamic content\n",
    "            html_pw = page.content()\n",
    "            browser.close()\n",
    "        results = parse_nebius_from_html(html_pw)\n",
    "    except Exception:\n",
    "        pass  # proceed with whatever we have\n",
    "\n",
    "# ---------- map YOUR results -> SLIM schema & save ----------\n",
    "rows = []\n",
    "ts = datetime.now(TZ).isoformat()\n",
    "for gpu, price in results.items():\n",
    "    rows.append({\n",
    "        \"provider\": \"Nebius\",\n",
    "        \"region\": \"Global\",            # keep simple; refine if you later detect regions\n",
    "        \"gpu_model\": gpu,              # map gpu_type -> gpu_model\n",
    "        \"type\": \"On-Demand\",\n",
    "        \"duration\": \"1h\",\n",
    "        \"gpu_count\": None,\n",
    "        \"price_hourly_usd\": price,     # map on_demand_price -> price_hourly_usd\n",
    "        \"source_url\": url,\n",
    "        \"fetched_at_utc\": ts,          # map scraped_at -> fetched_at_utc\n",
    "    })\n",
    "\n",
    "df_nebius = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "latest = _save_provider(df_nebius, \"nebius\")\n",
    "print(f\"Nebius rows this run: {len(df_nebius)}\")\n",
    "display(df_nebius.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[voltagepark] snapshot -> docs/data/snapshots/20250904_123626_voltagepark.csv\n",
      "[voltagepark] history  -> docs/data/history/voltagepark_history.csv\n",
      "[voltagepark] latest   -> docs/data/latest/voltagepark_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "      <td>2025-09-04 12:36:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region gpu_model       type duration gpu_count  \\\n",
       "0  VoltagePark     US      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                                         source_url  \\\n",
       "0              1.99  https://dashboard.voltagepark.com/order/config...   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-04 12:36:26  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= VoltagePark (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, tempfile, pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage + schema helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- YOUR scraping logic, adapted to slim schema ----\n",
    "async def scrape_voltagepark() -> pd.DataFrame:\n",
    "    url = \"https://dashboard.voltagepark.com/order/configure-deployment\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        await page.wait_for_timeout(5000)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    for line in html.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                # your original pattern\n",
    "                m = re.search(r\"\\$?(\\d+(?:\\.\\d+)?)(?=/GPU/hour)\", line)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    gpu = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                    rows.append({\n",
    "                        \"provider\": \"VoltagePark\",\n",
    "                        \"region\": \"US\",\n",
    "                        \"gpu_model\": gpu,\n",
    "                        \"type\": \"On-Demand\",\n",
    "                        \"duration\": \"1h\",\n",
    "                        \"gpu_count\": None,\n",
    "                        \"price_hourly_usd\": price,\n",
    "                        \"source_url\": url,\n",
    "                        \"fetched_at_utc\": _now_iso(),\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                # keep silent in prod; print minimal context if you want\n",
    "                # print(f\"[VoltagePark Parse Error] {e}\")\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # dedupe by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # sanity: plausible $/hr range\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in notebooks & scripts ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_voltage = arun(scrape_voltagepark())\n",
    "_save_provider(df_voltage, \"voltagepark\")\n",
    "display(df_voltage.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vastai] snapshot -> docs/data/snapshots/20250904_124214_vastai.csv\n",
      "[vastai] history  -> docs/data/history/vastai_history.csv\n",
      "[vastai] latest   -> docs/data/latest/vastai_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.25</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "      <td>2025-09-04 12:42:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0  Vast.ai  Global      H100  On-Demand       1h      None              1.25   \n",
       "\n",
       "                           source_url      fetched_at_utc  \n",
       "0  https://vast.ai/products/gpu-cloud 2025-09-04 12:42:14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Vast.ai (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled below\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- YOUR scraping logic, adapted to slim schema --------\n",
    "async def scrape_vast_products() -> pd.DataFrame:\n",
    "    url = \"https://vast.ai/products/gpu-cloud\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        # Try to reveal lazy content\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1500)\n",
    "        await page.evaluate(\"window.scrollTo(0, 0)\")\n",
    "        await page.wait_for_timeout(500)\n",
    "\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Your original approach: scan lines and pick $ numbers near H100/H200\n",
    "    for line in content.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                gpu_model = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                # pull all $-bearing tokens in the line\n",
    "                dollars = [s for s in re.split(r\"\\s+\", line) if \"$\" in s]\n",
    "                price_val = None\n",
    "                for token in dollars:\n",
    "                    clean = \"\".join(c for c in token if c.isdigit() or c == \".\")\n",
    "                    if not clean:\n",
    "                        continue\n",
    "                    price = float(clean)\n",
    "                    if 0.1 < price < 100:  # sanity filter like you had\n",
    "                        price_val = price\n",
    "                        break\n",
    "                if price_val is None:\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"Vast.ai\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": gpu_model,\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": None,\n",
    "                    \"price_hourly_usd\": price_val,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "            except Exception:\n",
    "                # swallow parse errors to keep the run clean\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # Deduplicate by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # Sanity clamp\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "df_vastp = arun(scrape_vast_products())\n",
    "_save_provider(df_vastp, \"vastai\")\n",
    "display(df_vastp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:49:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.35</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:49:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "1  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                 source_url      fetched_at_utc  \n",
       "0              1.99  https://www.shadeform.ai/ 2025-09-04 12:49:12  \n",
       "1              2.35  https://www.shadeform.ai/ 2025-09-04 12:49:12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Shadeform: precise matcher (nearest-price + hourly hint) ====\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# slim schema storage helpers (use the same ones you already have)\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "# --------- robust extractors ----------\n",
    "# require an hourly hint, allowing variants like \"/GPU/hour\"\n",
    "PRICE_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/GPU)?\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "GPU_TOKENS = {\n",
    "    \"H100\": re.compile(r\"\\bH100\\b\", re.I),\n",
    "    \"H200\": re.compile(r\"\\bH200\\b\", re.I),\n",
    "    # include B200 so we don't steal its prices\n",
    "    \"_OTHER\": re.compile(r\"\\b(?:B200|H800|A100|A800)\\b\", re.I),\n",
    "}\n",
    "\n",
    "def _find_token_positions(text: str):\n",
    "    positions = {k: [] for k in GPU_TOKENS.keys()}\n",
    "    for name, pat in GPU_TOKENS.items():\n",
    "        for m in pat.finditer(text):\n",
    "            positions[name].append(m.start())\n",
    "    return positions\n",
    "\n",
    "def _find_price_positions(text: str):\n",
    "    return [(float(m.group(1)), m.start()) for m in PRICE_RE.finditer(text)]\n",
    "\n",
    "def _nearest_price_to_token(text: str, token: str, window: int = 220):\n",
    "    \"\"\"Yield (model, price) pairs by attaching each token occurrence\n",
    "       to the nearest price with an hourly hint, only if it is closer\n",
    "       to this token than to any other GPU token.\"\"\"\n",
    "    tok_positions = _find_token_positions(text)\n",
    "    prices = _find_price_positions(text)\n",
    "    if not tok_positions.get(token) or not prices:\n",
    "        return []\n",
    "\n",
    "    # all GPU-ish positions (to compete for 'closeness')\n",
    "    competitor_positions = []\n",
    "    for k, pos_list in tok_positions.items():\n",
    "        if k == token:  # we compare against others later\n",
    "            continue\n",
    "        competitor_positions.extend(pos_list)\n",
    "\n",
    "    rows = []\n",
    "    for gpos in tok_positions[token]:\n",
    "        # candidates within a window around the GPU string\n",
    "        cands = [(price, ppos, abs(ppos - gpos)) for (price, ppos) in prices if abs(ppos - gpos) <= window]\n",
    "        if not cands:\n",
    "            continue\n",
    "        # pick nearest price to this token\n",
    "        price, ppos, dist = min(cands, key=lambda t: t[2])\n",
    "\n",
    "        # ensure this price isn't actually closer to another GPU token (e.g., B200)\n",
    "        if competitor_positions:\n",
    "            nearest_other = min(abs(ppos - op) for op in competitor_positions)\n",
    "            if nearest_other < dist:\n",
    "                continue  # skip: price belongs to another GPU mention\n",
    "\n",
    "        rows.append((token, price))\n",
    "    return rows\n",
    "\n",
    "# --------- scraper ----------\n",
    "async def scrape_shadeform_rich() -> pd.DataFrame:\n",
    "    url = \"https://www.shadeform.ai/\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(900)\n",
    "        body = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    body = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "    rows = []\n",
    "    for gpu in (\"H100\", \"H200\"):\n",
    "        for model, price in _nearest_price_to_token(body, gpu, window=220):\n",
    "            # sanity clamp to avoid accidental captures (tune if needed)\n",
    "            if not (0.25 <= price <= 8.0):\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"provider\": \"Shadeform\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": None,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# Example:\n",
    "df_shade = arun(scrape_shadeform_rich())\n",
    "display(df_shade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[coreweave] snapshot -> docs/data/snapshots/20250904_125500_coreweave.csv\n",
      "[coreweave] history  -> docs/data/history/coreweave_history.csv\n",
      "[coreweave] latest   -> docs/data/latest/coreweave_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>20.0</td>\n",
       "      <td>https://www.coreweave.com/pricing</td>\n",
       "      <td>2025-09-04 12:55:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0  CoreWeave     US      H100  On-Demand       1h      None              20.0   \n",
       "\n",
       "                          source_url      fetched_at_utc  \n",
       "0  https://www.coreweave.com/pricing 2025-09-04 12:55:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= CoreWeave (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- robust token/price matching --------\n",
    "PRICE_HOURLY_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:GPU\\s*/\\s*)?(?:h|hr|hour)\\b\",\n",
    "    re.I\n",
    ")\n",
    "# Fallback if the site omits 'hr' text; avoid monthly and memory suffixes\n",
    "PRICE_DOLLAR_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\b(?!\\s*(?:k|m|b|/mo|per\\s*month|/month|,?\\s*GB))\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "GPU_PATS = {\n",
    "    \"H100\": re.compile(r\"\\bH100\\b\", re.I),\n",
    "    \"H200\": re.compile(r\"\\bH200\\b\", re.I),\n",
    "    \"_OTHER\": re.compile(r\"\\b(?:B200|A100|A800|H800)\\b\", re.I),\n",
    "}\n",
    "\n",
    "def _find_positions(text: str, pat: re.Pattern):\n",
    "    return [m.start() for m in pat.finditer(text)]\n",
    "\n",
    "def _find_prices(text: str, prefer_hourly: bool = True):\n",
    "    pats = [PRICE_HOURLY_RE] + ([] if not prefer_hourly else [])  # first pass\n",
    "    prices = [(float(m.group(1)), m.start()) for m in PRICE_HOURLY_RE.finditer(text)]\n",
    "    if not prices:\n",
    "        prices = [(float(m.group(1)), m.start()) for m in PRICE_DOLLAR_RE.finditer(text)]\n",
    "    return prices\n",
    "\n",
    "def _nearest_prices(text: str, token: str, window: int = 240):\n",
    "    # Positions of our token vs. competitors\n",
    "    tok_pos = _find_positions(text, GPU_PATS[token])\n",
    "    if not tok_pos:\n",
    "        return []\n",
    "    comp_pos = []\n",
    "    for k, pat in GPU_PATS.items():\n",
    "        if k == token: continue\n",
    "        comp_pos.extend(_find_positions(text, pat))\n",
    "    prices = _find_prices(text)\n",
    "    out = []\n",
    "    for gpos in tok_pos:\n",
    "        cands = [(price, ppos, abs(ppos - gpos)) for (price, ppos) in prices if abs(ppos - gpos) <= window]\n",
    "        if not cands: \n",
    "            continue\n",
    "        price, ppos, dist = min(cands, key=lambda t: t[2])\n",
    "        if comp_pos:\n",
    "            nearest_other = min(abs(ppos - op) for op in comp_pos)\n",
    "            if nearest_other < dist:\n",
    "                continue\n",
    "        out.append((token, price))\n",
    "    return out\n",
    "\n",
    "# -------- CoreWeave scraper --------\n",
    "async def scrape_coreweave_async() -> pd.DataFrame:\n",
    "    url = \"https://www.coreweave.com/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help lazy content load\n",
    "        for _ in range(3):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "        # Wait for any pricing text to appear (best-effort)\n",
    "        try:\n",
    "            await page.wait_for_selector(\"text=/H100|H200/\", timeout=5000)\n",
    "        except Exception:\n",
    "            pass\n",
    "        body = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    body = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "    rows = []\n",
    "    for gpu in (\"H100\", \"H200\"):\n",
    "        for model, price in _nearest_prices(body, gpu, window=240):\n",
    "            # reasonable hourly range; widen if CoreWeave posts higher tiers\n",
    "            if not (0.25 <= price <= 25.0):\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"provider\": \"CoreWeave\",\n",
    "                \"region\": \"US\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": None,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # Deduplicate (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# -------- runner (works in scripts & notebooks) --------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_coreweave = arun(scrape_coreweave_async())\n",
    "latest_coreweave = _save_provider(df_coreweave, \"coreweave\")\n",
    "display(df_coreweave.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paperspace] snapshot -> docs/data/snapshots/20250904_130549_paperspace.csv\n",
      "[paperspace] history  -> docs/data/history/paperspace_history.csv\n",
      "[paperspace] latest   -> docs/data/latest/paperspace_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.24</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "      <td>2025-09-04 13:05:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Paperspace  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                          source_url      fetched_at_utc  \n",
       "0              2.24  https://www.paperspace.com/pricing 2025-09-04 13:05:49  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Paperspace (async) — use your approach, keep correct rows =================\n",
    "# Output schema (slim): provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                       price_hourly_usd, source_url, fetched_at_utc\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\"); HIST_DIR = BASE/\"history\"; SNAP_DIR = BASE/\"snapshots\"; LATEST_DIR = BASE/\"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso(): return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df); ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR/f\"{ts}_{slug}.csv\")\n",
    "    # history\n",
    "    hist = HIST_DIR/f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "                    .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                             \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "                    .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR/f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- strict extractors (but still tolerant to site markup) ----\n",
    "GPU_PAT = re.compile(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", re.I)\n",
    "# require an hourly hint somewhere in the same block to avoid platform prices, etc.\n",
    "PRICE_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:GPU\\s*/\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "async def scrape_paperspace() -> pd.DataFrame:\n",
    "    url = \"https://www.paperspace.com/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000)               # your simple navigation\n",
    "        await page.wait_for_timeout(8000)                  # your “just wait a few seconds”\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    rows = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # scan reasonable blocks; stick to your block-scan approach\n",
    "    for blk in soup.find_all([\"tr\",\"div\",\"section\",\"article\",\"li\"], recursive=True):\n",
    "        txt = blk.get_text(\" \", strip=True)\n",
    "        if not txt: \n",
    "            continue\n",
    "        # must mention H100/H200 AND 'hour' to qualify\n",
    "        if (\"H100\" not in txt and \"H200\" not in txt) or (\"hour\" not in txt.lower()):\n",
    "            continue\n",
    "\n",
    "        # model: first explicit H100/H200-ish token found\n",
    "        mm = GPU_PAT.search(txt)\n",
    "        if not mm:\n",
    "            continue\n",
    "        model = mm.group(1).upper()\n",
    "\n",
    "        # price: $… with an hourly hint in the same block\n",
    "        pm = PRICE_HOURLY.search(txt)\n",
    "        if not pm:\n",
    "            continue\n",
    "        price = float(pm.group(1))\n",
    "        # sanity band to drop weird captures\n",
    "        if not (0.2 <= price <= 50.0):\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"provider\": \"Paperspace\",\n",
    "            \"region\": \"Global\",\n",
    "            \"gpu_model\": model,          # \"H100\", \"H100 PCIE 80GB\", etc.\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": None,\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in both scripts & notebooks ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_paperspace = arun(scrape_paperspace())\n",
    "latest_paperspace = _save_provider(df_paperspace, \"paperspace\")\n",
    "display(df_paperspace.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensordock] snapshot -> docs/data/snapshots/20250904_131631_tensordock.csv\n",
      "[tensordock] history  -> docs/data/history/tensordock_history.csv\n",
      "[tensordock] latest   -> docs/data/latest/tensordock_latest.csv\n",
      "     provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  TensorDock  Global      H100  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                       source_url      fetched_at_utc  \n",
      "0              2.25  https://tensordock.com/gpu-h100 2025-09-04 13:16:31  \n"
     ]
    }
   ],
   "source": [
    "# ================= TensorDock H100 (static) — slim schema + per-provider history =================\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, requests, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- pages & patterns (from your code) --------\n",
    "PAGES = [\n",
    "    \"https://tensordock.com/gpu-h100\",\n",
    "    \"https://tensordock.com/cloud-gpus\",\n",
    "    \"https://tensordock.com/comparison-gcp\",\n",
    "]\n",
    "PATTERNS = [\n",
    "    re.compile(r\"H100.*?\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr\", re.I|re.S),\n",
    "    re.compile(r\"from\\s*\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr.*?H100\", re.I|re.S),\n",
    "    re.compile(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hour.*?H100\", re.I|re.S),\n",
    "]\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fall back handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- scraper (uses your logic, mapped to slim schema) --------\n",
    "def scrape_tensordock_public_h100() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for url in PAGES:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "            price = None\n",
    "            for pat in PATTERNS:\n",
    "                m = pat.search(text)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    break\n",
    "            if price and (0.2 <= price <= 50.0):  # sanity band for $/GPU/hr\n",
    "                rows.append({\n",
    "                    \"provider\": \"TensorDock\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": \"H100\",\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": 1,\n",
    "                    \"price_hourly_usd\": price,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"[TensorDock] {url} -> {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        return _ensure_slim(pd.DataFrame(columns=SLIM_COLS))\n",
    "\n",
    "    # Deduplicate: keep the **lowest** \"from\" price across pages\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = (df.sort_values(\"price_hourly_usd\")\n",
    "            .drop_duplicates(subset=[\"provider\",\"gpu_model\"], keep=\"first\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_tensordock = scrape_tensordock_public_h100()\n",
    "latest_tensordock = _save_provider(df_tensordock, \"tensordock\")\n",
    "print(df_tensordock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[brokkr] snapshot -> docs/data/snapshots/20250904_132203_brokkr.csv\n",
      "[brokkr] history  -> docs/data/history/brokkr_history.csv\n",
      "[brokkr] latest   -> docs/data/latest/brokkr_latest.csv\n",
      "              provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
      "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                              source_url  \\\n",
      "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
      "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
      "\n",
      "       fetched_at_utc  \n",
      "0 2025-09-04 13:22:03  \n",
      "1 2025-09-04 13:22:03  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              provider  region gpu_model       type duration  gpu_count  \\\n",
       "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
       "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                              source_url  \\\n",
       "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
       "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-04 13:22:03  \n",
       "1 2025-09-04 13:22:03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============== Hydra Host (Brokkr) — slim schema + per-provider history ==============\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same pattern as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- your Brokkr scraper, tightened to only accept \"per card-hour\" prices --------\n",
    "GPU_RE = r\"(H100|H200)\"\n",
    "PRICE_PER_CARDHR = r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:per\\s*card[-\\s]?hour|/card[-\\s]?hour)\\b\"\n",
    "PATS = [\n",
    "    re.compile(rf\"{GPU_RE}.{{0,220}}?{PRICE_PER_CARDHR}\", re.I | re.S),\n",
    "    re.compile(rf\"{PRICE_PER_CARDHR}.{{0,220}}?{GPU_RE}\", re.I | re.S),\n",
    "]\n",
    "\n",
    "async def scrape_brokkr() -> pd.DataFrame:\n",
    "    url = \"https://brokkr.hydrahost.com/inventory\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help hydrate lazy content\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(700)\n",
    "        body_text = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", body_text)\n",
    "    rows = []\n",
    "\n",
    "    for pat in PATS:\n",
    "        for m in pat.finditer(text):\n",
    "            # Depending on which pattern matched, group order differs\n",
    "            groups = m.groups()\n",
    "            # Normalize extraction: model + price are always present\n",
    "            if len(groups) == 2:\n",
    "                # pattern 1: (GPU, price)\n",
    "                gpu_model, price_str = groups\n",
    "            elif len(groups) == 3:\n",
    "                # pattern 2 returns (price, GPU) because of nested groups; pick numeric+gpu\n",
    "                # groups could be ('12.34', 'H100') or ('12.34', 'card-hour', 'H100') depending on regex engine\n",
    "                nums = [g for g in groups if g and re.fullmatch(r\"[0-9]+(?:\\.[0-9]+)?\", g)]\n",
    "                gpus = [g for g in groups if g and re.fullmatch(r\"H100|H200\", g, flags=re.I)]\n",
    "                if not nums or not gpus:\n",
    "                    continue\n",
    "                price_str, gpu_model = nums[0], gpus[0]\n",
    "            else:\n",
    "                # Safe fallback: find first number and first GPU token in the match\n",
    "                seg = m.group(0)\n",
    "                pm = re.search(r\"[0-9]+(?:\\.[0-9]+)?\", seg)\n",
    "                gm = re.search(r\"H100|H200\", seg, flags=re.I)\n",
    "                if not (pm and gm):\n",
    "                    continue\n",
    "                price_str, gpu_model = pm.group(0), gm.group(0)\n",
    "\n",
    "            try:\n",
    "                price = float(price_str)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # sanity band for per-card hour pricing\n",
    "            if not (0.2 <= price <= 50.0):\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"provider\": \"Hydra Host (Brokkr)\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": gpu_model.upper(),\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": price,   # per card-hour = per-GPU hourly\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# -------- runner that works in both notebooks & scripts --------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_brokkr = arun(scrape_brokkr())\n",
    "latest_brokkr = _save_provider(df_brokkr, \"brokkr\")\n",
    "print(df_brokkr)\n",
    "display(df_brokkr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[crusoecloud] snapshot -> docs/data/snapshots/20250904_132724_crusoecloud.csv\n",
      "[crusoecloud] history  -> docs/data/history/crusoecloud_history.csv\n",
      "[crusoecloud] latest   -> docs/data/latest/crusoecloud_latest.csv\n",
      "      provider  region gpu_model         type duration  gpu_count  \\\n",
      "0  CrusoeCloud  Global      H100    On-Demand       1h          1   \n",
      "1  CrusoeCloud  Global      H100  Reserved-1y       1h          1   \n",
      "2  CrusoeCloud  Global      H100  Reserved-3y       1h          1   \n",
      "3  CrusoeCloud  Global      H100  Reserved-6m       1h          1   \n",
      "4  CrusoeCloud  Global      H200    On-Demand       1h          1   \n",
      "5  CrusoeCloud  Global      H200  Reserved-1y       1h          1   \n",
      "6  CrusoeCloud  Global      H200  Reserved-3y       1h          1   \n",
      "7  CrusoeCloud  Global      H200  Reserved-6m       1h          1   \n",
      "\n",
      "   price_hourly_usd                           source_url      fetched_at_utc  \n",
      "0              3.90  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "1              2.93  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "2              2.54  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "3              3.12  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "4              4.29  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "5              3.22  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "6              2.79  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "7              3.43  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n"
     ]
    }
   ],
   "source": [
    "# ============ Crusoe Cloud (async) — table scrape → slim schema + history ============\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---------- slim schema + storage helpers (same as other providers) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{slug}.csv\")\n",
    "    # history append+dedupe\n",
    "    hist = HIST_DIR / f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "        .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "        .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                 \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "        .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest per gpu/type/region/duration\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- parsing helpers ----------\n",
    "PRICE_RE = re.compile(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "def _parse_price(cell_text: str):\n",
    "    if not cell_text: return None\n",
    "    m = PRICE_RE.search(cell_text.replace(\",\", \"\"))\n",
    "    if not m:\n",
    "        # fallback: plain $N.NN without explicit /hr\n",
    "        m2 = re.search(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\b\", cell_text.replace(\",\", \"\"))\n",
    "        return float(m2.group(1)) if m2 else None\n",
    "    return float(m.group(1))\n",
    "\n",
    "def _is_h_model(text: str) -> bool:\n",
    "    t = text.upper()\n",
    "    return (\"H100\" in t) or (\"H200\" in t)\n",
    "\n",
    "def _model_from(text: str) -> str:\n",
    "    return \"H100\" if \"H100\" in text.upper() else \"H200\"\n",
    "\n",
    "# ---------- scraper ----------\n",
    "async def scrape_crusoe_table() -> pd.DataFrame:\n",
    "    url = \"https://www.crusoe.ai/cloud/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help render\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows_out = []\n",
    "\n",
    "    # Find all table rows; filter to those that mention H100/H200\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        tds = [c.get_text(\" \", strip=True) for c in tr.find_all(\"td\")]\n",
    "        if not tds or not any(_is_h_model(c) for c in tds):\n",
    "            continue\n",
    "\n",
    "        model = _model_from(\" \".join(tds))\n",
    "\n",
    "        # Try to map columns conservatively:\n",
    "        # Common layout: [Model, On-Demand, Spot?, Reserved 6m, Reserved 1y, Reserved 3y, ...]\n",
    "        # We’ll grab by position if present, else try to read by header alignment.\n",
    "        on_demand = _parse_price(tds[1]) if len(tds) > 1 else None\n",
    "        res_6m    = _parse_price(tds[3]) if len(tds) > 3 else None\n",
    "        res_1y    = _parse_price(tds[4]) if len(tds) > 4 else None\n",
    "        res_3y    = _parse_price(tds[5]) if len(tds) > 5 else None\n",
    "\n",
    "        # Build normalised slim rows\n",
    "        if on_demand is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": on_demand,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_6m is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-6m\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_6m,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_1y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-1y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_1y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_3y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-3y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_3y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe & sanity\n",
    "    df = (df.sort_values([\"gpu_model\",\"type\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"type\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # plausible hourly band\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---------- runner that works in scripts & notebooks ----------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_crusoe = arun(scrape_crusoe_table())\n",
    "latest_crusoe = _save_provider(df_crusoe, \"crusoecloud\")\n",
    "print(df_crusoe.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   provider  region gpu_model       instance_type  gpu_count  \\\n",
      "0  OVHcloud  Global      H100  public-cloud/Price          2   \n",
      "1  OVHcloud  Global      H100  public-cloud/Price          4   \n",
      "\n",
      "   price_hourly_usd_instance  price_hourly_usd_per_gpu price_reserved_usd  \\\n",
      "0                       5.98                    2.9900               None   \n",
      "1                      11.97                    2.9925               None   \n",
      "\n",
      "  reserved_duration                   timestamp  \n",
      "0              None  2025-09-04T15:18:12.093698  \n",
      "1              None  2025-09-04T15:18:12.093773  \n"
     ]
    }
   ],
   "source": [
    "# OVHcloud H100/H200 — get the *correct per-GPU hourly price* from the public prices table\n",
    "# - Ties each $…/hour to the same row as H100/H200\n",
    "# - Extracts the GPU count from the row (1×/2×/4×/8× or “… GPUs”)\n",
    "# - per_gpu = instance_price / parsed_gpu_count  (NO 8× assumption)\n",
    "# - Returns both instance price and per-GPU price\n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def timestamp(): return datetime.utcnow().isoformat()\n",
    "\n",
    "USD_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "# GPU count detectors (row-level + cell/variant-level)\n",
    "COUNT_PATS = [\n",
    "    re.compile(r\"(\\d+)\\s*[×x]\\s*(?:NVIDIA\\s*)?(H100|H200)\\b\", re.I),  # \"8× H100\"\n",
    "    re.compile(r\"(H100|H200)\\s*[×x]\\s*(\\d+)\\b\", re.I),               # \"H100 × 8\"\n",
    "    re.compile(r\"(\\d+)\\s*(?:GPU|GPUs)\\b\", re.I),                     # \"8 GPUs\"\n",
    "    re.compile(r\"\\b(\\d+)\\s*[×x]\\b\", re.I),                           # \"4x\"\n",
    "]\n",
    "\n",
    "def _parse_count(text: str):\n",
    "    for pat in COUNT_PATS:\n",
    "        m = pat.search(text)\n",
    "        if not m: \n",
    "            continue\n",
    "        for g in m.groups():\n",
    "            if g and g.isdigit():\n",
    "                n = int(g)\n",
    "                if 1 <= n <= 16:\n",
    "                    return n\n",
    "    return None\n",
    "\n",
    "def scrape_ovhcloud_correct(url=\"https://www.ovhcloud.com/en/public-cloud/prices/\") -> pd.DataFrame:\n",
    "    html = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=60).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "    for table in soup.select(\"table\"):\n",
    "        # headers so we can label which column the number came from\n",
    "        headers = [th.get_text(\" \", strip=True) for th in table.select(\"thead th\")]\n",
    "        if not headers:\n",
    "            first = table.find(\"tr\")\n",
    "            if first:\n",
    "                headers = [td.get_text(\" \", strip=True) for td in first.find_all([\"th\",\"td\"])]\n",
    "\n",
    "        body_rows = table.select(\"tbody tr\") or table.select(\"tr\")\n",
    "        for tr in body_rows:\n",
    "            tds = tr.find_all(\"td\")\n",
    "            if not tds: \n",
    "                continue\n",
    "            cells = [td.get_text(\" \", strip=True) for td in tds]\n",
    "            row_txt = \" \".join(cells)\n",
    "            up = row_txt.upper()\n",
    "            if (\"H100\" not in up) and (\"H200\" not in up):\n",
    "                continue\n",
    "\n",
    "            model = \"H100\" if \"H100\" in up else \"H200\"\n",
    "            row_count = _parse_count(row_txt)\n",
    "\n",
    "            for idx, (td, cell) in enumerate(zip(tds, cells)):\n",
    "                m = USD_HOURLY.search(cell)\n",
    "                if not m:\n",
    "                    continue\n",
    "                instance_price = float(m.group(1))\n",
    "\n",
    "                # try counts in cell and header/variant too\n",
    "                var = headers[idx] if idx < len(headers) and headers else f\"col_{idx+1}\"\n",
    "                count = (\n",
    "                    _parse_count(cell) or\n",
    "                    _parse_count(var)  or\n",
    "                    row_count\n",
    "                )\n",
    "                if count is None:\n",
    "                    # if we can't prove node size, skip (prevents wrong divide)\n",
    "                    continue\n",
    "\n",
    "                per_gpu = instance_price / count\n",
    "                if not (0.25 <= per_gpu <= 20.0):\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"OVHcloud\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": model,\n",
    "                    \"instance_type\": f\"public-cloud/{var}\",\n",
    "                    \"gpu_count\": int(count),\n",
    "                    \"price_hourly_usd_instance\": round(instance_price, 4),\n",
    "                    \"price_hourly_usd_per_gpu\": round(per_gpu, 4),\n",
    "                    \"price_reserved_usd\": None,\n",
    "                    \"reserved_duration\": None,\n",
    "                    \"timestamp\": timestamp(),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = (df.sort_values([\"gpu_model\",\"price_hourly_usd_instance\"])\n",
    "                .drop_duplicates(subset=[\"gpu_model\",\"instance_type\",\"price_hourly_usd_instance\"], keep=\"last\")\n",
    "                .reset_index(drop=True))\n",
    "    return df\n",
    "\n",
    "# Example\n",
    "df_ovh = scrape_ovhcloud_correct()\n",
    "print(df_ovh.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Aggregate all providers → baseline market → score → save derived CSVs ===\n",
    "# Input: docs/data/latest/*_latest.csv (slim schema)\n",
    "# Output: docs/data/derived/market_index.csv, provider_scores_latest.csv, price_iq_latest.json\n",
    "\n",
    "import pandas as pd, numpy as np, json, tempfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Paths\n",
    "BASE = Path(\"docs/data\")\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "DERIVED_DIR = BASE / \"derived\"\n",
    "for d in (LATEST_DIR, DERIVED_DIR, BASE / \"history\", BASE / \"snapshots\"):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso(): \n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path) -> Path:\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "# ---------------- 1) Load & unify (build df FIRST) ----------------\n",
    "frames = []\n",
    "for p in sorted(LATEST_DIR.glob(\"*_latest.csv\")):\n",
    "    try:\n",
    "        d = pd.read_csv(p)\n",
    "        for c in SLIM_COLS:\n",
    "            if c not in d.columns: d[c] = None\n",
    "        frames.append(d[SLIM_COLS])\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load {p.name}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No latest provider CSVs found in docs/data/latest/\")\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# Basic hygiene\n",
    "raw[\"gpu_model\"] = raw[\"gpu_model\"].astype(str).str.upper().str.strip()\n",
    "raw[\"region\"]    = raw[\"region\"].astype(str).replace({\"nan\": None}).fillna(\"Global\").str.strip()\n",
    "raw[\"type\"]      = raw[\"type\"].astype(str).replace({\"nan\": None}).fillna(\"On-Demand\").str.strip()\n",
    "raw[\"duration\"]  = raw[\"duration\"].astype(str).replace({\"nan\": None}).fillna(\"1h\").str.strip()\n",
    "raw[\"gpu_count\"] = pd.to_numeric(raw[\"gpu_count\"], errors=\"coerce\")\n",
    "raw[\"price_hourly_usd\"] = pd.to_numeric(raw[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "raw[\"fetched_at_utc\"]   = pd.to_datetime(raw[\"fetched_at_utc\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Only H100/H200 and sane prices\n",
    "df = raw[\n",
    "    raw[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "].copy()\n",
    "df = df[(df[\"price_hourly_usd\"] > 0.05) & (df[\"price_hourly_usd\"] < 200)]\n",
    "\n",
    "# De-dup (latest wins)\n",
    "dedupe_keys = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\"]\n",
    "df = (df.sort_values(\"fetched_at_utc\")\n",
    "        .drop_duplicates(subset=dedupe_keys, keep=\"last\")\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# ---------------- 2) QA (now that df exists) ----------------\n",
    "qa = df.copy()\n",
    "\n",
    "# OVH must come from the correct scraper (per-GPU ~ $2–$4 for H100)\n",
    "qa = qa[~(\n",
    "    (qa[\"provider\"].eq(\"OVHcloud\")) &\n",
    "    (qa[\"gpu_model\"].eq(\"H100\")) &\n",
    "    ((qa[\"price_hourly_usd\"] < 2.0) | (qa[\"price_hourly_usd\"] > 4.0))\n",
    ")]\n",
    "\n",
    "# Shadeform: drop obvious marketing/outlier rows\n",
    "qa = qa[~(\n",
    "    (qa[\"provider\"].eq(\"Shadeform\")) &\n",
    "    (qa[\"price_hourly_usd\"] > 6.0)\n",
    ")]\n",
    "\n",
    "if qa.empty:\n",
    "    raise RuntimeError(\"No rows after QA; check scrapers or relax QA filters.\")\n",
    "\n",
    "# ---------------- 3) Market baselines from On-Demand only ----------------\n",
    "GROUP = [\"gpu_model\", \"region\"]\n",
    "od = qa[qa[\"type\"].eq(\"On-Demand\")].copy()\n",
    "if od.empty:\n",
    "    raise RuntimeError(\"No On-Demand rows to build market baselines.\")\n",
    "\n",
    "g = od.groupby(GROUP)[\"price_hourly_usd\"]\n",
    "market = pd.DataFrame({\n",
    "    \"market_count\": g.size(),\n",
    "    \"market_median\": g.median(),\n",
    "    \"market_mean\": g.mean(),\n",
    "    \"market_p25\": g.quantile(0.25),\n",
    "    \"market_p75\": g.quantile(0.75),\n",
    "}).reset_index()\n",
    "market[\"market_iqr\"] = market[\"market_p75\"] - market[\"market_p25\"]\n",
    "market[\"asof_utc\"] = _now_iso()\n",
    "\n",
    "# ---------------- 4) Score all rows vs market ----------------\n",
    "scored = qa.merge(market, on=GROUP, how=\"left\")\n",
    "scored[\"premium_vs_median\"] = (scored[\"price_hourly_usd\"] - scored[\"market_median\"]) / scored[\"market_median\"]\n",
    "iqr = scored[\"market_iqr\"].replace(0, np.nan)\n",
    "scored[\"is_outlier\"] = (\n",
    "    (scored[\"price_hourly_usd\"] < scored[\"market_p25\"] - 1.5*iqr) |\n",
    "    (scored[\"price_hourly_usd\"] > scored[\"market_p75\"] + 1.5*iqr)\n",
    ")\n",
    "# Price-IQ score (100 = at median; higher = cheaper)\n",
    "scored[\"price_score\"] = (scored[\"market_median\"] / scored[\"price_hourly_usd\"] * 100).clip(50, 150).round(1)\n",
    "\n",
    "# ---------------- 5) Save derived artifacts ----------------\n",
    "market_path = _safe_to_csv(market, DERIVED_DIR / \"market_index.csv\")\n",
    "scores_path = _safe_to_csv(scored, DERIVED_DIR / \"provider_scores_latest.csv\")\n",
    "\n",
    "# Lightweight JSON for dashboard (On-Demand leaderboard)\n",
    "leaderboard = (scored[scored[\"type\"].eq(\"On-Demand\")]\n",
    "               .sort_values([\"gpu_model\",\"region\",\"price_score\"], ascending=[True, True, False]))\n",
    "(lb := leaderboard[[\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"price_score\",\"premium_vs_median\",\"source_url\"]].copy()) \\\n",
    "    .assign(premium_vs_median=lambda x: x[\"premium_vs_median\"].round(4)) \\\n",
    "    .to_json(DERIVED_DIR / \"price_iq_latest.json\", orient=\"records\", indent=2)\n",
    "\n",
    "print(\"Derived saved:\\n- \", market_path, \"\\n- \", scores_path, \"\\n- \", DERIVED_DIR / \"price_iq_latest.json\")\n",
    "print(leaderboard.head(12)[[\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"price_score\"]].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derived saved:\n",
      " - docs/data/derived/market_index.csv\n",
      " - docs/data/derived/provider_scores_latest.csv\n",
      " - docs/data/derived/price_iq_latest.json\n",
      "\n",
      "Top On-Demand by price_score (first 12):\n",
      "            provider  region gpu_model       type  price_hourly_usd  price_score  premium_vs_median                                                    source_url\n",
      "             Vast.ai  Global      H100  On-Demand              1.25        150.0          -0.443207                            https://vast.ai/products/gpu-cloud\n",
      "              Nebius  Global      H100  On-Demand              2.00        112.2          -0.109131                                     https://nebius.com/prices\n",
      "          Paperspace  Global      H100  On-Demand              2.24        100.2          -0.002227                            https://www.paperspace.com/pricing\n",
      "          TensorDock  Global      H100  On-Demand              2.25         99.8           0.002227                               https://tensordock.com/gpu-h100\n",
      " Hydra Host (Brokkr)  Global      H100  On-Demand              2.30         97.6           0.024499                        https://brokkr.hydrahost.com/inventory\n",
      "         CrusoeCloud  Global      H100  On-Demand              3.90         57.6           0.737194                           https://www.crusoe.ai/cloud/pricing\n",
      "         VoltagePark      US      H100  On-Demand              1.99        150.0          -0.819009  https://dashboard.voltagepark.com/order/configure-deployment\n",
      "           CoreWeave      US      H100  On-Demand             20.00         55.0           0.819009                             https://www.coreweave.com/pricing\n",
      "              Nebius  Global      H200  On-Demand              2.30        107.6          -0.070707                                     https://nebius.com/prices\n",
      "           Shadeform  Global      H200  On-Demand              2.45        101.0          -0.010101                                     https://www.shadeform.ai/\n",
      " Hydra Host (Brokkr)  Global      H200  On-Demand              2.50         99.0           0.010101                        https://brokkr.hydrahost.com/inventory\n",
      "         CrusoeCloud  Global      H200  On-Demand              4.29         57.7           0.733333                           https://www.crusoe.ai/cloud/pricing\n"
     ]
    }
   ],
   "source": [
    "# ================================== PRICE IQ AGGREGATOR (FULL, CORRECT) ==================================\n",
    "# Loads all docs/data/latest/*_latest.csv (slim schema), validates, builds market baselines,\n",
    "# computes a simple “Price-IQ” score, and writes derived artifacts for your dashboard.\n",
    "#\n",
    "# Outputs (with safe fallback to tmp dir if docs/ is read-only):\n",
    "#   - docs/data/derived/market_index.csv\n",
    "#   - docs/data/derived/provider_scores_latest.csv\n",
    "#   - docs/data/derived/price_iq_latest.json   (lightweight leaderboard feed)\n",
    "#\n",
    "# Assumptions / fixes:\n",
    "#   - Baselines are computed from **On-Demand** rows only.\n",
    "#   - OVHcloud (H100) rows are kept only when per-GPU price is in a plausible band (≈ $2–$4).\n",
    "#   - Shadeform obvious marketing/outlier rows are dropped (> $6/GPU/hr).\n",
    "#   - Only H100/H200, per-GPU hourly (already normalized by scrapers).\n",
    "#   - Dedupe: newest fetched_at_utc wins per (provider, region, gpu_model, type, duration).\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import pandas as pd, numpy as np, json, tempfile, os\n",
    "\n",
    "# ----------------------------- config / schema -----------------------------\n",
    "BASE = Path(\"docs/data\")\n",
    "LATEST_DIR  = BASE / \"latest\"\n",
    "DERIVED_DIR = BASE / \"derived\"\n",
    "\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path) -> Path:\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        # read-only workspace fallback\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "# ----------------------------- load all latest -----------------------------\n",
    "frames = []\n",
    "for p in sorted(LATEST_DIR.glob(\"*_latest.csv\")):\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        # enforce contract\n",
    "        for c in SLIM_COLS:\n",
    "            if c not in df.columns:\n",
    "                df[c] = None\n",
    "        df = df[SLIM_COLS]\n",
    "        frames.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load {p.name}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No provider files found in docs/data/latest/*.csv\")\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# ----------------------------- hygiene / normalization -----------------------------\n",
    "raw[\"provider\"] = raw[\"provider\"].astype(str).str.strip()\n",
    "raw[\"region\"]   = raw[\"region\"].astype(str).replace({\"nan\": None}).fillna(\"Global\").str.strip()\n",
    "raw[\"gpu_model\"]= raw[\"gpu_model\"].astype(str).str.upper().str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "raw[\"type\"]     = raw[\"type\"].astype(str).replace({\"nan\": None}).fillna(\"On-Demand\").str.strip()\n",
    "raw[\"duration\"] = raw[\"duration\"].astype(str).replace({\"nan\": None}).fillna(\"1h\").str.strip()\n",
    "\n",
    "raw[\"gpu_count\"]        = pd.to_numeric(raw[\"gpu_count\"], errors=\"coerce\")\n",
    "raw[\"price_hourly_usd\"] = pd.to_numeric(raw[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "raw[\"fetched_at_utc\"]   = pd.to_datetime(raw[\"fetched_at_utc\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# Only H100/H200 with sane price band (broad)\n",
    "df = raw[\n",
    "    raw[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "].copy()\n",
    "df = df[(df[\"price_hourly_usd\"] > 0.05) & (df[\"price_hourly_usd\"] < 200)]\n",
    "\n",
    "# Keep the newest row per (provider, region, model, type, duration)\n",
    "dedupe_keys = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\"]\n",
    "df = (df.sort_values(\"fetched_at_utc\")\n",
    "        .drop_duplicates(subset=dedupe_keys, keep=\"last\")\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# ----------------------------- QA filters (provider-specific sanity) -----------------------------\n",
    "# OVHcloud H100: keep only realistic per-GPU prices (we parse correct counts in its scraper; ~ $2–$4/GPU/hr)\n",
    "mask_ovh_bad = (\n",
    "    (df[\"provider\"].eq(\"OVHcloud\")) &\n",
    "    (df[\"gpu_model\"].eq(\"H100\")) &\n",
    "    ((df[\"price_hourly_usd\"] < 2.0) | (df[\"price_hourly_usd\"] > 4.0))\n",
    ")\n",
    "df = df[~mask_ovh_bad]\n",
    "\n",
    "# Shadeform: drop obviously bogus rows (marketing blurbs that the scraper might catch)\n",
    "mask_shade_bad = (df[\"provider\"].eq(\"Shadeform\")) & (df[\"price_hourly_usd\"] > 6.0)\n",
    "df = df[~mask_shade_bad]\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# ----------------------------- market baselines (On-Demand only) -----------------------------\n",
    "GROUP = [\"gpu_model\", \"region\"]  # change to [\"gpu_model\"] for global-only baselines\n",
    "od = df[df[\"type\"].eq(\"On-Demand\")].copy()\n",
    "\n",
    "if od.empty:\n",
    "    raise RuntimeError(\"No On-Demand rows after QA; cannot build baselines.\")\n",
    "\n",
    "g = od.groupby(GROUP)[\"price_hourly_usd\"]\n",
    "market = pd.DataFrame({\n",
    "    \"market_count\": g.size(),\n",
    "    \"market_median\": g.median(),\n",
    "    \"market_mean\": g.mean(),\n",
    "    \"market_p25\": g.quantile(0.25),\n",
    "    \"market_p75\": g.quantile(0.75),\n",
    "}).reset_index()\n",
    "market[\"market_iqr\"] = (market[\"market_p75\"] - market[\"market_p25\"]).round(6)\n",
    "market[\"asof_utc\"] = _now_iso()\n",
    "\n",
    "# ----------------------------- score all rows vs On-Demand baselines -----------------------------\n",
    "scored = df.merge(market, on=GROUP, how=\"left\")\n",
    "\n",
    "# Premium vs median (negative = cheaper than market)\n",
    "scored[\"premium_vs_median\"] = (scored[\"price_hourly_usd\"] - scored[\"market_median\"]) / scored[\"market_median\"]\n",
    "\n",
    "# Tukey outliers vs IQR (flag only; do not drop in case you want to display)\n",
    "iqr = scored[\"market_iqr\"].replace(0, np.nan)\n",
    "scored[\"is_outlier\"] = (\n",
    "    (scored[\"price_hourly_usd\"] < scored[\"market_p25\"] - 1.5*iqr) |\n",
    "    (scored[\"price_hourly_usd\"] > scored[\"market_p75\"] + 1.5*iqr)\n",
    ")\n",
    "\n",
    "scored[\"price_score\"] = (scored[\"market_median\"] / scored[\"price_hourly_usd\"] * 100).clip(50, 150).round(1)\n",
    "\n",
    "market_path = _safe_to_csv(market, DERIVED_DIR / \"market_index.csv\")\n",
    "scores_path = _safe_to_csv(scored, DERIVED_DIR / \"provider_scores_latest.csv\")\n",
    "\n",
    "leaderboard = (scored[scored[\"type\"].eq(\"On-Demand\")]\n",
    "               .sort_values([\"gpu_model\",\"region\",\"price_score\"], ascending=[True, True, False]))\n",
    "json_rows = (leaderboard[[\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"price_score\",\"premium_vs_median\",\"source_url\"\n",
    "]].copy())\n",
    "json_rows[\"premium_vs_median\"] = json_rows[\"premium_vs_median\"].round(4)\n",
    "\n",
    "json_path = DERIVED_DIR / \"price_iq_latest.json\"\n",
    "try:\n",
    "    json_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    json_rows.to_json(json_path, orient=\"records\", indent=2)\n",
    "except Exception:\n",
    "    json_path = Path(tempfile.gettempdir()) / json_path.name\n",
    "    json_rows.to_json(json_path, orient=\"records\", indent=2)\n",
    "\n",
    "print(\"Derived saved:\")\n",
    "print(\" -\", market_path)\n",
    "print(\" -\", scores_path)\n",
    "print(\" -\", json_path)\n",
    "\n",
    "preview_cols = [\"provider\",\"region\",\"gpu_model\",\"type\",\"price_hourly_usd\",\"price_score\",\"premium_vs_median\",\"source_url\"]\n",
    "print(\"\\nTop On-Demand by price_score (first 12):\")\n",
    "print(leaderboard[preview_cols].head(12).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
