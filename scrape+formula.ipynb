{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# single source of truth for dashboard outputs\n",
    "OUTPUT_DIR = Path(\"docs/data/derived\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_csv(df, name):\n",
    "    p = OUTPUT_DIR / name\n",
    "    df.to_csv(p, index=False)\n",
    "    print(f\"✔ saved: {p}\")\n",
    "\n",
    "# …then everywhere you save:\n",
    "# save_csv(provider_scores_df, \"provider_scores_latest.csv\")\n",
    "# save_csv(roi_df,              \"roi_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape provider\n",
    "# --- Async helper that works in notebooks and GitHub Actions ---\n",
    "import asyncio\n",
    "\n",
    "def await_safe(coro):\n",
    "    \"\"\"\n",
    "    Run an async coroutine from anywhere:\n",
    "    - If an event loop is already running (Jupyter/nbconvert), use nest_asyncio + run_until_complete\n",
    "    - Otherwise, use asyncio.run\n",
    "    \"\"\"\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            try:\n",
    "                import nest_asyncio\n",
    "                nest_asyncio.apply()\n",
    "            except Exception:\n",
    "                pass\n",
    "            return loop.run_until_complete(coro)\n",
    "        else:\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        # No current loop\n",
    "        return asyncio.run(coro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>8X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>8</td>\n",
       "      <td>2.99</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>4X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>4</td>\n",
       "      <td>3.09</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>2X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>2</td>\n",
       "      <td>3.19</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>1.49</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lambda Labs</td>\n",
       "      <td>US</td>\n",
       "      <td>1X H100 SXM</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>3.29</td>\n",
       "      <td>https://cloud.lambdalabs.com/pricing</td>\n",
       "      <td>2025-09-04 11:15:09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region    gpu_model       type duration  gpu_count  \\\n",
       "0  Lambda Labs     US  8X H100 SXM  On-Demand       1h          8   \n",
       "1  Lambda Labs     US  4X H100 SXM  On-Demand       1h          4   \n",
       "2  Lambda Labs     US  2X H100 SXM  On-Demand       1h          2   \n",
       "3  Lambda Labs     US      1X H200  On-Demand       1h          1   \n",
       "4  Lambda Labs     US  1X H100 SXM  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                            source_url      fetched_at_utc  \n",
       "0              2.99  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "1              3.09  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "2              3.19  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "3              1.49  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  \n",
       "4              3.29  https://cloud.lambdalabs.com/pricing 2025-09-04 11:15:09  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lambda Labs \n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from typing import Optional\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _norm_gpu(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")  # treat GH200 as H200\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(s: str) -> Optional[int]:\n",
    "    if not isinstance(s, str): \n",
    "        return None\n",
    "    m = re.search(r\"(\\d+)x\", s, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_in(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): \n",
    "        return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrapes https://cloud.lambdalabs.com/pricing and returns SLIM rows\n",
    "    for H100/H200 (On-Demand, 1h). If `region` provided, overrides detected region.\n",
    "    \"\"\"\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    rows_out = []\n",
    "    tables = soup.find_all(\"table\")\n",
    "    for table in tables:\n",
    "        tbl_region = region or _infer_region(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds:\n",
    "                continue\n",
    "            row_text = \" | \".join(tds)\n",
    "\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "\n",
    "            price = _price_in(row_text)\n",
    "            if price is None:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            gpu_model = _norm_gpu(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            count = _gpu_count(gpu_model)\n",
    "\n",
    "            rows_out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": gpu_model,      \n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": count,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    df = df[keep].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# Example:\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "display(df_lambda.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[lambda_labs] snapshot -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/20250904_112436_lambda_labs.csv\n",
      "[lambda_labs] history  -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/lambda_labs_history.csv\n",
      "[lambda_labs] latest   -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/lambda_labs_latest.csv\n",
      "[runpod] snapshot -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/20250904_112441_runpod.csv\n",
      "[runpod] history  -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/runpod_history.csv\n",
      "[runpod] latest   -> /var/folders/2_/9wdv7zh56p95l_j0dkkc12zw0000gn/T/runpod_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200 141 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>3.59</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-04 11:24:41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RunPod</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100 PCIE 80 GB</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.runpod.io/pricing</td>\n",
       "      <td>2025-09-04 11:24:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region        gpu_model       type duration gpu_count  \\\n",
       "4   RunPod  Global      H200 141 GB  On-Demand       1h      None   \n",
       "6   RunPod  Global  H100 PCIE 80 GB  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                     source_url      fetched_at_utc  \n",
       "4              3.59  https://www.runpod.io/pricing 2025-09-04 11:24:41  \n",
       "6              1.99  https://www.runpod.io/pricing 2025-09-04 11:24:41  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===== Lambda Labs (static) + RunPod (async) with per-provider history =====\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, os, asyncio, pandas as pd, tempfile\n",
    "from typing import Optional, Dict, Any\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "# -------- storage (per-provider history/snapshots) --------\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        # fallback to tmp if workspace is read-only\n",
    "        tmp = Path(tempfile.gettempdir()) / \"gpu_data\"\n",
    "        d = tmp / d.name\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    try:\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    except Exception:\n",
    "        snap_path = Path(tempfile.gettempdir()) / f\"{ts}_{provider_slug}.csv\"\n",
    "        df.to_csv(snap_path, index=False)\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    try:\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    except Exception:\n",
    "        hist_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_history.csv\"\n",
    "        all_df.to_csv(hist_path, index=False)\n",
    "    # latest (newest rows only per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    try:\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    except Exception:\n",
    "        latest_path = Path(tempfile.gettempdir()) / f\"{provider_slug}_latest.csv\"\n",
    "        latest.to_csv(latest_path, index=False)\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ------------------------- Lambda Labs (static) -------------------------\n",
    "def _norm_gpu_lambda(s: str) -> str:\n",
    "    s = re.sub(r\"\\bon[-\\s]?demand\\b\", \"\", s, flags=re.I)\n",
    "    s = s.replace(\"NVIDIA\", \"\").strip()\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.upper().replace(\"GH200\", \"H200\")\n",
    "    return s.strip()\n",
    "\n",
    "def _gpu_count(text: str) -> Optional[int]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"(\\d+)\\s*x\", text, flags=re.I)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def _price_dollar(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def _infer_region_lambda(table) -> str:\n",
    "    hdr = table.find_previous([\"h2\",\"h3\",\"h4\",\"p\"])\n",
    "    if hdr:\n",
    "        t = hdr.get_text(\" \", strip=True).lower()\n",
    "        if \"europe\" in t or \"eu\" in t: return \"EU\"\n",
    "        if \"united states\" in t or \"us\" in t or \"usa\" in t: return \"US\"\n",
    "    return \"US\"\n",
    "\n",
    "def scrape_lambda_labs(region: Optional[str] = None) -> pd.DataFrame:\n",
    "    url = \"https://cloud.lambdalabs.com/pricing\"\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30); r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    out = []\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        tbl_region = region or _infer_region_lambda(table)\n",
    "        for tr in table.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds: continue\n",
    "            row_text = \" | \".join(tds)\n",
    "            if not (re.search(r\"\\bH100\\b\", row_text, re.I) or re.search(r\"\\bH200\\b|\\bGH200\\b\", row_text, re.I)):\n",
    "                continue\n",
    "            price = _price_dollar(row_text)\n",
    "            if price is None: continue\n",
    "            gpu_cell = next((c for c in tds if (\"H100\" in c.upper() or \"H200\" in c.upper() or \"GH200\" in c.upper())), None)\n",
    "            model = _norm_gpu_lambda(gpu_cell or (\"H100\" if \"H100\" in row_text.upper() else \"H200\"))\n",
    "            out.append({\n",
    "                \"provider\": \"Lambda Labs\",\n",
    "                \"region\": tbl_region,\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": _gpu_count(model),\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    keep = df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", regex=True, na=False)\n",
    "    return _ensure_slim(df[keep].reset_index(drop=True))\n",
    "\n",
    "# --------------------------- RunPod (async) ---------------------------\n",
    "def _extract_gpu_model_runpod(text: str) -> Optional[str]:\n",
    "    if not isinstance(text, str): return None\n",
    "    text_up = re.sub(r\"\\s+\", \" \", text.upper())\n",
    "    m = re.search(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", text_up)\n",
    "    return m.group(1).strip() if m else None\n",
    "\n",
    "def _price_hourly_runpod(text: str) -> Optional[float]:\n",
    "    if not isinstance(text, str): return None\n",
    "    t = text.replace(\",\", \"\")\n",
    "    m = re.search(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", t, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "async def scrape_runpod_async() -> pd.DataFrame:\n",
    "    from playwright.async_api import async_playwright\n",
    "    url = \"https://www.runpod.io/pricing\"; region = \"Global\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1200)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\"], class_=re.compile(r\"(price|pricing|card|grid|table)\", re.I))\n",
    "    if not nodes:\n",
    "        nodes = soup.find_all([\"section\",\"div\",\"article\",\"li\",\"tr\",\"p\",\"span\"])\n",
    "\n",
    "    out = []\n",
    "    for n in nodes:\n",
    "        text = n.get_text(\" \", strip=True)\n",
    "        if \"H100\" not in text and \"H200\" not in text: \n",
    "            continue\n",
    "        price = _price_hourly_runpod(text)\n",
    "        if price is None:\n",
    "            continue\n",
    "        model = _extract_gpu_model_runpod(text)\n",
    "        if model is None:\n",
    "            continue\n",
    "        out.append({\n",
    "            \"provider\": \"RunPod\",\n",
    "            \"region\": region,\n",
    "            \"gpu_model\": model,\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": _gpu_count(text),\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "    df = pd.DataFrame(out)\n",
    "    if df.empty: return _ensure_slim(df)\n",
    "    df = df[df[\"gpu_model\"].str.contains(r\"\\bH100\\b|\\bH200\\b\", na=False)]\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)].reset_index(drop=True)\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# --------------- Runner that works in scripts & notebooks ---------------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "# Lambda Labs\n",
    "df_lambda = scrape_lambda_labs(region=\"US\")\n",
    "_save_provider(df_lambda, \"lambda_labs\")\n",
    "\n",
    "# RunPod\n",
    "df_runpod = arun(scrape_runpod_async())\n",
    "_save_provider(df_runpod, \"runpod\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nebius] snapshot -> docs/data/snapshots/20250904_114258_nebius.csv\n",
      "[nebius] history  -> docs/data/history/nebius_history.csv\n",
      "[nebius] latest   -> docs/data/latest/nebius_latest.csv\n",
      "Nebius rows this run: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04T11:42:58.443649+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>https://nebius.com/prices</td>\n",
       "      <td>2025-09-04T11:42:58.443649+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0   Nebius  Global      H200  On-Demand       1h      None               2.3   \n",
       "1   Nebius  Global      H100  On-Demand       1h      None               2.0   \n",
       "\n",
       "                  source_url                    fetched_at_utc  \n",
       "0  https://nebius.com/prices  2025-09-04T11:42:58.443649+00:00  \n",
       "1  https://nebius.com/prices  2025-09-04T11:42:58.443649+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Nebius H100/H200 scraper (uses YOUR parsing + per-provider history) ---\n",
    "\n",
    "import re, time, tempfile, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pytz\n",
    "\n",
    "# ---------- your original config ----------\n",
    "url = \"https://nebius.com/prices\"\n",
    "UA = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124 Safari/537.36\"}\n",
    "TZ = pytz.utc\n",
    "MIN_PRICE, MAX_PRICE = 0.3, 20.0   # sanity for $/GPU/hr\n",
    "\n",
    "# ---------- storage dirs (snapshot/history/latest) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # we'll fall back to tmp if write fails later\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(TZ).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- your original parsing (unchanged) ----------\n",
    "def find_price_strict(text: str):\n",
    "    \"\"\"Match $X/hr, $X per hour, $X/hour, case-insensitive.\"\"\"\n",
    "    if not text: return None\n",
    "    m = re.search(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", text, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def parse_nebius_from_html(html: str) -> dict:\n",
    "    \"\"\"Return {'H100': price, 'H200': price} if found.\"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    results = {}\n",
    "\n",
    "    # Focus on plausible pricing containers first (tables / pricing sections)\n",
    "    blocks = []\n",
    "    blocks.extend(soup.find_all(\"table\"))\n",
    "    if not blocks:\n",
    "        blocks.extend(soup.find_all([\"section\",\"div\"], class_=re.compile(\"price|pricing|compute\", re.I)))\n",
    "    if not blocks:\n",
    "        blocks = soup.find_all([\"div\",\"tr\",\"li\",\"p\",\"span\"])\n",
    "\n",
    "    for blk in blocks:\n",
    "        t = blk.get_text(\" \", strip=True)\n",
    "        if not t: continue\n",
    "\n",
    "        has_h100 = bool(re.search(r\"\\bH100\\b\", t, flags=re.I))\n",
    "        has_h200 = bool(re.search(r\"\\bH200\\b\", t, flags=re.I))\n",
    "        if not (has_h100 or has_h200): continue\n",
    "\n",
    "        price = find_price_strict(t)\n",
    "        if price is None or not (MIN_PRICE <= price <= MAX_PRICE): continue\n",
    "\n",
    "        if has_h100 and \"H100\" not in results:\n",
    "            results[\"H100\"] = price\n",
    "        if has_h200 and \"H200\" not in results:\n",
    "            results[\"H200\"] = price\n",
    "\n",
    "        if len(results) == 2:\n",
    "            break\n",
    "\n",
    "    return results\n",
    "\n",
    "# ---------- run (your flow) ----------\n",
    "html = None\n",
    "try:\n",
    "    r = requests.get(url, headers=UA, timeout=30)\n",
    "    if r.status_code == 200 and r.text:\n",
    "        html = r.text\n",
    "except Exception:\n",
    "    html = None\n",
    "\n",
    "results = {}\n",
    "if html:\n",
    "    results = parse_nebius_from_html(html)\n",
    "\n",
    "# Optional Playwright fallback if nothing found\n",
    "if not results:\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            page = browser.new_page()\n",
    "            page.goto(url, wait_until=\"networkidle\", timeout=60000)\n",
    "            page.wait_for_timeout(2000)  # allow dynamic content\n",
    "            html_pw = page.content()\n",
    "            browser.close()\n",
    "        results = parse_nebius_from_html(html_pw)\n",
    "    except Exception:\n",
    "        pass  # proceed with whatever we have\n",
    "\n",
    "# ---------- map YOUR results -> SLIM schema & save ----------\n",
    "rows = []\n",
    "ts = datetime.now(TZ).isoformat()\n",
    "for gpu, price in results.items():\n",
    "    rows.append({\n",
    "        \"provider\": \"Nebius\",\n",
    "        \"region\": \"Global\",            # keep simple; refine if you later detect regions\n",
    "        \"gpu_model\": gpu,              # map gpu_type -> gpu_model\n",
    "        \"type\": \"On-Demand\",\n",
    "        \"duration\": \"1h\",\n",
    "        \"gpu_count\": None,\n",
    "        \"price_hourly_usd\": price,     # map on_demand_price -> price_hourly_usd\n",
    "        \"source_url\": url,\n",
    "        \"fetched_at_utc\": ts,          # map scraped_at -> fetched_at_utc\n",
    "    })\n",
    "\n",
    "df_nebius = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "latest = _save_provider(df_nebius, \"nebius\")\n",
    "print(f\"Nebius rows this run: {len(df_nebius)}\")\n",
    "display(df_nebius.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[voltagepark] snapshot -> docs/data/snapshots/20250904_123626_voltagepark.csv\n",
      "[voltagepark] history  -> docs/data/history/voltagepark_history.csv\n",
      "[voltagepark] latest   -> docs/data/latest/voltagepark_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://dashboard.voltagepark.com/order/config...</td>\n",
       "      <td>2025-09-04 12:36:26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      provider region gpu_model       type duration gpu_count  \\\n",
       "0  VoltagePark     US      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                                         source_url  \\\n",
       "0              1.99  https://dashboard.voltagepark.com/order/config...   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-04 12:36:26  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= VoltagePark (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, tempfile, pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage + schema helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- YOUR scraping logic, adapted to slim schema ----\n",
    "async def scrape_voltagepark() -> pd.DataFrame:\n",
    "    url = \"https://dashboard.voltagepark.com/order/configure-deployment\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        await page.wait_for_timeout(5000)\n",
    "\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    for line in html.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                # your original pattern\n",
    "                m = re.search(r\"\\$?(\\d+(?:\\.\\d+)?)(?=/GPU/hour)\", line)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    gpu = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                    rows.append({\n",
    "                        \"provider\": \"VoltagePark\",\n",
    "                        \"region\": \"US\",\n",
    "                        \"gpu_model\": gpu,\n",
    "                        \"type\": \"On-Demand\",\n",
    "                        \"duration\": \"1h\",\n",
    "                        \"gpu_count\": None,\n",
    "                        \"price_hourly_usd\": price,\n",
    "                        \"source_url\": url,\n",
    "                        \"fetched_at_utc\": _now_iso(),\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                # keep silent in prod; print minimal context if you want\n",
    "                # print(f\"[VoltagePark Parse Error] {e}\")\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # dedupe by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # sanity: plausible $/hr range\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in notebooks & scripts ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_voltage = arun(scrape_voltagepark())\n",
    "_save_provider(df_voltage, \"voltagepark\")\n",
    "display(df_voltage.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vastai] snapshot -> docs/data/snapshots/20250904_124214_vastai.csv\n",
      "[vastai] history  -> docs/data/history/vastai_history.csv\n",
      "[vastai] latest   -> docs/data/latest/vastai_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.25</td>\n",
       "      <td>https://vast.ai/products/gpu-cloud</td>\n",
       "      <td>2025-09-04 12:42:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  provider  region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0  Vast.ai  Global      H100  On-Demand       1h      None              1.25   \n",
       "\n",
       "                           source_url      fetched_at_utc  \n",
       "0  https://vast.ai/products/gpu-cloud 2025-09-04 12:42:14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Vast.ai (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled below\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- YOUR scraping logic, adapted to slim schema --------\n",
    "async def scrape_vast_products() -> pd.DataFrame:\n",
    "    url = \"https://vast.ai/products/gpu-cloud\"\n",
    "    rows = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000)\n",
    "        # Try to reveal lazy content\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(1500)\n",
    "        await page.evaluate(\"window.scrollTo(0, 0)\")\n",
    "        await page.wait_for_timeout(500)\n",
    "\n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    # Your original approach: scan lines and pick $ numbers near H100/H200\n",
    "    for line in content.splitlines():\n",
    "        if (\"H100\" in line or \"H200\" in line) and \"$\" in line:\n",
    "            try:\n",
    "                gpu_model = \"H100\" if \"H100\" in line else \"H200\"\n",
    "                # pull all $-bearing tokens in the line\n",
    "                dollars = [s for s in re.split(r\"\\s+\", line) if \"$\" in s]\n",
    "                price_val = None\n",
    "                for token in dollars:\n",
    "                    clean = \"\".join(c for c in token if c.isdigit() or c == \".\")\n",
    "                    if not clean:\n",
    "                        continue\n",
    "                    price = float(clean)\n",
    "                    if 0.1 < price < 100:  # sanity filter like you had\n",
    "                        price_val = price\n",
    "                        break\n",
    "                if price_val is None:\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"Vast.ai\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": gpu_model,\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": None,\n",
    "                    \"price_hourly_usd\": price_val,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "            except Exception:\n",
    "                # swallow parse errors to keep the run clean\n",
    "                pass\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # Deduplicate by (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # Sanity clamp\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "df_vastp = arun(scrape_vast_products())\n",
    "_save_provider(df_vastp, \"vastai\")\n",
    "display(df_vastp.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>1.99</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:49:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.35</td>\n",
       "      <td>https://www.shadeform.ai/</td>\n",
       "      <td>2025-09-04 12:49:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "1  Shadeform  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                 source_url      fetched_at_utc  \n",
       "0              1.99  https://www.shadeform.ai/ 2025-09-04 12:49:12  \n",
       "1              2.35  https://www.shadeform.ai/ 2025-09-04 12:49:12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== Shadeform: precise matcher (nearest-price + hourly hint) ====\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# slim schema storage helpers (use the same ones you already have)\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "\n",
    "def _now_iso():\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "# --------- robust extractors ----------\n",
    "# require an hourly hint, allowing variants like \"/GPU/hour\"\n",
    "PRICE_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/GPU)?\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "GPU_TOKENS = {\n",
    "    \"H100\": re.compile(r\"\\bH100\\b\", re.I),\n",
    "    \"H200\": re.compile(r\"\\bH200\\b\", re.I),\n",
    "    # include B200 so we don't steal its prices\n",
    "    \"_OTHER\": re.compile(r\"\\b(?:B200|H800|A100|A800)\\b\", re.I),\n",
    "}\n",
    "\n",
    "def _find_token_positions(text: str):\n",
    "    positions = {k: [] for k in GPU_TOKENS.keys()}\n",
    "    for name, pat in GPU_TOKENS.items():\n",
    "        for m in pat.finditer(text):\n",
    "            positions[name].append(m.start())\n",
    "    return positions\n",
    "\n",
    "def _find_price_positions(text: str):\n",
    "    return [(float(m.group(1)), m.start()) for m in PRICE_RE.finditer(text)]\n",
    "\n",
    "def _nearest_price_to_token(text: str, token: str, window: int = 220):\n",
    "    \"\"\"Yield (model, price) pairs by attaching each token occurrence\n",
    "       to the nearest price with an hourly hint, only if it is closer\n",
    "       to this token than to any other GPU token.\"\"\"\n",
    "    tok_positions = _find_token_positions(text)\n",
    "    prices = _find_price_positions(text)\n",
    "    if not tok_positions.get(token) or not prices:\n",
    "        return []\n",
    "\n",
    "    # all GPU-ish positions (to compete for 'closeness')\n",
    "    competitor_positions = []\n",
    "    for k, pos_list in tok_positions.items():\n",
    "        if k == token:  # we compare against others later\n",
    "            continue\n",
    "        competitor_positions.extend(pos_list)\n",
    "\n",
    "    rows = []\n",
    "    for gpos in tok_positions[token]:\n",
    "        # candidates within a window around the GPU string\n",
    "        cands = [(price, ppos, abs(ppos - gpos)) for (price, ppos) in prices if abs(ppos - gpos) <= window]\n",
    "        if not cands:\n",
    "            continue\n",
    "        # pick nearest price to this token\n",
    "        price, ppos, dist = min(cands, key=lambda t: t[2])\n",
    "\n",
    "        # ensure this price isn't actually closer to another GPU token (e.g., B200)\n",
    "        if competitor_positions:\n",
    "            nearest_other = min(abs(ppos - op) for op in competitor_positions)\n",
    "            if nearest_other < dist:\n",
    "                continue  # skip: price belongs to another GPU mention\n",
    "\n",
    "        rows.append((token, price))\n",
    "    return rows\n",
    "\n",
    "# --------- scraper ----------\n",
    "async def scrape_shadeform_rich() -> pd.DataFrame:\n",
    "    url = \"https://www.shadeform.ai/\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=60000, wait_until=\"domcontentloaded\")\n",
    "        await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(900)\n",
    "        body = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    body = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "    rows = []\n",
    "    for gpu in (\"H100\", \"H200\"):\n",
    "        for model, price in _nearest_price_to_token(body, gpu, window=220):\n",
    "            # sanity clamp to avoid accidental captures (tune if needed)\n",
    "            if not (0.25 <= price <= 8.0):\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"provider\": \"Shadeform\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": None,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# Example:\n",
    "df_shade = arun(scrape_shadeform_rich())\n",
    "display(df_shade)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[coreweave] snapshot -> docs/data/snapshots/20250904_125500_coreweave.csv\n",
      "[coreweave] history  -> docs/data/history/coreweave_history.csv\n",
      "[coreweave] latest   -> docs/data/latest/coreweave_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>20.0</td>\n",
       "      <td>https://www.coreweave.com/pricing</td>\n",
       "      <td>2025-09-04 12:55:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    provider region gpu_model       type duration gpu_count  price_hourly_usd  \\\n",
       "0  CoreWeave     US      H100  On-Demand       1h      None              20.0   \n",
       "\n",
       "                          source_url      fetched_at_utc  \n",
       "0  https://www.coreweave.com/pricing 2025-09-04 12:55:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= CoreWeave (async) — slim schema + per-provider history =================\n",
    "# Slim schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#              price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fallback handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns:\n",
    "            out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # snapshot\n",
    "    snap_path = SNAP_DIR / f\"{ts}_{provider_slug}.csv\"\n",
    "    snap_path = _safe_to_csv(df, snap_path)\n",
    "\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = LATEST_DIR / f\"{provider_slug}_latest.csv\"\n",
    "    latest_path = _safe_to_csv(latest, latest_path)\n",
    "\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- robust token/price matching --------\n",
    "PRICE_HOURLY_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:GPU\\s*/\\s*)?(?:h|hr|hour)\\b\",\n",
    "    re.I\n",
    ")\n",
    "# Fallback if the site omits 'hr' text; avoid monthly and memory suffixes\n",
    "PRICE_DOLLAR_RE = re.compile(\n",
    "    r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\b(?!\\s*(?:k|m|b|/mo|per\\s*month|/month|,?\\s*GB))\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "GPU_PATS = {\n",
    "    \"H100\": re.compile(r\"\\bH100\\b\", re.I),\n",
    "    \"H200\": re.compile(r\"\\bH200\\b\", re.I),\n",
    "    \"_OTHER\": re.compile(r\"\\b(?:B200|A100|A800|H800)\\b\", re.I),\n",
    "}\n",
    "\n",
    "def _find_positions(text: str, pat: re.Pattern):\n",
    "    return [m.start() for m in pat.finditer(text)]\n",
    "\n",
    "def _find_prices(text: str, prefer_hourly: bool = True):\n",
    "    pats = [PRICE_HOURLY_RE] + ([] if not prefer_hourly else [])  # first pass\n",
    "    prices = [(float(m.group(1)), m.start()) for m in PRICE_HOURLY_RE.finditer(text)]\n",
    "    if not prices:\n",
    "        prices = [(float(m.group(1)), m.start()) for m in PRICE_DOLLAR_RE.finditer(text)]\n",
    "    return prices\n",
    "\n",
    "def _nearest_prices(text: str, token: str, window: int = 240):\n",
    "    # Positions of our token vs. competitors\n",
    "    tok_pos = _find_positions(text, GPU_PATS[token])\n",
    "    if not tok_pos:\n",
    "        return []\n",
    "    comp_pos = []\n",
    "    for k, pat in GPU_PATS.items():\n",
    "        if k == token: continue\n",
    "        comp_pos.extend(_find_positions(text, pat))\n",
    "    prices = _find_prices(text)\n",
    "    out = []\n",
    "    for gpos in tok_pos:\n",
    "        cands = [(price, ppos, abs(ppos - gpos)) for (price, ppos) in prices if abs(ppos - gpos) <= window]\n",
    "        if not cands: \n",
    "            continue\n",
    "        price, ppos, dist = min(cands, key=lambda t: t[2])\n",
    "        if comp_pos:\n",
    "            nearest_other = min(abs(ppos - op) for op in comp_pos)\n",
    "            if nearest_other < dist:\n",
    "                continue\n",
    "        out.append((token, price))\n",
    "    return out\n",
    "\n",
    "# -------- CoreWeave scraper --------\n",
    "async def scrape_coreweave_async() -> pd.DataFrame:\n",
    "    url = \"https://www.coreweave.com/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help lazy content load\n",
    "        for _ in range(3):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "        # Wait for any pricing text to appear (best-effort)\n",
    "        try:\n",
    "            await page.wait_for_selector(\"text=/H100|H200/\", timeout=5000)\n",
    "        except Exception:\n",
    "            pass\n",
    "        body = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    body = re.sub(r\"\\s+\", \" \", body)\n",
    "\n",
    "    rows = []\n",
    "    for gpu in (\"H100\", \"H200\"):\n",
    "        for model, price in _nearest_prices(body, gpu, window=240):\n",
    "            # reasonable hourly range; widen if CoreWeave posts higher tiers\n",
    "            if not (0.25 <= price <= 25.0):\n",
    "                continue\n",
    "            rows.append({\n",
    "                \"provider\": \"CoreWeave\",\n",
    "                \"region\": \"US\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": None,\n",
    "                \"price_hourly_usd\": price,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # Deduplicate (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# -------- runner (works in scripts & notebooks) --------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_coreweave = arun(scrape_coreweave_async())\n",
    "latest_coreweave = _save_provider(df_coreweave, \"coreweave\")\n",
    "display(df_coreweave.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[paperspace] snapshot -> docs/data/snapshots/20250904_130549_paperspace.csv\n",
      "[paperspace] history  -> docs/data/history/paperspace_history.csv\n",
      "[paperspace] latest   -> docs/data/latest/paperspace_latest.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>None</td>\n",
       "      <td>2.24</td>\n",
       "      <td>https://www.paperspace.com/pricing</td>\n",
       "      <td>2025-09-04 13:05:49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     provider  region gpu_model       type duration gpu_count  \\\n",
       "0  Paperspace  Global      H100  On-Demand       1h      None   \n",
       "\n",
       "   price_hourly_usd                          source_url      fetched_at_utc  \n",
       "0              2.24  https://www.paperspace.com/pricing 2025-09-04 13:05:49  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================= Paperspace (async) — use your approach, keep correct rows =================\n",
    "# Output schema (slim): provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                       price_hourly_usd, source_url, fetched_at_utc\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- storage helpers (same as other providers) ----\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\"); HIST_DIR = BASE/\"history\"; SNAP_DIR = BASE/\"snapshots\"; LATEST_DIR = BASE/\"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso(): return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df); ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR/f\"{ts}_{slug}.csv\")\n",
    "    # history\n",
    "    hist = HIST_DIR/f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "                    .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                             \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "                    .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR/f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---- strict extractors (but still tolerant to site markup) ----\n",
    "GPU_PAT = re.compile(r\"(H(?:100|200)(?:\\s*(?:SXM|PCIE|NVL))?(?:\\s*\\d{2,3}\\s*GB)?)\", re.I)\n",
    "# require an hourly hint somewhere in the same block to avoid platform prices, etc.\n",
    "PRICE_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:GPU\\s*/\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "async def scrape_paperspace() -> pd.DataFrame:\n",
    "    url = \"https://www.paperspace.com/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000)               # your simple navigation\n",
    "        await page.wait_for_timeout(8000)                  # your “just wait a few seconds”\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    rows = []\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    # scan reasonable blocks; stick to your block-scan approach\n",
    "    for blk in soup.find_all([\"tr\",\"div\",\"section\",\"article\",\"li\"], recursive=True):\n",
    "        txt = blk.get_text(\" \", strip=True)\n",
    "        if not txt: \n",
    "            continue\n",
    "        # must mention H100/H200 AND 'hour' to qualify\n",
    "        if (\"H100\" not in txt and \"H200\" not in txt) or (\"hour\" not in txt.lower()):\n",
    "            continue\n",
    "\n",
    "        # model: first explicit H100/H200-ish token found\n",
    "        mm = GPU_PAT.search(txt)\n",
    "        if not mm:\n",
    "            continue\n",
    "        model = mm.group(1).upper()\n",
    "\n",
    "        # price: $… with an hourly hint in the same block\n",
    "        pm = PRICE_HOURLY.search(txt)\n",
    "        if not pm:\n",
    "            continue\n",
    "        price = float(pm.group(1))\n",
    "        # sanity band to drop weird captures\n",
    "        if not (0.2 <= price <= 50.0):\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"provider\": \"Paperspace\",\n",
    "            \"region\": \"Global\",\n",
    "            \"gpu_model\": model,          # \"H100\", \"H100 PCIE 80GB\", etc.\n",
    "            \"type\": \"On-Demand\",\n",
    "            \"duration\": \"1h\",\n",
    "            \"gpu_count\": None,\n",
    "            \"price_hourly_usd\": price,\n",
    "            \"source_url\": url,\n",
    "            \"fetched_at_utc\": _now_iso(),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---- runner that works in both scripts & notebooks ----\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_paperspace = arun(scrape_paperspace())\n",
    "latest_paperspace = _save_provider(df_paperspace, \"paperspace\")\n",
    "display(df_paperspace.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensordock] snapshot -> docs/data/snapshots/20250904_131631_tensordock.csv\n",
      "[tensordock] history  -> docs/data/history/tensordock_history.csv\n",
      "[tensordock] latest   -> docs/data/latest/tensordock_latest.csv\n",
      "     provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  TensorDock  Global      H100  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                       source_url      fetched_at_utc  \n",
      "0              2.25  https://tensordock.com/gpu-h100 2025-09-04 13:16:31  \n"
     ]
    }
   ],
   "source": [
    "# ================= TensorDock H100 (static) — slim schema + per-provider history =================\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, requests, pandas as pd, tempfile\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "# -------- pages & patterns (from your code) --------\n",
    "PAGES = [\n",
    "    \"https://tensordock.com/gpu-h100\",\n",
    "    \"https://tensordock.com/cloud-gpus\",\n",
    "    \"https://tensordock.com/comparison-gcp\",\n",
    "]\n",
    "PATTERNS = [\n",
    "    re.compile(r\"H100.*?\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr\", re.I|re.S),\n",
    "    re.compile(r\"from\\s*\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hr.*?H100\", re.I|re.S),\n",
    "    re.compile(r\"\\$([0-9]+(?:\\.[0-9]+)?)\\s*/?\\s*hour.*?H100\", re.I|re.S),\n",
    "]\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# -------- storage + schema helpers (same as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception:\n",
    "        pass  # fall back handled in _safe_to_csv\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "              .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\"))\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- scraper (uses your logic, mapped to slim schema) --------\n",
    "def scrape_tensordock_public_h100() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for url in PAGES:\n",
    "        try:\n",
    "            r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "            text = soup.get_text(\" \", strip=True)\n",
    "            price = None\n",
    "            for pat in PATTERNS:\n",
    "                m = pat.search(text)\n",
    "                if m:\n",
    "                    price = float(m.group(1))\n",
    "                    break\n",
    "            if price and (0.2 <= price <= 50.0):  # sanity band for $/GPU/hr\n",
    "                rows.append({\n",
    "                    \"provider\": \"TensorDock\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": \"H100\",\n",
    "                    \"type\": \"On-Demand\",\n",
    "                    \"duration\": \"1h\",\n",
    "                    \"gpu_count\": 1,\n",
    "                    \"price_hourly_usd\": price,\n",
    "                    \"source_url\": url,\n",
    "                    \"fetched_at_utc\": _now_iso(),\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"[TensorDock] {url} -> {e}\")\n",
    "\n",
    "    if not rows:\n",
    "        return _ensure_slim(pd.DataFrame(columns=SLIM_COLS))\n",
    "\n",
    "    # Deduplicate: keep the **lowest** \"from\" price across pages\n",
    "    df = pd.DataFrame(rows)\n",
    "    df = (df.sort_values(\"price_hourly_usd\")\n",
    "            .drop_duplicates(subset=[\"provider\",\"gpu_model\"], keep=\"first\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_tensordock = scrape_tensordock_public_h100()\n",
    "latest_tensordock = _save_provider(df_tensordock, \"tensordock\")\n",
    "print(df_tensordock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[brokkr] snapshot -> docs/data/snapshots/20250904_132203_brokkr.csv\n",
      "[brokkr] history  -> docs/data/history/brokkr_history.csv\n",
      "[brokkr] latest   -> docs/data/latest/brokkr_latest.csv\n",
      "              provider  region gpu_model       type duration  gpu_count  \\\n",
      "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
      "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
      "\n",
      "   price_hourly_usd                              source_url  \\\n",
      "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
      "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
      "\n",
      "       fetched_at_utc  \n",
      "0 2025-09-04 13:22:03  \n",
      "1 2025-09-04 13:22:03  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>price_hourly_usd</th>\n",
       "      <th>source_url</th>\n",
       "      <th>fetched_at_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>https://brokkr.hydrahost.com/inventory</td>\n",
       "      <td>2025-09-04 13:22:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              provider  region gpu_model       type duration  gpu_count  \\\n",
       "0  Hydra Host (Brokkr)  Global      H100  On-Demand       1h          1   \n",
       "1  Hydra Host (Brokkr)  Global      H200  On-Demand       1h          1   \n",
       "\n",
       "   price_hourly_usd                              source_url  \\\n",
       "0               2.3  https://brokkr.hydrahost.com/inventory   \n",
       "1               2.5  https://brokkr.hydrahost.com/inventory   \n",
       "\n",
       "       fetched_at_utc  \n",
       "0 2025-09-04 13:22:03  \n",
       "1 2025-09-04 13:22:03  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============== Hydra Host (Brokkr) — slim schema + per-provider history ==============\n",
    "# Output schema: provider, region, gpu_model, type, duration, gpu_count,\n",
    "#                price_hourly_usd, source_url, fetched_at_utc\n",
    "# Py 3.8 compatible\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------- storage + schema helpers (same pattern as other providers) --------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=False)\n",
    "        return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name\n",
    "        df.to_csv(tmp, index=False)\n",
    "        return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, provider_slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # snapshot\n",
    "    snap_path = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{provider_slug}.csv\")\n",
    "    # history (append + dedupe)\n",
    "    hist_path = HIST_DIR / f\"{provider_slug}_history.csv\"\n",
    "    if hist_path.exists():\n",
    "        old = pd.read_csv(hist_path, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (\n",
    "        all_df.dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "              .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                       \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "              .sort_values(\"fetched_at_utc\")\n",
    "    )\n",
    "    hist_path = _safe_to_csv(all_df, hist_path)\n",
    "    # latest (newest per gpu/type/region/duration)\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{provider_slug}_latest.csv\")\n",
    "    print(f\"[{provider_slug}] snapshot -> {snap_path}\\n[{provider_slug}] history  -> {hist_path}\\n[{provider_slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# -------- your Brokkr scraper, tightened to only accept \"per card-hour\" prices --------\n",
    "GPU_RE = r\"(H100|H200)\"\n",
    "PRICE_PER_CARDHR = r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:per\\s*card[-\\s]?hour|/card[-\\s]?hour)\\b\"\n",
    "PATS = [\n",
    "    re.compile(rf\"{GPU_RE}.{{0,220}}?{PRICE_PER_CARDHR}\", re.I | re.S),\n",
    "    re.compile(rf\"{PRICE_PER_CARDHR}.{{0,220}}?{GPU_RE}\", re.I | re.S),\n",
    "]\n",
    "\n",
    "async def scrape_brokkr() -> pd.DataFrame:\n",
    "    url = \"https://brokkr.hydrahost.com/inventory\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help hydrate lazy content\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(700)\n",
    "        body_text = await page.inner_text(\"body\")\n",
    "        await browser.close()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", body_text)\n",
    "    rows = []\n",
    "\n",
    "    for pat in PATS:\n",
    "        for m in pat.finditer(text):\n",
    "            # Depending on which pattern matched, group order differs\n",
    "            groups = m.groups()\n",
    "            # Normalize extraction: model + price are always present\n",
    "            if len(groups) == 2:\n",
    "                # pattern 1: (GPU, price)\n",
    "                gpu_model, price_str = groups\n",
    "            elif len(groups) == 3:\n",
    "                # pattern 2 returns (price, GPU) because of nested groups; pick numeric+gpu\n",
    "                # groups could be ('12.34', 'H100') or ('12.34', 'card-hour', 'H100') depending on regex engine\n",
    "                nums = [g for g in groups if g and re.fullmatch(r\"[0-9]+(?:\\.[0-9]+)?\", g)]\n",
    "                gpus = [g for g in groups if g and re.fullmatch(r\"H100|H200\", g, flags=re.I)]\n",
    "                if not nums or not gpus:\n",
    "                    continue\n",
    "                price_str, gpu_model = nums[0], gpus[0]\n",
    "            else:\n",
    "                # Safe fallback: find first number and first GPU token in the match\n",
    "                seg = m.group(0)\n",
    "                pm = re.search(r\"[0-9]+(?:\\.[0-9]+)?\", seg)\n",
    "                gm = re.search(r\"H100|H200\", seg, flags=re.I)\n",
    "                if not (pm and gm):\n",
    "                    continue\n",
    "                price_str, gpu_model = pm.group(0), gm.group(0)\n",
    "\n",
    "            try:\n",
    "                price = float(price_str)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # sanity band for per-card hour pricing\n",
    "            if not (0.2 <= price <= 50.0):\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"provider\": \"Hydra Host (Brokkr)\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": gpu_model.upper(),\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": price,   # per card-hour = per-GPU hourly\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe (gpu_model, price)\n",
    "    df = (df.sort_values([\"gpu_model\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# -------- runner that works in both notebooks & scripts --------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_brokkr = arun(scrape_brokkr())\n",
    "latest_brokkr = _save_provider(df_brokkr, \"brokkr\")\n",
    "print(df_brokkr)\n",
    "display(df_brokkr.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[crusoecloud] snapshot -> docs/data/snapshots/20250904_132724_crusoecloud.csv\n",
      "[crusoecloud] history  -> docs/data/history/crusoecloud_history.csv\n",
      "[crusoecloud] latest   -> docs/data/latest/crusoecloud_latest.csv\n",
      "      provider  region gpu_model         type duration  gpu_count  \\\n",
      "0  CrusoeCloud  Global      H100    On-Demand       1h          1   \n",
      "1  CrusoeCloud  Global      H100  Reserved-1y       1h          1   \n",
      "2  CrusoeCloud  Global      H100  Reserved-3y       1h          1   \n",
      "3  CrusoeCloud  Global      H100  Reserved-6m       1h          1   \n",
      "4  CrusoeCloud  Global      H200    On-Demand       1h          1   \n",
      "5  CrusoeCloud  Global      H200  Reserved-1y       1h          1   \n",
      "6  CrusoeCloud  Global      H200  Reserved-3y       1h          1   \n",
      "7  CrusoeCloud  Global      H200  Reserved-6m       1h          1   \n",
      "\n",
      "   price_hourly_usd                           source_url      fetched_at_utc  \n",
      "0              3.90  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "1              2.93  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "2              2.54  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "3              3.12  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "4              4.29  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "5              3.22  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "6              2.79  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n",
      "7              3.43  https://www.crusoe.ai/cloud/pricing 2025-09-04 13:27:24  \n"
     ]
    }
   ],
   "source": [
    "# ============ Crusoe Cloud (async) — table scrape → slim schema + history ============\n",
    "\n",
    "import re, asyncio, pandas as pd, tempfile\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ---------- slim schema + storage helpers (same as other providers) ----------\n",
    "SLIM_COLS = [\n",
    "    \"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "    \"price_hourly_usd\",\"source_url\",\"fetched_at_utc\"\n",
    "]\n",
    "BASE = Path(\"docs/data\")\n",
    "HIST_DIR = BASE / \"history\"\n",
    "SNAP_DIR = BASE / \"snapshots\"\n",
    "LATEST_DIR = BASE / \"latest\"\n",
    "for d in (HIST_DIR, SNAP_DIR, LATEST_DIR):\n",
    "    try: d.mkdir(parents=True, exist_ok=True)\n",
    "    except Exception: pass\n",
    "\n",
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "def _ensure_slim(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for c in SLIM_COLS:\n",
    "        if c not in out.columns: out[c] = None\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"], errors=\"coerce\")\n",
    "    out[\"fetched_at_utc\"] = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "    return out[SLIM_COLS]\n",
    "\n",
    "def _safe_to_csv(df: pd.DataFrame, path: Path):\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True); df.to_csv(path, index=False); return path\n",
    "    except Exception:\n",
    "        tmp = Path(tempfile.gettempdir()) / path.name; df.to_csv(tmp, index=False); return tmp\n",
    "\n",
    "def _save_provider(df: pd.DataFrame, slug: str):\n",
    "    df = _ensure_slim(df)\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "    snap = _safe_to_csv(df, SNAP_DIR / f\"{ts}_{slug}.csv\")\n",
    "    # history append+dedupe\n",
    "    hist = HIST_DIR / f\"{slug}_history.csv\"\n",
    "    if hist.exists():\n",
    "        old = pd.read_csv(hist, low_memory=False)\n",
    "        old[\"fetched_at_utc\"] = pd.to_datetime(old[\"fetched_at_utc\"], errors=\"coerce\", utc=True).dt.tz_convert(None)\n",
    "        all_df = pd.concat([old, df], ignore_index=True)\n",
    "    else:\n",
    "        all_df = df.copy()\n",
    "    all_df = (all_df\n",
    "        .dropna(subset=[\"gpu_model\",\"price_hourly_usd\"])\n",
    "        .drop_duplicates(subset=[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\n",
    "                                 \"fetched_at_utc\",\"price_hourly_usd\"], keep=\"last\")\n",
    "        .sort_values(\"fetched_at_utc\"))\n",
    "    hist = _safe_to_csv(all_df, hist)\n",
    "    # latest per gpu/type/region/duration\n",
    "    key = [\"gpu_model\",\"type\",\"region\",\"duration\"]\n",
    "    latest = all_df.sort_values(\"fetched_at_utc\").drop_duplicates(subset=key, keep=\"last\")\n",
    "    latest_path = _safe_to_csv(latest, LATEST_DIR / f\"{slug}_latest.csv\")\n",
    "    print(f\"[{slug}] snapshot -> {snap}\\n[{slug}] history  -> {hist}\\n[{slug}] latest   -> {latest_path}\")\n",
    "    return latest\n",
    "\n",
    "# ---------- parsing helpers ----------\n",
    "PRICE_RE = re.compile(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*per\\s*)?\\s*(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "def _parse_price(cell_text: str):\n",
    "    if not cell_text: return None\n",
    "    m = PRICE_RE.search(cell_text.replace(\",\", \"\"))\n",
    "    if not m:\n",
    "        # fallback: plain $N.NN without explicit /hr\n",
    "        m2 = re.search(r\"\\$?\\s*([0-9]+(?:\\.[0-9]+)?)\\b\", cell_text.replace(\",\", \"\"))\n",
    "        return float(m2.group(1)) if m2 else None\n",
    "    return float(m.group(1))\n",
    "\n",
    "def _is_h_model(text: str) -> bool:\n",
    "    t = text.upper()\n",
    "    return (\"H100\" in t) or (\"H200\" in t)\n",
    "\n",
    "def _model_from(text: str) -> str:\n",
    "    return \"H100\" if \"H100\" in text.upper() else \"H200\"\n",
    "\n",
    "# ---------- scraper ----------\n",
    "async def scrape_crusoe_table() -> pd.DataFrame:\n",
    "    url = \"https://www.crusoe.ai/cloud/pricing\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url, timeout=90000, wait_until=\"domcontentloaded\")\n",
    "        # help render\n",
    "        for _ in range(2):\n",
    "            await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "            await page.wait_for_timeout(800)\n",
    "        html = await page.content()\n",
    "        await browser.close()\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rows_out = []\n",
    "\n",
    "    # Find all table rows; filter to those that mention H100/H200\n",
    "    for tr in soup.find_all(\"tr\"):\n",
    "        tds = [c.get_text(\" \", strip=True) for c in tr.find_all(\"td\")]\n",
    "        if not tds or not any(_is_h_model(c) for c in tds):\n",
    "            continue\n",
    "\n",
    "        model = _model_from(\" \".join(tds))\n",
    "\n",
    "        # Try to map columns conservatively:\n",
    "        # Common layout: [Model, On-Demand, Spot?, Reserved 6m, Reserved 1y, Reserved 3y, ...]\n",
    "        # We’ll grab by position if present, else try to read by header alignment.\n",
    "        on_demand = _parse_price(tds[1]) if len(tds) > 1 else None\n",
    "        res_6m    = _parse_price(tds[3]) if len(tds) > 3 else None\n",
    "        res_1y    = _parse_price(tds[4]) if len(tds) > 4 else None\n",
    "        res_3y    = _parse_price(tds[5]) if len(tds) > 5 else None\n",
    "\n",
    "        # Build normalised slim rows\n",
    "        if on_demand is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"On-Demand\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": on_demand,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_6m is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-6m\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_6m,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_1y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-1y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_1y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "        if res_3y is not None:\n",
    "            rows_out.append({\n",
    "                \"provider\": \"CrusoeCloud\",\n",
    "                \"region\": \"Global\",\n",
    "                \"gpu_model\": model,\n",
    "                \"type\": \"Reserved-3y\",\n",
    "                \"duration\": \"1h\",\n",
    "                \"gpu_count\": 1,\n",
    "                \"price_hourly_usd\": res_3y,\n",
    "                \"source_url\": url,\n",
    "                \"fetched_at_utc\": _now_iso(),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows_out, columns=SLIM_COLS)\n",
    "    if df.empty:\n",
    "        return _ensure_slim(df)\n",
    "\n",
    "    # de-dupe & sanity\n",
    "    df = (df.sort_values([\"gpu_model\",\"type\",\"price_hourly_usd\",\"fetched_at_utc\"])\n",
    "            .drop_duplicates(subset=[\"gpu_model\",\"type\",\"price_hourly_usd\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "    # plausible hourly band\n",
    "    df = df[(df[\"price_hourly_usd\"] > 0) & (df[\"price_hourly_usd\"] < 200)]\n",
    "    return _ensure_slim(df)\n",
    "\n",
    "# ---------- runner that works in scripts & notebooks ----------\n",
    "def arun(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        import nest_asyncio; nest_asyncio.apply()\n",
    "        return loop.run_until_complete(coro)\n",
    "\n",
    "# ------------------------------ RUN --------------------------------\n",
    "df_crusoe = arun(scrape_crusoe_table())\n",
    "latest_crusoe = _save_provider(df_crusoe, \"crusoecloud\")\n",
    "print(df_crusoe.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   provider  region gpu_model       instance_type  gpu_count  \\\n",
      "0  OVHcloud  Global      H100  public-cloud/Price          2   \n",
      "1  OVHcloud  Global      H100  public-cloud/Price          4   \n",
      "\n",
      "   price_hourly_usd_instance  price_hourly_usd_per_gpu price_reserved_usd  \\\n",
      "0                       5.98                    2.9900               None   \n",
      "1                      11.97                    2.9925               None   \n",
      "\n",
      "  reserved_duration                   timestamp  \n",
      "0              None  2025-09-04T15:18:12.093698  \n",
      "1              None  2025-09-04T15:18:12.093773  \n"
     ]
    }
   ],
   "source": [
    "# OVHcloud H100/H200 — get the *correct per-GPU hourly price* from the public prices table\n",
    "# - Ties each $…/hour to the same row as H100/H200\n",
    "# - Extracts the GPU count from the row (1×/2×/4×/8× or “… GPUs”)\n",
    "# - per_gpu = instance_price / parsed_gpu_count  (NO 8× assumption)\n",
    "# - Returns both instance price and per-GPU price\n",
    "\n",
    "import re, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "def timestamp(): return datetime.utcnow().isoformat()\n",
    "\n",
    "USD_HOURLY = re.compile(r\"\\$\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(?:/|\\s*(?:per|an)\\s*)?(?:h|hr|hour)\\b\", re.I)\n",
    "\n",
    "# GPU count detectors (row-level + cell/variant-level)\n",
    "COUNT_PATS = [\n",
    "    re.compile(r\"(\\d+)\\s*[×x]\\s*(?:NVIDIA\\s*)?(H100|H200)\\b\", re.I),  # \"8× H100\"\n",
    "    re.compile(r\"(H100|H200)\\s*[×x]\\s*(\\d+)\\b\", re.I),               # \"H100 × 8\"\n",
    "    re.compile(r\"(\\d+)\\s*(?:GPU|GPUs)\\b\", re.I),                     # \"8 GPUs\"\n",
    "    re.compile(r\"\\b(\\d+)\\s*[×x]\\b\", re.I),                           # \"4x\"\n",
    "]\n",
    "\n",
    "def _parse_count(text: str):\n",
    "    for pat in COUNT_PATS:\n",
    "        m = pat.search(text)\n",
    "        if not m: \n",
    "            continue\n",
    "        for g in m.groups():\n",
    "            if g and g.isdigit():\n",
    "                n = int(g)\n",
    "                if 1 <= n <= 16:\n",
    "                    return n\n",
    "    return None\n",
    "\n",
    "def scrape_ovhcloud_correct(url=\"https://www.ovhcloud.com/en/public-cloud/prices/\") -> pd.DataFrame:\n",
    "    html = requests.get(url, headers={\"User-Agent\":\"Mozilla/5.0\"}, timeout=60).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "    for table in soup.select(\"table\"):\n",
    "        # headers so we can label which column the number came from\n",
    "        headers = [th.get_text(\" \", strip=True) for th in table.select(\"thead th\")]\n",
    "        if not headers:\n",
    "            first = table.find(\"tr\")\n",
    "            if first:\n",
    "                headers = [td.get_text(\" \", strip=True) for td in first.find_all([\"th\",\"td\"])]\n",
    "\n",
    "        body_rows = table.select(\"tbody tr\") or table.select(\"tr\")\n",
    "        for tr in body_rows:\n",
    "            tds = tr.find_all(\"td\")\n",
    "            if not tds: \n",
    "                continue\n",
    "            cells = [td.get_text(\" \", strip=True) for td in tds]\n",
    "            row_txt = \" \".join(cells)\n",
    "            up = row_txt.upper()\n",
    "            if (\"H100\" not in up) and (\"H200\" not in up):\n",
    "                continue\n",
    "\n",
    "            model = \"H100\" if \"H100\" in up else \"H200\"\n",
    "            row_count = _parse_count(row_txt)\n",
    "\n",
    "            for idx, (td, cell) in enumerate(zip(tds, cells)):\n",
    "                m = USD_HOURLY.search(cell)\n",
    "                if not m:\n",
    "                    continue\n",
    "                instance_price = float(m.group(1))\n",
    "\n",
    "                # try counts in cell and header/variant too\n",
    "                var = headers[idx] if idx < len(headers) and headers else f\"col_{idx+1}\"\n",
    "                count = (\n",
    "                    _parse_count(cell) or\n",
    "                    _parse_count(var)  or\n",
    "                    row_count\n",
    "                )\n",
    "                if count is None:\n",
    "                    # if we can't prove node size, skip (prevents wrong divide)\n",
    "                    continue\n",
    "\n",
    "                per_gpu = instance_price / count\n",
    "                if not (0.25 <= per_gpu <= 20.0):\n",
    "                    continue\n",
    "\n",
    "                rows.append({\n",
    "                    \"provider\": \"OVHcloud\",\n",
    "                    \"region\": \"Global\",\n",
    "                    \"gpu_model\": model,\n",
    "                    \"instance_type\": f\"public-cloud/{var}\",\n",
    "                    \"gpu_count\": int(count),\n",
    "                    \"price_hourly_usd_instance\": round(instance_price, 4),\n",
    "                    \"price_hourly_usd_per_gpu\": round(per_gpu, 4),\n",
    "                    \"price_reserved_usd\": None,\n",
    "                    \"reserved_duration\": None,\n",
    "                    \"timestamp\": timestamp(),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if not df.empty:\n",
    "        df = (df.sort_values([\"gpu_model\",\"price_hourly_usd_instance\"])\n",
    "                .drop_duplicates(subset=[\"gpu_model\",\"instance_type\",\"price_hourly_usd_instance\"], keep=\"last\")\n",
    "                .reset_index(drop=True))\n",
    "    return df\n",
    "\n",
    "# Example\n",
    "df_ovh = scrape_ovhcloud_correct()\n",
    "print(df_ovh.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAFETY HARDENER (drop-in) ===============================================\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "NUM_COLS = [\n",
    "    \"price_hourly_usd\",\"price_hourly_usd_instance\",\"price_reserved_usd\",\n",
    "    \"effective_price_usd_per_gpu_hr\",\"gpu_count\",\n",
    "    \"market_med\",\"market_mean\",\"market_median\",\"market_p25\",\"market_p75\",\"market_cnt\",\"market_iqr\",\n",
    "    \"price_score\",\"premium_vs_median\",\"premium_vs_mkt\"\n",
    "]\n",
    "DATE_COLS = [\"fetched_at_utc\",\"timestamp\",\"asof_utc\",\"ts_utc\",\"ts_iso\",\"last_obs_date\",\"date_target\"]\n",
    "\n",
    "def _sanitize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not isinstance(df, pd.DataFrame): return df\n",
    "\n",
    "    # 1) datetimes\n",
    "    for c in DATE_COLS:\n",
    "        if c in df.columns:\n",
    "            s = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "            df[c] = s.dt.tz_convert(None)\n",
    "\n",
    "    # keep a pure pandas datetime64 day column (not python date objects)\n",
    "    if \"date\" in df.columns:\n",
    "        d = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        # if it was tz-aware, drop tz; always floor to day\n",
    "        if getattr(d.dtype, \"tz\", None) is not None:\n",
    "            d = d.dt.tz_convert(None)\n",
    "        df[\"date\"] = d.dt.floor(\"D\")\n",
    "\n",
    "    # 2) numerics (coerce before any clip/ratios)\n",
    "    for c in NUM_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run on any frames that exist in this notebook cell\n",
    "for _name in (\"raw\",\"df\",\"qa\",\"market\",\"scored\",\"od\",\"norm\",\"hist\",\"model_df\",\"last_rows\"):\n",
    "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
    "        globals()[_name] = _sanitize(globals()[_name])\n",
    "\n",
    "# If you're about to use last_rows ratios (ML baseline), ensure these are numeric:\n",
    "if \"last_rows\" in globals() and isinstance(last_rows, pd.DataFrame):\n",
    "    for c in [\"market_p25\",\"market_p75\",\"market_med\",\"price_hourly_usd\"]:\n",
    "        if c in last_rows.columns:\n",
    "            last_rows[c] = pd.to_numeric(last_rows[c], errors=\"coerce\")\n",
    "# ============================================================================ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ wrote docs/data/derived/provider_scores_latest.csv (19 rows)\n",
      "✔ wrote docs/data/derived/roi_comparison.csv (32 rows)\n",
      "✔ wrote docs/data/derived/market_index.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>effective_price_usd_per_gpu_hr</th>\n",
       "      <th>priceiq_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>US</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>79.238604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.00</td>\n",
       "      <td>69.864799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.30</td>\n",
       "      <td>79.239799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VoltagePark</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.99</td>\n",
       "      <td>79.269266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.25</td>\n",
       "      <td>79.272484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.45</td>\n",
       "      <td>66.774049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.24</td>\n",
       "      <td>60.535615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>4.290226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.25</td>\n",
       "      <td>51.166597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H200</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.50</td>\n",
       "      <td>54.294696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              provider  region gpu_model       type duration  gpu_count  \\\n",
       "0               Nebius      US      H200  On-Demand       1h        NaN   \n",
       "1               Nebius  Global      H100  On-Demand       1h        NaN   \n",
       "2               Nebius  Global      H200  On-Demand       1h        NaN   \n",
       "3          VoltagePark      US      H100  On-Demand       1h        NaN   \n",
       "4              Vast.ai  Global      H100  On-Demand       1h        NaN   \n",
       "5            Shadeform  Global      H200  On-Demand       1h        NaN   \n",
       "6           Paperspace  Global      H100  On-Demand       1h        NaN   \n",
       "7            CoreWeave      US      H100  On-Demand       1h        1.0   \n",
       "8           TensorDock  Global      H100  On-Demand       1h        1.0   \n",
       "9  Hydra Host (Brokkr)  Global      H200  On-Demand       1h        1.0   \n",
       "\n",
       "   effective_price_usd_per_gpu_hr  priceiq_score  \n",
       "0                            2.30      79.238604  \n",
       "1                            2.00      69.864799  \n",
       "2                            2.30      79.239799  \n",
       "3                            1.99      79.269266  \n",
       "4                            1.25      79.272484  \n",
       "5                            2.45      66.774049  \n",
       "6                            2.24      60.535615  \n",
       "7                           20.00       4.290226  \n",
       "8                            2.25      51.166597  \n",
       "9                            2.50      54.294696  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>best_provider</th>\n",
       "      <th>best_region</th>\n",
       "      <th>price_per_gpu_hr</th>\n",
       "      <th>total_cost_usd</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H100</td>\n",
       "      <td>8</td>\n",
       "      <td>1 hour</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H100</td>\n",
       "      <td>8</td>\n",
       "      <td>1 day</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>240.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H100</td>\n",
       "      <td>8</td>\n",
       "      <td>1 week</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1680.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H100</td>\n",
       "      <td>8</td>\n",
       "      <td>1 month</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>7200.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>H100</td>\n",
       "      <td>16</td>\n",
       "      <td>1 hour</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>H100</td>\n",
       "      <td>16</td>\n",
       "      <td>1 day</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>480.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>H100</td>\n",
       "      <td>16</td>\n",
       "      <td>1 week</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>3360.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>H100</td>\n",
       "      <td>16</td>\n",
       "      <td>1 month</td>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>1.25</td>\n",
       "      <td>14400.0</td>\n",
       "      <td>2025-09-04T12:42:14Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  gpu_model  gpu_count duration best_provider best_region  price_per_gpu_hr  \\\n",
       "0      H100          8   1 hour       Vast.ai      Global              1.25   \n",
       "1      H100          8    1 day       Vast.ai      Global              1.25   \n",
       "2      H100          8   1 week       Vast.ai      Global              1.25   \n",
       "3      H100          8  1 month       Vast.ai      Global              1.25   \n",
       "4      H100         16   1 hour       Vast.ai      Global              1.25   \n",
       "5      H100         16    1 day       Vast.ai      Global              1.25   \n",
       "6      H100         16   1 week       Vast.ai      Global              1.25   \n",
       "7      H100         16  1 month       Vast.ai      Global              1.25   \n",
       "\n",
       "   total_cost_usd             timestamp  \n",
       "0            10.0  2025-09-04T12:42:14Z  \n",
       "1           240.0  2025-09-04T12:42:14Z  \n",
       "2          1680.0  2025-09-04T12:42:14Z  \n",
       "3          7200.0  2025-09-04T12:42:14Z  \n",
       "4            20.0  2025-09-04T12:42:14Z  \n",
       "5           480.0  2025-09-04T12:42:14Z  \n",
       "6          3360.0  2025-09-04T12:42:14Z  \n",
       "7         14400.0  2025-09-04T12:42:14Z  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === ONE-CELL: unify → score (\"Silicon-style\") → ROI → write CSVs =============\n",
    "import re, math, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# minimal hardening (drop-in before scoring/ROI)\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "df[\"fetched_at_utc\"] = pd.to_datetime(df[\"fetched_at_utc\"], errors=\"coerce\", utc=True)\n",
    "df[\"date\"] = df[\"fetched_at_utc\"].dt.tz_convert(None).dt.floor(\"D\")\n",
    "\n",
    "for c in [\"price_hourly_usd\",\"effective_price_usd_per_gpu_hr\",\n",
    "          \"market_median\",\"market_mean\",\"market_p25\",\"market_p75\",\"market_cnt\"]:\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# ---------- config ----------\n",
    "BASE = Path(\"docs/data\")\n",
    "LATEST_DIR  = BASE / \"latest\"\n",
    "DERIVED_DIR = BASE / \"derived\"\n",
    "for d in (LATEST_DIR, DERIVED_DIR): d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _now_iso(): return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def to_hours(s):\n",
    "    if not isinstance(s,str) or not s.strip(): return np.nan\n",
    "    s = s.strip().lower().rstrip(\"s\")\n",
    "    m = re.match(r\"^(\\d+(?:\\.\\d+)?)\\s*([a-z]+)$\", s)\n",
    "    if not m: return np.nan\n",
    "    q, u = float(m.group(1)), m.group(2)\n",
    "    mult = {\"h\":1,\"hr\":1,\"hour\":1,\"d\":24,\"day\":24,\"w\":168,\"wk\":168,\"week\":168,\"mo\":720,\"month\":720}.get(u, np.nan)\n",
    "    return q*mult\n",
    "\n",
    "def _to_num(x):\n",
    "    try:\n",
    "        if x is None or (isinstance(x,str) and not x.strip()): return np.nan\n",
    "        return float(re.sub(r\"[^\\d.\\-]\", \"\", str(x)))\n",
    "    except: return np.nan\n",
    "\n",
    "EXPECTED = {\n",
    "    \"provider\":            [\"provider\",\"Provider\",\"name\"],\n",
    "    \"region\":              [\"region\",\"Region\",\"location\",\"geo\",\"cloud_region\",\"area\"],\n",
    "    \"gpu_model\":           [\"gpu_model\",\"gpu\",\"model\",\"GPU\"],\n",
    "    \"gpu_count\":           [\"gpu_count\",\"count\",\"gpus\",\"GPU_count\"],\n",
    "    \"type\":                [\"type\",\"price_type\"],\n",
    "    \"duration\":            [\"duration\",\"reserved_duration\",\"term\"],\n",
    "    \"price_hourly_usd\":    [\"price_hourly_usd\",\"price\",\"usd_per_gpu_hr\",\"price_hourly_usd_per_gpu\"],\n",
    "    \"price_reserved_usd\":  [\"price_reserved_usd\",\"reserved_usd_per_gpu_hr\"],\n",
    "    \"price_hourly_usd_instance\": [\"price_hourly_usd_instance\",\"instance_price\"],\n",
    "    \"timestamp\":           [\"timestamp\",\"fetched_at_utc\",\"ts_utc\",\"ts_iso\",\"time\",\"start_time_iso\"],\n",
    "    \"source_url\":          [\"source_url\",\"url\",\"link\"]\n",
    "}\n",
    "\n",
    "def normalise_any(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Map any provider frame into a canonical schema and compute effective per-GPU price.\"\"\"\n",
    "    df = df.copy()\n",
    "    # map synonyms → columns\n",
    "    for tgt, cands in EXPECTED.items():\n",
    "        for c in cands:\n",
    "            if c in df.columns:\n",
    "                if c != tgt: df[tgt] = df[c]\n",
    "                break\n",
    "        if tgt not in df.columns: df[tgt] = pd.NA\n",
    "\n",
    "    # types & defaults\n",
    "    df[\"gpu_model\"] = df[\"gpu_model\"].astype(str).str.upper().str.strip()\n",
    "    df[\"region\"]    = df[\"region\"].astype(str).replace({\"nan\":None}).fillna(\"Global\").str.strip()\n",
    "    df[\"type\"]      = df[\"type\"].astype(str).replace({\"nan\":None}).fillna(\"On-Demand\").str.strip()\n",
    "    df[\"duration\"]  = df[\"duration\"].astype(str).replace({\"nan\":None,\"\":None}).fillna(\"1h\").str.strip()\n",
    "    df[\"gpu_count\"] = pd.to_numeric(df[\"gpu_count\"], errors=\"coerce\")\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "    df[\"price_hourly_usd\"]    = pd.to_numeric(df[\"price_hourly_usd\"].map(_to_num))\n",
    "    df[\"price_reserved_usd\"]  = pd.to_numeric(df[\"price_reserved_usd\"].map(_to_num))\n",
    "    df[\"price_hourly_usd_instance\"] = pd.to_numeric(df[\"price_hourly_usd_instance\"].map(_to_num))\n",
    "\n",
    "    # if an instance price exists but per-GPU doesn't, divide by proven gpu_count\n",
    "    mask_need_split = df[\"price_hourly_usd\"].isna() & df[\"price_hourly_usd_instance\"].notna() & df[\"gpu_count\"].notna()\n",
    "    df.loc[mask_need_split, \"price_hourly_usd\"] = df.loc[mask_need_split, \"price_hourly_usd_instance\"] / df.loc[mask_need_split, \"gpu_count\"]\n",
    "\n",
    "    # effective per-GPU hourly (prefer on-demand; else reserved; if reserved looks \"total for term\", divide by hours)\n",
    "    eff = df[\"price_hourly_usd\"].copy()\n",
    "    dur_h = df[\"duration\"].map(to_hours)\n",
    "    use_reserved = eff.isna() & df[\"price_reserved_usd\"].notna()\n",
    "    total_like   = (df[\"price_reserved_usd\"] > 10) & dur_h.notna()\n",
    "    eff[use_reserved & total_like] = df[\"price_reserved_usd\"] / dur_h\n",
    "    eff[use_reserved & ~total_like] = df[\"price_reserved_usd\"]\n",
    "    df[\"effective_price_usd_per_gpu_hr\"] = eff\n",
    "\n",
    "    # keep only sane rows\n",
    "    df = df[(df[\"gpu_model\"].isin([\"H100\",\"H200\"])) & (df[\"effective_price_usd_per_gpu_hr\"].between(0.05, 200))]\n",
    "    # final timestamp ISO\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    cols = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\n",
    "            \"price_hourly_usd\",\"price_reserved_usd\",\"effective_price_usd_per_gpu_hr\",\n",
    "            \"timestamp\",\"source_url\"]\n",
    "    extra = [c for c in df.columns if c not in cols]\n",
    "    return df[cols+extra]\n",
    "\n",
    "def percentile_rev(s: pd.Series) -> pd.Series:\n",
    "    # lower price → higher score percentile\n",
    "    r = s.rank(pct=True, method=\"average\")\n",
    "    return 1.0 - (r - r.min())/(r.max()-r.min()+1e-12)\n",
    "\n",
    "def add_priceiq_score(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    grp = [\"gpu_model\",\"region\"] if df[\"region\"].nunique() > 1 else [\"gpu_model\"]\n",
    "    df[\"_pct\"] = df.groupby(grp)[\"effective_price_usd_per_gpu_hr\"].transform(percentile_rev)\n",
    "    # capacity bonus (0..15)\n",
    "    cap = np.log1p(df[\"gpu_count\"].fillna(1))/np.log1p(256)\n",
    "    df[\"_cap\"] = 15.0*cap.clip(0,1)\n",
    "    # recency bonus (0..10)\n",
    "    def rec(ts):\n",
    "        try:\n",
    "            t = pd.to_datetime(ts, utc=True)\n",
    "            age_h = max(0.0, (pd.Timestamp.utcnow()-t).total_seconds()/3600)\n",
    "            return 10.0*math.exp(-age_h/72.0)\n",
    "        except: return 0.0\n",
    "    df[\"_rec\"] = df[\"timestamp\"].map(rec)\n",
    "    df[\"priceiq_score\"] = (df[\"_pct\"]*75.0 + df[\"_cap\"] + df[\"_rec\"]).clip(0,100)\n",
    "    return df.drop(columns=[\"_pct\",\"_cap\",\"_rec\"], errors=\"ignore\")\n",
    "\n",
    "# ---------- 1) gather frames: in-memory first, else docs/data/latest/*.csv ----------\n",
    "frames = []\n",
    "# any variable starting with df_ or latest_ that's a DataFrame\n",
    "for name, obj in list(globals().items()):\n",
    "    if isinstance(obj, pd.DataFrame) and (name.startswith(\"df_\") or name.startswith(\"latest_\")):\n",
    "        if not obj.empty:\n",
    "            frames.append(obj.copy())\n",
    "\n",
    "if not frames:\n",
    "    for p in sorted(LATEST_DIR.glob(\"*_latest.csv\")):\n",
    "        try:\n",
    "            frames.append(pd.read_csv(p))\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to load {p.name}: {e}\")\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No provider frames found (neither in-memory nor docs/data/latest/*.csv). Run scrapers first.\")\n",
    "\n",
    "raw = pd.concat(frames, ignore_index=True)\n",
    "norm = normalise_any(raw)\n",
    "\n",
    "# dedupe: latest by (provider, region, gpu_model, type, duration, gpu_count, effective price)\n",
    "key = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\"effective_price_usd_per_gpu_hr\"]\n",
    "norm = (norm.sort_values(\"timestamp\").drop_duplicates(subset=key, keep=\"last\").reset_index(drop=True))\n",
    "\n",
    "# ---------- 2) build market baselines (On-Demand only) ----------\n",
    "# ---------- 2) build market baselines IN-PLACE (no merge needed) ----------\n",
    "scored = norm.copy()\n",
    "gcols = [\"gpu_model\", \"region\"]\n",
    "g = scored.groupby(gcols)[\"effective_price_usd_per_gpu_hr\"]\n",
    "\n",
    "scored[\"market_count\"]  = g.transform(\"size\")\n",
    "scored[\"market_median\"] = g.transform(\"median\")\n",
    "scored[\"market_mean\"]   = g.transform(\"mean\")\n",
    "scored[\"market_p25\"]    = g.transform(lambda s: s.quantile(0.25))\n",
    "scored[\"market_p75\"]    = g.transform(lambda s: s.quantile(0.75))\n",
    "scored[\"market_iqr\"]    = scored[\"market_p75\"] - scored[\"market_p25\"]\n",
    "scored[\"asof_utc\"]      = _now_iso()\n",
    "\n",
    "# safe premium vs median\n",
    "mm = scored[\"market_median\"].replace(0, np.nan)\n",
    "scored[\"premium_vs_median\"] = (scored[\"effective_price_usd_per_gpu_hr\"] - mm) / mm\n",
    "\n",
    "# (optional) also keep a compact market table for debugging/export\n",
    "market = (scored[[*gcols, \"market_count\",\"market_median\",\"market_mean\",\"market_p25\",\"market_p75\",\"market_iqr\"]]\n",
    "          .drop_duplicates().reset_index(drop=True))\n",
    "\n",
    "# ---------- 3) add Silicon-style score & write provider_scores_latest.csv ----------\n",
    "scored = add_priceiq_score(scored)\n",
    "scores_path = DERIVED_DIR / \"provider_scores_latest.csv\"\n",
    "scored.to_csv(scores_path, index=False)\n",
    "print(\"✔ wrote\", scores_path, f\"({len(scored)} rows)\")\n",
    "\n",
    "\n",
    "# ---------- 4) ROI table (cheapest total cost per scenario) ----------\n",
    "def roi_table(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    base = df[df[\"effective_price_usd_per_gpu_hr\"].notna()].copy()\n",
    "    if base.empty: return pd.DataFrame()\n",
    "    scenarios = []\n",
    "    models = [m for m in [\"H100\",\"H200\"] if m in set(base[\"gpu_model\"])] or sorted(base[\"gpu_model\"].unique())\n",
    "    counts = [8,16,32,64]\n",
    "    durs   = [\"1 hour\",\"1 day\",\"1 week\",\"1 month\"]\n",
    "    for m in models:\n",
    "        for c in counts:\n",
    "            for d in durs:\n",
    "                h = to_hours(d)\n",
    "                if not h or np.isnan(h): continue\n",
    "                sub = base[base[\"gpu_model\"].eq(m)].copy()\n",
    "                if sub.empty: continue\n",
    "                sub[\"total_cost\"] = sub[\"effective_price_usd_per_gpu_hr\"] * c * h\n",
    "                best = sub.sort_values([\"total_cost\",\"effective_price_usd_per_gpu_hr\"]).head(1)\n",
    "                if best.empty: continue\n",
    "                r = best.iloc[0]\n",
    "                scenarios.append({\n",
    "                    \"gpu_model\": m,\n",
    "                    \"gpu_count\": int(c),\n",
    "                    \"duration\": d,\n",
    "                    \"best_provider\": r[\"provider\"],\n",
    "                    \"best_region\": r[\"region\"],\n",
    "                    \"price_per_gpu_hr\": round(float(r[\"effective_price_usd_per_gpu_hr\"]), 4),\n",
    "                    \"total_cost_usd\": round(float(r[\"total_cost\"]), 2),\n",
    "                    \"timestamp\": r[\"timestamp\"]\n",
    "                })\n",
    "    return pd.DataFrame(scenarios)\n",
    "\n",
    "roi_df = roi_table(scored)\n",
    "roi_path = DERIVED_DIR / \"roi_comparison.csv\"\n",
    "roi_df.to_csv(roi_path, index=False)\n",
    "print(\"✔ wrote\", roi_path, f\"({len(roi_df)} rows)\")\n",
    "\n",
    "# ---------- 5) (optional) market index for debugging ----------\n",
    "market_path = DERIVED_DIR / \"market_index.csv\"\n",
    "market.to_csv(market_path, index=False)\n",
    "print(\"✔ wrote\", market_path)\n",
    "\n",
    "# quick peek\n",
    "display(scored.head(10)[[\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\"effective_price_usd_per_gpu_hr\",\"priceiq_score\"]])\n",
    "display(roi_df.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAFETY HARDENER (drop-in) ===============================================\n",
    "import pandas as pd, numpy as np\n",
    "\n",
    "NUM_COLS = [\n",
    "    \"price_hourly_usd\",\"price_hourly_usd_instance\",\"price_reserved_usd\",\n",
    "    \"effective_price_usd_per_gpu_hr\",\"gpu_count\",\n",
    "    \"market_med\",\"market_mean\",\"market_median\",\"market_p25\",\"market_p75\",\"market_cnt\",\"market_iqr\",\n",
    "    \"price_score\",\"premium_vs_median\",\"premium_vs_mkt\"\n",
    "]\n",
    "DATE_COLS = [\"fetched_at_utc\",\"timestamp\",\"asof_utc\",\"ts_utc\",\"ts_iso\",\"last_obs_date\",\"date_target\"]\n",
    "\n",
    "def _sanitize(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not isinstance(df, pd.DataFrame): return df\n",
    "\n",
    "    # 1) datetimes\n",
    "    for c in DATE_COLS:\n",
    "        if c in df.columns:\n",
    "            s = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "            df[c] = s.dt.tz_convert(None)\n",
    "\n",
    "    # keep a pure pandas datetime64 day column (not python date objects)\n",
    "    if \"date\" in df.columns:\n",
    "        d = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        # if it was tz-aware, drop tz; always floor to day\n",
    "        if getattr(d.dtype, \"tz\", None) is not None:\n",
    "            d = d.dt.tz_convert(None)\n",
    "        df[\"date\"] = d.dt.floor(\"D\")\n",
    "\n",
    "    # 2) numerics (coerce before any clip/ratios)\n",
    "    for c in NUM_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run on any frames that exist in this notebook cell\n",
    "for _name in (\"raw\",\"df\",\"qa\",\"market\",\"scored\",\"od\",\"norm\",\"hist\",\"model_df\",\"last_rows\"):\n",
    "    if _name in globals() and isinstance(globals()[_name], pd.DataFrame):\n",
    "        globals()[_name] = _sanitize(globals()[_name])\n",
    "\n",
    "# If you're about to use last_rows ratios (ML baseline), ensure these are numeric:\n",
    "if \"last_rows\" in globals() and isinstance(last_rows, pd.DataFrame):\n",
    "    for c in [\"market_p25\",\"market_p75\",\"market_med\",\"price_hourly_usd\"]:\n",
    "        if c in last_rows.columns:\n",
    "            last_rows[c] = pd.to_numeric(last_rows[c], errors=\"coerce\")\n",
    "# ============================================================================ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ML] Not enough data — using baseline: yhat = last price; bands from market dispersion.\n",
      "✔ wrote docs/data/derived/price_predictions.csv (21 rows)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provider</th>\n",
       "      <th>region</th>\n",
       "      <th>gpu_model</th>\n",
       "      <th>type</th>\n",
       "      <th>duration</th>\n",
       "      <th>gpu_count</th>\n",
       "      <th>date_target</th>\n",
       "      <th>yhat</th>\n",
       "      <th>p10</th>\n",
       "      <th>p50</th>\n",
       "      <th>p90</th>\n",
       "      <th>last_obs_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>3.900</td>\n",
       "      <td>3.5948</td>\n",
       "      <td>3.900</td>\n",
       "      <td>5.1293</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>Reserved-1y</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>2.930</td>\n",
       "      <td>2.7007</td>\n",
       "      <td>2.930</td>\n",
       "      <td>3.8536</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>Reserved-3y</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>2.540</td>\n",
       "      <td>2.3412</td>\n",
       "      <td>2.540</td>\n",
       "      <td>3.3407</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CrusoeCloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>Reserved-6m</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>3.120</td>\n",
       "      <td>2.8758</td>\n",
       "      <td>3.120</td>\n",
       "      <td>4.1035</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hydra Host (Brokkr)</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.1200</td>\n",
       "      <td>2.300</td>\n",
       "      <td>3.0250</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nebius</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>2.000</td>\n",
       "      <td>1.8435</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.6304</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OVHcloud</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.2959</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Paperspace</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.0647</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.9461</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shadeform</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>10.000</td>\n",
       "      <td>9.2174</td>\n",
       "      <td>10.000</td>\n",
       "      <td>13.1522</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TensorDock</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.0739</td>\n",
       "      <td>2.250</td>\n",
       "      <td>2.9592</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Vast.ai</td>\n",
       "      <td>Global</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.1522</td>\n",
       "      <td>1.250</td>\n",
       "      <td>1.6440</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CoreWeave</td>\n",
       "      <td>US</td>\n",
       "      <td>H100</td>\n",
       "      <td>On-Demand</td>\n",
       "      <td>1h</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2025-09-05</td>\n",
       "      <td>20.000</td>\n",
       "      <td>12.0000</td>\n",
       "      <td>20.000</td>\n",
       "      <td>28.1901</td>\n",
       "      <td>2025-09-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               provider  region gpu_model         type duration  gpu_count  \\\n",
       "0           CrusoeCloud  Global      H100    On-Demand       1h        1.0   \n",
       "1           CrusoeCloud  Global      H100  Reserved-1y       1h        1.0   \n",
       "2           CrusoeCloud  Global      H100  Reserved-3y       1h        1.0   \n",
       "3           CrusoeCloud  Global      H100  Reserved-6m       1h        1.0   \n",
       "4   Hydra Host (Brokkr)  Global      H100    On-Demand       1h        1.0   \n",
       "5                Nebius  Global      H100    On-Demand       1h        NaN   \n",
       "6              OVHcloud  Global      H100    On-Demand       1h        8.0   \n",
       "7            Paperspace  Global      H100    On-Demand       1h        NaN   \n",
       "8             Shadeform  Global      H100    On-Demand       1h        NaN   \n",
       "9            TensorDock  Global      H100    On-Demand       1h        1.0   \n",
       "10              Vast.ai  Global      H100    On-Demand       1h        NaN   \n",
       "11            CoreWeave      US      H100    On-Demand       1h        1.0   \n",
       "\n",
       "   date_target    yhat      p10     p50      p90 last_obs_date  \n",
       "0   2025-09-05   3.900   3.5948   3.900   5.1293    2025-09-04  \n",
       "1   2025-09-05   2.930   2.7007   2.930   3.8536    2025-09-04  \n",
       "2   2025-09-05   2.540   2.3412   2.540   3.3407    2025-09-04  \n",
       "3   2025-09-05   3.120   2.8758   3.120   4.1035    2025-09-04  \n",
       "4   2025-09-05   2.300   2.1200   2.300   3.0250    2025-09-04  \n",
       "5   2025-09-05   2.000   1.8435   2.000   2.6304    2025-09-04  \n",
       "6   2025-09-05   0.225   0.2074   0.225   0.2959    2025-09-04  \n",
       "7   2025-09-05   2.240   2.0647   2.240   2.9461    2025-09-04  \n",
       "8   2025-09-05  10.000   9.2174  10.000  13.1522    2025-09-04  \n",
       "9   2025-09-05   2.250   2.0739   2.250   2.9592    2025-09-04  \n",
       "10  2025-09-05   1.250   1.1522   1.250   1.6440    2025-09-04  \n",
       "11  2025-09-05  20.000  12.0000  20.000  28.1901    2025-09-04  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== ML (robust): Next-day per-GPU price with safe fallback ==================\n",
    "# Works on old sklearn. Trains only if enough history; else baseline fallback.\n",
    "import re, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# ---- helpers -----------------------------------------------------------------\n",
    "def mape(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    m = y_true != 0\n",
    "    return np.mean(np.abs((y_true[m] - y_pred[m]) / np.abs(y_true[m]))) if np.any(m) else np.nan\n",
    "\n",
    "def make_ohe():\n",
    "    try:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        return OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "def _ensure_day_datetime(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"datetime64[ns] day-floor; never python date; NaT on failure.\"\"\"\n",
    "    s = pd.to_datetime(s, errors=\"coerce\")\n",
    "    try: s = s.dt.tz_convert(None)\n",
    "    except Exception: pass\n",
    "    return s.dt.floor(\"D\")\n",
    "\n",
    "def _num(df: pd.DataFrame, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# ---- paths -------------------------------------------------------------------\n",
    "BASE      = Path(\"docs/data\")\n",
    "HIST_DIR  = BASE / \"history\"\n",
    "DERIVED   = BASE / \"derived\"\n",
    "DERIVED.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- small utils reused from your aggregator ---------------------------------\n",
    "def to_hours(s):\n",
    "    if not isinstance(s,str) or not s.strip(): return np.nan\n",
    "    s = s.strip().lower().rstrip(\"s\")\n",
    "    m = re.match(r\"^(\\d+(?:\\.\\d+)?)\\s*([a-z]+)$\", s)\n",
    "    if not m: return np.nan\n",
    "    q,u = float(m.group(1)), m.group(2)\n",
    "    mult = {\"h\":1,\"hr\":1,\"hour\":1,\"d\":24,\"day\":24,\"w\":168,\"wk\":168,\"week\":168,\"mo\":720,\"month\":720}.get(u, np.nan)\n",
    "    return q*mult\n",
    "\n",
    "def _to_num(x):\n",
    "    try:\n",
    "        if x is None or (isinstance(x,str) and not x.strip()): return np.nan\n",
    "        return float(re.sub(r\"[^\\d.\\-]\", \"\", str(x)))\n",
    "    except: return np.nan\n",
    "\n",
    "def normalise_history(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    req = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\",\"gpu_count\",\"price_hourly_usd\",\"fetched_at_utc\"]\n",
    "    for c in req:\n",
    "        if c not in out.columns: out[c] = pd.PandasDtype.NA if hasattr(pd, \"PandasDtype\") else pd.NA\n",
    "    out[\"gpu_model\"] = out[\"gpu_model\"].astype(str).str.upper().str.strip()\n",
    "    out[\"region\"]    = out[\"region\"].astype(str).replace({\"nan\":None}).fillna(\"Global\").str.strip()\n",
    "    out[\"type\"]      = out[\"type\"].astype(str).replace({\"nan\":None}).fillna(\"On-Demand\").str.strip()\n",
    "    out[\"duration\"]  = out[\"duration\"].astype(str).replace({\"nan\":None,\"\":None}).fillna(\"1h\").str.strip()\n",
    "    out[\"gpu_count\"] = pd.to_numeric(out[\"gpu_count\"], errors=\"coerce\")\n",
    "    out[\"price_hourly_usd\"] = pd.to_numeric(out[\"price_hourly_usd\"].map(_to_num))\n",
    "    out[\"fetched_at_utc\"]   = pd.to_datetime(out[\"fetched_at_utc\"], errors=\"coerce\", utc=True)\n",
    "    out = out[out[\"gpu_model\"].isin([\"H100\",\"H200\"])]\n",
    "    out = out[out[\"price_hourly_usd\"].between(0.05, 200)]\n",
    "    out[\"hours\"] = out[\"duration\"].map(to_hours).fillna(1.0)\n",
    "    # IMPORTANT: keep as datetime64[ns], not python date\n",
    "    out[\"date\"]  = _ensure_day_datetime(out[\"fetched_at_utc\"])\n",
    "    return out\n",
    "\n",
    "# 1) Load & unify history\n",
    "frames = []\n",
    "for p in sorted(HIST_DIR.glob(\"*_history.csv\")):\n",
    "    try: frames.append(pd.read_csv(p, low_memory=False))\n",
    "    except Exception as e: print(f\"[WARN] read {p.name}: {e}\")\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No history CSVs in docs/data/history/ — run scrapers that save history first.\")\n",
    "hist = normalise_history(pd.concat(frames, ignore_index=True))\n",
    "\n",
    "# Keep last obs per (provider,region,gpu,type,duration) per day\n",
    "keys = [\"provider\",\"region\",\"gpu_model\",\"type\",\"duration\"]\n",
    "hist = (hist.sort_values(\"fetched_at_utc\")\n",
    "            .drop_duplicates(subset=keys+[\"date\"], keep=\"last\")\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "# 2) Daily market features\n",
    "g = hist.groupby([\"date\",\"gpu_model\",\"region\"])[\"price_hourly_usd\"]\n",
    "market = pd.DataFrame({\n",
    "    \"market_cnt\": g.size(),\n",
    "    \"market_med\": g.median(),\n",
    "    \"market_p25\": g.quantile(0.25),\n",
    "    \"market_p75\": g.quantile(0.75),\n",
    "}).reset_index()\n",
    "\n",
    "df = hist.merge(market, on=[\"date\",\"gpu_model\",\"region\"], how=\"left\")\n",
    "# ensure numeric before ratios/clip\n",
    "_num(df, [\"price_hourly_usd\",\"market_med\",\"market_p25\",\"market_p75\",\"market_cnt\"])\n",
    "df[\"premium_vs_mkt\"] = (df[\"price_hourly_usd\"] - df[\"market_med\"]) / df[\"market_med\"]\n",
    "\n",
    "# 3) Supervised set with lags (t → t+1)\n",
    "df = df.sort_values(keys+[\"date\"]).reset_index(drop=True)\n",
    "for col in [\"price_hourly_usd\",\"premium_vs_mkt\",\"market_med\",\"market_p25\",\"market_p75\",\"market_cnt\"]:\n",
    "    df[f\"{col}_lag1\"] = df.groupby(keys)[col].shift(1)\n",
    "\n",
    "model_df = df.dropna(subset=[\"price_hourly_usd_lag1\",\"market_med_lag1\"]).copy()\n",
    "\n",
    "# ---- SAFETY GATES ----\n",
    "enough_rows   = len(model_df) >= 60\n",
    "enough_dates  = model_df[\"date\"].nunique() >= 12\n",
    "can_train_ml  = bool(enough_rows and enough_dates)\n",
    "\n",
    "def build_next_features(df_all: pd.DataFrame, model_df: pd.DataFrame):\n",
    "    # normalize date column defensively\n",
    "    if \"date\" in df_all.columns:\n",
    "        df_all = df_all.copy()\n",
    "        df_all[\"date\"] = _ensure_day_datetime(df_all[\"date\"])\n",
    "    last_date = model_df[\"date\"].max()\n",
    "    next_date = last_date + timedelta(days=1)\n",
    "    last_per_group = (df_all[df_all[\"date\"]==last_date]\n",
    "                      .dropna(subset=[\"price_hourly_usd\",\"market_med\"])\n",
    "                      .copy())\n",
    "    return last_per_group, last_date, next_date\n",
    "\n",
    "# 4A) ML path (only if enough data)\n",
    "pred = None\n",
    "if can_train_ml:\n",
    "    y = np.log1p(model_df[\"price_hourly_usd\"])\n",
    "    X = model_df[[\n",
    "        \"gpu_model\",\"region\",\"type\",\"duration\",\n",
    "        \"gpu_count\",\n",
    "        \"market_med_lag1\",\"market_p25_lag1\",\"market_p75_lag1\",\"market_cnt_lag1\",\n",
    "        \"price_hourly_usd_lag1\",\"premium_vs_mkt_lag1\",\n",
    "    ]]\n",
    "\n",
    "    cat_cols = [\"gpu_model\",\"region\",\"type\",\"duration\"]\n",
    "    num_cols = [c for c in X.columns if c not in cat_cols]\n",
    "    pre = ColumnTransformer([\n",
    "        (\"cat\", make_ohe(), cat_cols),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ])\n",
    "\n",
    "    point = Pipeline([(\"pre\", pre),\n",
    "                      (\"model\", GradientBoostingRegressor(loss=\"ls\", n_estimators=600, learning_rate=0.05, max_depth=3))])\n",
    "\n",
    "    # guarded CV\n",
    "    n_samples = len(model_df)\n",
    "    n_dates = model_df[\"date\"].nunique()\n",
    "    n_splits = min(5, max(2, n_dates - 1))\n",
    "    if n_samples <= n_splits:\n",
    "        n_splits = 2 if n_samples > 2 else 0\n",
    "\n",
    "    if n_splits >= 2:\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        maes, mapes = [], []\n",
    "        for tr, va in tscv.split(X):\n",
    "            point.fit(X.iloc[tr], y.iloc[tr])\n",
    "            pred_cv = np.expm1(point.predict(X.iloc[va]))\n",
    "            truth   = model_df.iloc[va][\"price_hourly_usd\"].values\n",
    "            maes.append(mean_absolute_error(truth, pred_cv))\n",
    "            mapes.append(mape(truth, pred_cv))\n",
    "        print(f\"[CV] MAE={np.mean(maes):.3f}, MAPE={np.mean(mapes)*100:.1f}% over {len(maes)} folds\")\n",
    "    else:\n",
    "        print(\"[CV] skipped (not enough samples)\")\n",
    "\n",
    "    point.fit(X, y)\n",
    "\n",
    "    def q_pipe(alpha):\n",
    "        return Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"model\", GradientBoostingRegressor(loss=\"quantile\", alpha=alpha,\n",
    "                                                n_estimators=700, learning_rate=0.05, max_depth=3))\n",
    "        ])\n",
    "    q10 = q_pipe(0.10).fit(X, y)\n",
    "    q50 = q_pipe(0.50).fit(X, y)\n",
    "    q90 = q_pipe(0.90).fit(X, y)\n",
    "\n",
    "    last_per_group, last_date, target_date = build_next_features(df, model_df)\n",
    "    if last_per_group.empty:\n",
    "        can_train_ml = False\n",
    "    else:\n",
    "        # ensure numeric for ratios if you add any later\n",
    "        _num(last_per_group, [\"market_med\",\"market_p25\",\"market_p75\",\"price_hourly_usd\"])\n",
    "        nextX = last_per_group[[\n",
    "            \"gpu_model\",\"region\",\"type\",\"duration\",\"gpu_count\",\n",
    "            \"market_med\",\"market_p25\",\"market_p75\",\"market_cnt\",\n",
    "            \"price_hourly_usd\",\"premium_vs_mkt\",\n",
    "        ]].rename(columns={\n",
    "            \"market_med\":\"market_med_lag1\",\n",
    "            \"market_p25\":\"market_p25_lag1\",\n",
    "            \"market_p75\":\"market_p75_lag1\",\n",
    "            \"market_cnt\":\"market_cnt_lag1\",\n",
    "            \"price_hourly_usd\":\"price_hourly_usd_lag1\",\n",
    "            \"premium_vs_mkt\":\"premium_vs_mkt_lag1\",\n",
    "        })\n",
    "\n",
    "        yhat   = np.expm1(point.predict(nextX))\n",
    "        yhat10 = np.expm1(q10.predict(nextX))\n",
    "        yhat50 = np.expm1(q50.predict(nextX))\n",
    "        yhat90 = np.expm1(q90.predict(nextX))\n",
    "\n",
    "        pred = last_per_group[keys + [\"gpu_count\"]].copy()\n",
    "        pred[\"date_target\"]   = pd.to_datetime(str(target_date))\n",
    "        pred[\"yhat\"]          = yhat.round(4)\n",
    "        pred[\"p10\"]           = yhat10.round(4)\n",
    "        pred[\"p50\"]           = yhat50.round(4)\n",
    "        pred[\"p90\"]           = yhat90.round(4)\n",
    "        pred[\"last_obs_date\"] = pd.to_datetime(str(last_date))\n",
    "\n",
    "# 4B) Baseline fallback (no crash, uses last price + market dispersion)\n",
    "if pred is None:\n",
    "    print(\"[ML] Not enough data — using baseline: yhat = last price; bands from market dispersion.\")\n",
    "    # normalize date, then pick latest valid day\n",
    "    if \"date\" in df.columns:\n",
    "        df = df.copy()\n",
    "        df[\"date\"] = _ensure_day_datetime(df[\"date\"])\n",
    "    last_date = df.loc[df[\"date\"].notna(), \"date\"].max()\n",
    "    target_date = last_date + timedelta(days=1)\n",
    "\n",
    "    last_per_group = (df[df[\"date\"]==last_date]\n",
    "                      .dropna(subset=[\"price_hourly_usd\",\"market_med\",\"market_p25\",\"market_p75\"])\n",
    "                      .copy())\n",
    "    # numeric before ratios/clip\n",
    "    _num(last_per_group, [\"market_p25\",\"market_p75\",\"market_med\",\"price_hourly_usd\"])\n",
    "\n",
    "    # bands via market dispersion\n",
    "    ratio25 = (last_per_group[\"market_p25\"] / last_per_group[\"market_med\"]).clip(0.6, 1.0).fillna(0.9)\n",
    "    ratio75 = (last_per_group[\"market_p75\"] / last_per_group[\"market_med\"]).clip(1.0, 1.6).fillna(1.1)\n",
    "    y_last  = last_per_group[\"price_hourly_usd\"].values\n",
    "    p10     = (y_last * ratio25.values).round(4)\n",
    "    p50     = y_last.round(4)\n",
    "    p90     = (y_last * ratio75.values).round(4)\n",
    "\n",
    "    pred = last_per_group[keys + [\"gpu_count\"]].copy()\n",
    "    pred[\"date_target\"]   = pd.to_datetime(str(target_date))\n",
    "    pred[\"yhat\"]          = p50\n",
    "    pred[\"p10\"]           = p10\n",
    "    pred[\"p50\"]           = p50\n",
    "    pred[\"p90\"]           = p90\n",
    "    pred[\"last_obs_date\"] = pd.to_datetime(str(last_date))\n",
    "\n",
    "# 5) Save predictions\n",
    "OUT = DERIVED / \"price_predictions.csv\"\n",
    "pred = pred.sort_values([\"gpu_model\",\"region\",\"provider\",\"type\",\"duration\"]).reset_index(drop=True)\n",
    "pred.to_csv(OUT, index=False)\n",
    "print(f\"✔ wrote {OUT} ({len(pred)} rows)\")\n",
    "display(pred.head(12))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
